#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é˜¿å°”å…¹æµ·é»˜ç—‡ç»„(AD Group)æ•°æ®æå–è„šæœ¬
ä»åŸå§‹ç›®å½•æå–ADç»„çš„åŸå§‹æ•°æ®å’Œé¢„å¤„ç†æ•°æ®åˆ°é¡¹ç›®çš„ç»Ÿä¸€ç›®å½•ç»“æ„ä¸­
"""

import os
import shutil
from pathlib import Path
from datetime import datetime

def extract_ad_data():
    """æå–ADç»„æ•°æ®åˆ°é¡¹ç›®ç›®å½•"""
    print("ğŸ§  å¼€å§‹æå–é˜¿å°”å…¹æµ·é»˜ç—‡ç»„(AD Group)æ•°æ®...")
    print("=" * 60)
    
    # ADç»„æ•°æ®æºè·¯å¾„
    ad_base = r"C:\Users\asino\entropy\ip\mci-dataprocessing\trans_ad"
    
    # é¡¹ç›®ç›®å½•è·¯å¾„
    project_dirs = {
        'ad_raw': 'data/ad_raw',
        'ad_processed': 'data/ad_processed', 
        'ad_calibrated': 'data/ad_calibrated'
    }
    
    # åˆ›å»ºç›®å½•ç»“æ„
    for dir_name, dir_path in project_dirs.items():
        os.makedirs(dir_path, exist_ok=True)
        print(f"âœ“ åˆ›å»ºç›®å½•: {dir_path}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_groups': 0,
        'raw_files': 0,
        'processed_files': 0,
        'groups_with_raw': [],
        'groups_with_processed': [],
        'groups_missing_data': []
    }
    
    print(f"\nğŸ” æ‰«æADç»„æ•°æ®æº: {ad_base}")
    
    # è·å–æ‰€æœ‰ADç»„ç›®å½•
    if not os.path.exists(ad_base):
        print(f"âŒ ADç»„æ•°æ®æºç›®å½•ä¸å­˜åœ¨: {ad_base}")
        return
    
    # æ‰«ææ‰€æœ‰æ•°å­—ç›®å½•
    for item in os.listdir(ad_base):
        if item.isdigit():
            group_num = item
            group_path = os.path.join(ad_base, group_num)
            
            if os.path.isdir(group_path):
                stats['total_groups'] += 1
                group_name = f"ad_group_{group_num}"
                
                print(f"\n--- å¤„ç† {group_name} ---")
                
                # æ£€æŸ¥å¹¶å¤åˆ¶åŸå§‹æ•°æ®
                rawdata_path = os.path.join(group_path, "rawdata")
                if os.path.exists(rawdata_path):
                    raw_files = copy_files(
                        rawdata_path,
                        os.path.join(project_dirs['ad_raw'], group_name),
                        file_pattern="*.txt",
                        description="åŸå§‹æ•°æ®"
                    )
                    stats['raw_files'] += raw_files
                    if raw_files > 0:
                        stats['groups_with_raw'].append(group_name)
                else:
                    print(f"  âš ï¸  rawdataç›®å½•ä¸å­˜åœ¨")
                
                # æ£€æŸ¥å¹¶å¤åˆ¶é¢„å¤„ç†æ•°æ®
                csvfile_path = os.path.join(group_path, "csvfile")
                if os.path.exists(csvfile_path):
                    processed_files = copy_files(
                        csvfile_path,
                        os.path.join(project_dirs['ad_processed'], group_name),
                        file_pattern="*.csv",
                        description="é¢„å¤„ç†æ•°æ®"
                    )
                    stats['processed_files'] += processed_files
                    if processed_files > 0:
                        stats['groups_with_processed'].append(group_name)
                else:
                    print(f"  âš ï¸  csvfileç›®å½•ä¸å­˜åœ¨")
                
                # è®°å½•ç¼ºå¤±æ•°æ®çš„ç»„
                if group_name not in stats['groups_with_raw'] and group_name not in stats['groups_with_processed']:
                    stats['groups_missing_data'].append(group_name)
    
    # ç”ŸæˆADç»„æ•°æ®æ‘˜è¦
    generate_ad_summary(stats)
    
    # æ‰“å°æå–ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ‰ ADç»„æ•°æ®æå–å®Œæˆ!")
    print(f"ğŸ“Š æ€»è®¡æ‰«æ: {stats['total_groups']} ä¸ªADç»„")
    print(f"ğŸ“ åŸå§‹æ–‡ä»¶: {stats['raw_files']} ä¸ª")
    print(f"ğŸ“„ é¢„å¤„ç†æ–‡ä»¶: {stats['processed_files']} ä¸ª")
    print(f"âœ… æœ‰åŸå§‹æ•°æ®çš„ç»„: {len(stats['groups_with_raw'])} ä¸ª")
    print(f"âœ… æœ‰é¢„å¤„ç†æ•°æ®çš„ç»„: {len(stats['groups_with_processed'])} ä¸ª")
    
    if stats['groups_missing_data']:
        print(f"âš ï¸  ç¼ºå¤±æ•°æ®çš„ç»„: {len(stats['groups_missing_data'])} ä¸ª")
        print(f"   {', '.join(stats['groups_missing_data'])}")

def copy_files(source_dir, target_dir, file_pattern="*", description="æ–‡ä»¶"):
    """å¤åˆ¶æ–‡ä»¶åˆ°ç›®æ ‡ç›®å½•
    
    Args:
        source_dir: æºç›®å½•
        target_dir: ç›®æ ‡ç›®å½•  
        file_pattern: æ–‡ä»¶æ¨¡å¼
        description: æ–‡ä»¶æè¿°
        
    Returns:
        int: å¤åˆ¶çš„æ–‡ä»¶æ•°é‡
    """
    if not os.path.exists(source_dir):
        return 0
    
    # åˆ›å»ºç›®æ ‡ç›®å½•
    os.makedirs(target_dir, exist_ok=True)
    
    # è·å–åŒ¹é…çš„æ–‡ä»¶
    from glob import glob
    files = glob(os.path.join(source_dir, file_pattern))
    
    if not files:
        print(f"  âŒ æœªæ‰¾åˆ°{description}: {file_pattern}")
        return 0
    
    copied_count = 0
    for file_path in files:
        if os.path.isfile(file_path):
            file_name = os.path.basename(file_path)
            target_file = os.path.join(target_dir, file_name)
            
            try:
                shutil.copy2(file_path, target_file)
                copied_count += 1
            except Exception as e:
                print(f"  âŒ å¤åˆ¶å¤±è´¥ {file_name}: {e}")
    
    print(f"  âœ“ {description}: {copied_count} ä¸ªæ–‡ä»¶")
    return copied_count

def generate_ad_summary(stats):
    """ç”ŸæˆADç»„æ•°æ®æ‘˜è¦æ–‡æ¡£"""
    print("\nğŸ“‹ ç”ŸæˆADç»„æ•°æ®æ‘˜è¦...")
    
    summary_lines = [
        "# é˜¿å°”å…¹æµ·é»˜ç—‡ç»„(AD Group)æ•°æ®æ¦‚è§ˆ\n",
        f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        f"æå–è‡ª: C:\\Users\\asino\\entropy\\ip\\mci-dataprocessing\\trans_ad\n",
        "\n## æ•°æ®æ¦‚å†µ\n",
        f"- **æ€»ç»„æ•°**: {stats['total_groups']} ä¸ªADç»„\n",
        f"- **åŸå§‹æ–‡ä»¶**: {stats['raw_files']} ä¸ª.txtæ–‡ä»¶\n", 
        f"- **é¢„å¤„ç†æ–‡ä»¶**: {stats['processed_files']} ä¸ª.csvæ–‡ä»¶\n",
        f"- **æœ‰åŸå§‹æ•°æ®çš„ç»„**: {len(stats['groups_with_raw'])} ä¸ª\n",
        f"- **æœ‰é¢„å¤„ç†æ•°æ®çš„ç»„**: {len(stats['groups_with_processed'])} ä¸ª\n",
        "\n## ç›®å½•ç»“æ„\n",
        "```\n",
        "data/\n",
        "â”œâ”€â”€ ad_raw/                   # ADç»„åŸå§‹æ•°æ®\n",
        "â”‚   â”œâ”€â”€ ad_group_1/           # ADç¬¬1ç»„æ•°æ®\n",
        "â”‚   â”‚   â”œâ”€â”€ 1.txt             # ä»»åŠ¡1åŸå§‹æ•°æ®\n",
        "â”‚   â”‚   â”œâ”€â”€ 2.txt             # ä»»åŠ¡2åŸå§‹æ•°æ®\n",
        "â”‚   â”‚   â”œâ”€â”€ 3.txt             # ä»»åŠ¡3åŸå§‹æ•°æ®\n",
        "â”‚   â”‚   â”œâ”€â”€ 4.txt             # ä»»åŠ¡4åŸå§‹æ•°æ®\n",
        "â”‚   â”‚   â””â”€â”€ 5.txt             # ä»»åŠ¡5åŸå§‹æ•°æ®\n",
        "â”‚   â””â”€â”€ ...\n",
        "â”œâ”€â”€ ad_processed/             # ADç»„é¢„å¤„ç†æ•°æ®\n",
        "â”‚   â”œâ”€â”€ ad_group_1/           # ADç¬¬1ç»„å¤„ç†ç»“æœ\n",
        "â”‚   â”‚   â”œâ”€â”€ ad1q1_preprocessed.csv  # ä»»åŠ¡1é¢„å¤„ç†ç»“æœ\n",
        "â”‚   â”‚   â”œâ”€â”€ ad1q2_preprocessed.csv  # ä»»åŠ¡2é¢„å¤„ç†ç»“æœ\n",
        "â”‚   â”‚   â”œâ”€â”€ ad1q3_preprocessed.csv  # ä»»åŠ¡3é¢„å¤„ç†ç»“æœ\n",
        "â”‚   â”‚   â”œâ”€â”€ ad1q4_preprocessed.csv  # ä»»åŠ¡4é¢„å¤„ç†ç»“æœ\n",
        "â”‚   â”‚   â””â”€â”€ ad1q5_preprocessed.csv  # ä»»åŠ¡5é¢„å¤„ç†ç»“æœ\n",
        "â”‚   â””â”€â”€ ...\n",
        "â””â”€â”€ ad_calibrated/            # ADç»„æ ¡å‡†æ•°æ®\n",
        "    â”œâ”€â”€ ad_group_1/           # ADç¬¬1ç»„æ ¡å‡†ç»“æœ\n",
        "    â”‚   â”œâ”€â”€ ad1q1_preprocessed_calibrated.csv\n",
        "    â”‚   â”œâ”€â”€ ad1q2_preprocessed_calibrated.csv\n",
        "    â”‚   â”œâ”€â”€ ad1q3_preprocessed_calibrated.csv\n",
        "    â”‚   â”œâ”€â”€ ad1q4_preprocessed_calibrated.csv\n",
        "    â”‚   â””â”€â”€ ad1q5_preprocessed_calibrated.csv\n",
        "    â””â”€â”€ ...\n",
        "```\n",
        "\n## æ–‡ä»¶å‘½åè§„èŒƒ\n",
        "- **åŸå§‹æ•°æ®**: `1.txt`, `2.txt`, `3.txt`, `4.txt`, `5.txt`\n",
        "- **é¢„å¤„ç†æ•°æ®**: `ad{ç»„å·}q{ä»»åŠ¡ç¼–å·}_preprocessed.csv`\n",
        "- **æ ¡å‡†æ•°æ®**: `ad{ç»„å·}q{ä»»åŠ¡ç¼–å·}_preprocessed_calibrated.csv`\n",
        "- **ç»„åæ ¼å¼**: `ad_group_{ç»„å·}`\n",
        "\n## æ•°æ®å®Œæ•´æ€§\n"
    ]
    
    # æ·»åŠ è¯¦ç»†çš„ç»„æ•°æ®ç»Ÿè®¡
    if stats['groups_with_raw']:
        summary_lines.append("### æœ‰åŸå§‹æ•°æ®çš„ç»„\n")
        for group in sorted(stats['groups_with_raw']):
            summary_lines.append(f"- {group}\n")
        summary_lines.append("\n")
    
    if stats['groups_with_processed']:
        summary_lines.append("### æœ‰é¢„å¤„ç†æ•°æ®çš„ç»„\n")
        for group in sorted(stats['groups_with_processed']):
            summary_lines.append(f"- {group}\n")
        summary_lines.append("\n")
    
    if stats['groups_missing_data']:
        summary_lines.append("### ç¼ºå¤±æ•°æ®çš„ç»„\n")
        for group in sorted(stats['groups_missing_data']):
            summary_lines.append(f"- {group} âš ï¸\n")
        summary_lines.append("\n")
    
    summary_lines.extend([
        "## ä¸å…¶ä»–ç»„çš„å¯¹æ¯”\n",
        "| æ•°æ®ç»„ | ç»„æ•° | åŸå§‹æ–‡ä»¶ | é¢„å¤„ç†æ–‡ä»¶ | å‘½åå‰ç¼€ |\n",
        "|--------|------|----------|------------|----------|\n",
        "| Control Group | 20 | ~95 | 100 | n | \n",
        "| MCI Group | 21 | ~105 | 105 | m |\n",
        f"| AD Group | {len(stats['groups_with_processed'])} | {stats['raw_files']} | {stats['processed_files']} | ad |\n",
        "\n## ç ”ç©¶æ„ä¹‰\n",
        "- **å¯¹ç…§ç»„**: å¥åº·å¯¹ç…§\n", 
        "- **MCIç»„**: è½»åº¦è®¤çŸ¥éšœç¢\n",
        "- **ADç»„**: é˜¿å°”å…¹æµ·é»˜ç—‡æ‚£è€…\n",
        "- **ç ”ç©¶ä»·å€¼**: æ”¯æŒè®¤çŸ¥éšœç¢ç–¾ç—…è¿›å±•çš„çœ¼çƒè¿½è¸ªå¯¹æ¯”ç ”ç©¶\n",
    ])
    
    # ä¿å­˜ADç»„æ‘˜è¦
    with open("data/AD_SUMMARY.md", "w", encoding="utf-8") as f:
        f.writelines(summary_lines)
    
    print("   âœ“ ADç»„æ•°æ®æ‘˜è¦å·²ç”Ÿæˆ: data/AD_SUMMARY.md")

def main():
    """ä¸»å‡½æ•°"""
    try:
        extract_ad_data()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æå–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise

if __name__ == "__main__":
    main() 