#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ç»¼åˆç‰¹å¾æå–ä¸æ•´åˆæ¨¡å— (ç¬¬å…­æ¨¡å—) - APIæ¥å£
æä¾›ä»»åŠ¡çº§åˆ«ã€ROIçº§åˆ«ã€RQAå¢å¼ºç‰¹å¾çš„æå–å’Œæ•´åˆåŠŸèƒ½
"""

import os
import pandas as pd
import numpy as np
import json
import traceback
from datetime import datetime
from flask import Blueprint, request, jsonify
from scipy import stats
from scipy.spatial.distance import euclidean
from scipy.stats import entropy
import warnings
warnings.filterwarnings('ignore')

# åˆ›å»ºBlueprint
feature_extraction_bp = Blueprint('feature_extraction', __name__)

# åŸºç¡€è·¯å¾„é…ç½®
BASE_DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data')
FEATURE_RESULTS_DIR = os.path.join(BASE_DATA_DIR, 'feature_extraction_results')

# ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
os.makedirs(FEATURE_RESULTS_DIR, exist_ok=True)

class FeatureExtractor:
    """ç»¼åˆç‰¹å¾æå–å™¨"""
    
    def __init__(self):
        self.base_data_dir = BASE_DATA_DIR
        self.results_dir = FEATURE_RESULTS_DIR
        
    def load_all_data(self):
        """åŠ è½½æ‰€æœ‰å¿…è¦çš„æ•°æ®æ–‡ä»¶"""
        try:
            data = {}
            
            # åŠ è½½äº‹ä»¶æ•°æ®
            events_path = os.path.join(self.base_data_dir, 'event_analysis_results', 'All_Events.csv')
            if os.path.exists(events_path):
                data['events'] = pd.read_csv(events_path)
                print(f"âœ… åŠ è½½äº‹ä»¶æ•°æ®: {len(data['events'])} æ¡è®°å½•")
            
            # åŠ è½½ROIæ±‡æ€»æ•°æ®
            roi_path = os.path.join(self.base_data_dir, 'event_analysis_results', 'All_ROI_Summary.csv')
            if os.path.exists(roi_path):
                data['roi_summary'] = pd.read_csv(roi_path)
                print(f"âœ… åŠ è½½ROIæ±‡æ€»æ•°æ®: {len(data['roi_summary'])} æ¡è®°å½•")
            
            # åŠ è½½RQAæ•°æ®ï¼ˆä»æœ€è¿‘çš„æµç¨‹ç»“æœï¼‰
            rqa_dirs = [d for d in os.listdir(os.path.join(self.base_data_dir, 'rqa_pipeline_results')) 
                       if os.path.isdir(os.path.join(self.base_data_dir, 'rqa_pipeline_results', d))]
            
            if rqa_dirs:
                # å–æœ€æ–°çš„RQAç»“æœ
                latest_rqa_dir = sorted(rqa_dirs)[-1]
                rqa_file = os.path.join(self.base_data_dir, 'rqa_pipeline_results', 
                                      latest_rqa_dir, 'step2_data_merging', 
                                      'All_Subjects_RQA_EyeMetrics.csv')
                if os.path.exists(rqa_file):
                    data['rqa'] = pd.read_csv(rqa_file)
                    print(f"âœ… åŠ è½½RQAæ•°æ®: {len(data['rqa'])} æ¡è®°å½•")
            
            # åŠ è½½MMSEè¯„åˆ†æ•°æ®
            mmse_data = []
            mmse_dir = os.path.join(self.base_data_dir, 'MMSE_Score')
            if os.path.exists(mmse_dir):
                group_mapping = {
                    'æ§åˆ¶ç»„.csv': 'Control',
                    'è½»åº¦è®¤çŸ¥éšœç¢ç»„.csv': 'MCI', 
                    'é˜¿å°”å…¹æµ·é»˜ç—‡ç»„.csv': 'AD'
                }
                
                for file_name, group in group_mapping.items():
                    file_path = os.path.join(mmse_dir, file_name)
                    if os.path.exists(file_path):
                        mmse_df = pd.read_csv(file_path)
                        mmse_df['Group'] = group
                        mmse_data.append(mmse_df)
                
                if mmse_data:
                    data['mmse'] = pd.concat(mmse_data, ignore_index=True)
                    print(f"âœ… åŠ è½½MMSEæ•°æ®: {len(data['mmse'])} æ¡è®°å½•")
            
            return data
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            traceback.print_exc()
            return {}
    
    def extract_task_level_features(self, data):
        """æå–ä»»åŠ¡çº§åˆ«ç‰¹å¾"""
        print("ğŸ”„ å¼€å§‹æå–ä»»åŠ¡çº§åˆ«ç‰¹å¾...")
        
        if 'events' not in data:
            raise ValueError("ç¼ºå°‘äº‹ä»¶æ•°æ®")
        
        events_df = data['events']
        task_features = []
        
        # æŒ‰ADQ_IDåˆ†ç»„å¤„ç†æ¯ä¸ªä»»åŠ¡
        for adq_id in events_df['ADQ_ID'].unique():
            task_data = events_df[events_df['ADQ_ID'] == adq_id]
            
            # åŸºæœ¬ä¿¡æ¯
            subject_id = adq_id[:-2]  # å¦‚ n1q1 -> n1
            task_id = adq_id[-2:]     # å¦‚ n1q1 -> q1
            group = task_data['Group'].iloc[0] if 'Group' in task_data.columns else 'Unknown'
            
            # ç‰¹å¾è®¡ç®—
            features = {
                'ADQ_ID': adq_id,
                'SubjectID': subject_id,
                'TaskID': task_id,
                'Group': group
            }
            
            # 1. ä»»åŠ¡å®Œæˆæ—¶é—´ï¼ˆåŸºäºäº‹ä»¶çš„æŒç»­æ—¶é—´ä¼°ç®—ï¼‰
            fixations = task_data[task_data['EventType'] == 'fixation']
            if not fixations.empty:
                features['TaskCompletionTime'] = fixations['Duration_ms'].sum()
                
                # 2. é¦–æ¬¡æ³¨è§†æ—¶å»¶ï¼ˆç¬¬ä¸€ä¸ªæœ‰æ•ˆæ³¨è§†çš„å¼€å§‹æ—¶é—´ï¼‰
                first_fixation = fixations.iloc[0]
                features['FirstFixationLatency'] = first_fixation.get('StartIndex', 0) * 10  # å‡è®¾10ms/sample
                
                # 3. æ‰«è§†è·¯å¾„é•¿åº¦ï¼ˆåŸºäºå¹…åº¦ç´¯ç§¯ï¼‰
                features['ScanpathLength'] = fixations['Amplitude_deg'].sum()
                
                # 4. æ³¨è§†è½¬ç§»ç†µ
                roi_sequence = fixations['ROI'].dropna().tolist()
                features['FixationTransitionEntropy'] = self._calculate_transition_entropy(roi_sequence)
                
                # 5. æ³¨è§†å’Œæ‰«è§†ç»Ÿè®¡
                features['TotalFixationCount'] = len(fixations)
                features['TotalSaccadeCount'] = len(task_data[task_data['EventType'] == 'saccade'])
                features['AverageFixationDuration'] = fixations['Duration_ms'].mean()
                
                # 6. ä»»åŠ¡æœ‰æ•ˆæ€§è¯„åˆ†ï¼ˆåŸºäºæ³¨è§†è´¨é‡ï¼‰
                valid_fixations = fixations[fixations['Duration_ms'] >= 100]  # æœ‰æ•ˆæ³¨è§†é˜ˆå€¼
                features['TaskValidityScore'] = len(valid_fixations) / len(fixations) if len(fixations) > 0 else 0
            else:
                # é»˜è®¤å€¼
                for key in ['TaskCompletionTime', 'FirstFixationLatency', 'ScanpathLength', 
                           'FixationTransitionEntropy', 'TotalFixationCount', 'TotalSaccadeCount',
                           'AverageFixationDuration', 'TaskValidityScore']:
                    features[key] = 0
            
            task_features.append(features)
        
        task_features_df = pd.DataFrame(task_features)
        print(f"âœ… æå–ä»»åŠ¡çº§åˆ«ç‰¹å¾å®Œæˆ: {len(task_features_df)} ä¸ªä»»åŠ¡")
        return task_features_df
    
    def extract_roi_level_features(self, data):
        """æå–ROIçº§åˆ«ç‰¹å¾"""
        print("ğŸ”„ å¼€å§‹æå–ROIçº§åˆ«ç‰¹å¾...")
        
        if 'roi_summary' not in data or 'events' not in data:
            raise ValueError("ç¼ºå°‘ROIæ•°æ®æˆ–äº‹ä»¶æ•°æ®")
        
        roi_df = data['roi_summary']
        events_df = data['events']
        roi_features = []
        
        # æŒ‰ADQ_IDå’ŒROIåˆ†ç»„å¤„ç†
        for adq_id in roi_df['ADQ_ID'].unique():
            adq_data = roi_df[roi_df['ADQ_ID'] == adq_id]
            events_data = events_df[events_df['ADQ_ID'] == adq_id]
            
            subject_id = adq_id[:-2]
            task_id = adq_id[-2:]
            group = adq_data['Group'].iloc[0] if 'Group' in adq_data.columns else 'Unknown'
            
            # è®¡ç®—æ€»ä»»åŠ¡æ—¶é—´
            total_task_time = events_data[events_data['EventType'] == 'fixation']['Duration_ms'].sum()
            
            for _, roi_row in adq_data.iterrows():
                roi_name = roi_row['ROI']
                
                # ç¡®å®šROIç±»å‹
                roi_type = 'BG'  # é»˜è®¤èƒŒæ™¯
                if 'INST' in roi_name:
                    roi_type = 'INST'
                elif 'KW' in roi_name:
                    roi_type = 'KW'
                
                features = {
                    'ADQ_ID': adq_id,
                    'SubjectID': subject_id,
                    'TaskID': task_id,
                    'Group': group,
                    'ROI_Name': roi_name,
                    'ROI_Type': roi_type
                }
                
                # 1. æ³¨è§†æ—¶é—´ç™¾åˆ†æ¯”
                dwell_time = roi_row.get('FixTime', 0) * 1000  # è½¬æ¢ä¸ºms
                features['DwellTimePercentage'] = (dwell_time / total_task_time * 100) if total_task_time > 0 else 0
                
                # 2. è®¿é—®æ¬¡æ•°å’Œå›è§†æ¬¡æ•°
                features['VisitCount'] = roi_row.get('EnterCount', 0)
                features['RevisitCount'] = roi_row.get('RegressionCount', 0)
                
                # 3. å¹³å‡è®¿é—®æŒç»­æ—¶é—´
                visit_count = features['VisitCount']
                features['AverageVisitDuration'] = dwell_time / visit_count if visit_count > 0 else 0
                
                # 4. é¦–æ¬¡è¿›å…¥æ—¶é—´ï¼ˆåŸºäºäº‹ä»¶åºåˆ—ä¼°ç®—ï¼‰
                roi_events = events_data[events_data['ROI'] == roi_name]
                if not roi_events.empty:
                    first_event = roi_events.iloc[0]
                    features['FirstEntryTime'] = first_event.get('StartIndex', 0) * 10  # å‡è®¾10ms/sample
                else:
                    features['FirstEntryTime'] = 0
                
                # 5. ROIé‡è¦æ€§è¯„åˆ†ï¼ˆåŸºäºæ³¨è§†æ—¶é—´å’Œè®¿é—®é¢‘ç‡ï¼‰
                time_score = min(features['DwellTimePercentage'] / 20, 1)  # å½’ä¸€åŒ–åˆ°0-1
                visit_score = min(features['VisitCount'] / 10, 1)  # å½’ä¸€åŒ–åˆ°0-1
                features['ROI_Importance_Score'] = (time_score + visit_score) / 2
                
                roi_features.append(features)
        
        roi_features_df = pd.DataFrame(roi_features)
        print(f"âœ… æå–ROIçº§åˆ«ç‰¹å¾å®Œæˆ: {len(roi_features_df)} ä¸ªROIè®°å½•")
        return roi_features_df
    
    def extract_rqa_enhanced_features(self, data):
        """æå–RQAå¢å¼ºç‰¹å¾"""
        print("ğŸ”„ å¼€å§‹æå–RQAå¢å¼ºç‰¹å¾...")
        
        if 'rqa' not in data:
            raise ValueError("ç¼ºå°‘RQAæ•°æ®")
        
        rqa_df = data['rqa']
        rqa_features = []
        
        # æŒ‰è¢«è¯•åˆ†ç»„å¤„ç†
        for subject_id in rqa_df['ID'].str[:-2].unique():  # å»æ‰ä»»åŠ¡ID
            subject_data = rqa_df[rqa_df['ID'].str.startswith(subject_id)]
            
            if len(subject_data) < 2:  # è‡³å°‘éœ€è¦2ä¸ªä»»åŠ¡æ‰èƒ½è®¡ç®—å˜å¼‚æ€§
                continue
            
            group = subject_data['Group'].iloc[0]
            
            features = {
                'SubjectID': subject_id,
                'Group': group
            }
            
            # RQAæŒ‡æ ‡
            rqa_metrics = ['RR-2D-xy', 'RR-1D-x', 'DET-2D-xy', 'DET-1D-x', 'ENT-2D-xy', 'ENT-1D-x']
            
            # 1. è·¨ä»»åŠ¡RQAå˜å¼‚æ€§ï¼ˆå˜å¼‚ç³»æ•°ï¼‰
            for metric in rqa_metrics:
                if metric in subject_data.columns:
                    values = subject_data[metric].dropna()
                    if len(values) > 1:
                        cv = np.std(values) / np.mean(values) if np.mean(values) != 0 else 0
                        features[f'{metric}_CoV'] = cv
                    else:
                        features[f'{metric}_CoV'] = 0
            
            # 2. RQAæ—¶é—´æ¼”å˜è¶‹åŠ¿ï¼ˆçº¿æ€§æ–œç‡ï¼‰
            tasks_order = sorted(subject_data['q'].unique())
            for metric in rqa_metrics:
                if metric in subject_data.columns:
                    values = []
                    for task in tasks_order:
                        task_data = subject_data[subject_data['q'] == task]
                        if not task_data.empty:
                            values.append(task_data[metric].iloc[0])
                    
                    if len(values) >= 3:  # è‡³å°‘3ä¸ªç‚¹æ‰èƒ½è®¡ç®—è¶‹åŠ¿
                        x = np.arange(len(values))
                        slope, _ = np.polyfit(x, values, 1)
                        features[f'{metric}_Trend_Slope'] = slope
                    else:
                        features[f'{metric}_Trend_Slope'] = 0
            
            # 3. RQAç¨³å®šæ€§æŒ‡æ•°ï¼ˆåŸºäºæ ‡å‡†å·®ï¼‰
            for metric in rqa_metrics:
                if metric in subject_data.columns:
                    values = subject_data[metric].dropna()
                    if len(values) > 1:
                        stability = 1 / (1 + np.std(values))  # æ ‡å‡†å·®è¶Šå°ï¼Œç¨³å®šæ€§è¶Šé«˜
                        features[f'{metric}_Stability_Index'] = stability
                    else:
                        features[f'{metric}_Stability_Index'] = 1
            
            # 4. ç»¼åˆRQAå˜å¼‚æ€§æŒ‡æ ‡
            all_covs = [features.get(f'{metric}_CoV', 0) for metric in rqa_metrics]
            features['RQA_Overall_Variability'] = np.mean([cv for cv in all_covs if cv > 0])
            
            rqa_features.append(features)
        
        rqa_features_df = pd.DataFrame(rqa_features)
        print(f"âœ… æå–RQAå¢å¼ºç‰¹å¾å®Œæˆ: {len(rqa_features_df)} ä¸ªè¢«è¯•")
        return rqa_features_df
    
    def integrate_features_with_mmse(self, task_features, roi_features, rqa_features, mmse_data):
        """æ•´åˆæ‰€æœ‰ç‰¹å¾å¹¶åˆå¹¶MMSEæ•°æ®"""
        print("ğŸ”„ å¼€å§‹æ•´åˆç‰¹å¾...")
        
        # 1. åˆ›å»ºä¸»ç‰¹å¾è¡¨ï¼ˆä»»åŠ¡çº§åˆ«ï¼‰
        master_features = task_features.copy()
        
        # æ·»åŠ MMSEè¯„åˆ†
        if mmse_data is not None and not mmse_data.empty:
            mmse_mapping = {}
            for _, row in mmse_data.iterrows():
                subject_id = row['å—è¯•è€…']
                mmse_score = row['æ€»åˆ†']
                group = row['Group']
                mmse_mapping[subject_id] = {'MMSE_Score': mmse_score, 'MMSE_Group': group}
            
            master_features['MMSE_Score'] = master_features['SubjectID'].map(
                lambda x: mmse_mapping.get(x, {}).get('MMSE_Score', np.nan)
            )
            master_features['MMSE_Group'] = master_features['SubjectID'].map(
                lambda x: mmse_mapping.get(x, {}).get('MMSE_Group', 'Unknown')
            )
        
        # 2. èšåˆROIç‰¹å¾åˆ°ä»»åŠ¡çº§åˆ«
        roi_agg = roi_features.groupby(['ADQ_ID']).agg({
            'DwellTimePercentage': 'sum',
            'VisitCount': 'sum',
            'RevisitCount': 'sum',
            'ROI_Importance_Score': 'mean'
        }).reset_index()
        
        # åˆ†åˆ«è®¡ç®—å…³é”®ROIå’ŒèƒŒæ™¯ROIçš„ç»Ÿè®¡
        key_roi_stats = roi_features[roi_features['ROI_Type'].isin(['INST', 'KW'])].groupby('ADQ_ID').agg({
            'DwellTimePercentage': 'sum',
            'VisitCount': 'sum'
        }).reset_index()
        key_roi_stats.columns = ['ADQ_ID', 'KeyROI_DwellTime', 'KeyROI_VisitCount']
        
        bg_roi_stats = roi_features[roi_features['ROI_Type'] == 'BG'].groupby('ADQ_ID').agg({
            'DwellTimePercentage': 'sum',
            'VisitCount': 'sum'
        }).reset_index()
        bg_roi_stats.columns = ['ADQ_ID', 'BG_DwellTime', 'BG_VisitCount']
        
        # åˆå¹¶ROIç»Ÿè®¡åˆ°ä¸»ç‰¹å¾è¡¨
        master_features = master_features.merge(roi_agg, on='ADQ_ID', how='left')
        master_features = master_features.merge(key_roi_stats, on='ADQ_ID', how='left')
        master_features = master_features.merge(bg_roi_stats, on='ADQ_ID', how='left')
        
        # è®¡ç®—å…³é”®ROI vs èƒŒæ™¯ROIæ¯”ç‡
        master_features['KeyBG_Ratio'] = master_features['KeyROI_DwellTime'] / (master_features['BG_DwellTime'] + 1)
        
        # 3. æ·»åŠ RQAå¢å¼ºç‰¹å¾
        if rqa_features is not None and not rqa_features.empty:
            master_features = master_features.merge(rqa_features, on=['SubjectID', 'Group'], how='left')
        
        # 4. åˆ›å»ºè¢«è¯•æ±‡æ€»è¡¨
        subject_summary = master_features.groupby(['SubjectID', 'Group']).agg({
            'TaskCompletionTime': 'mean',
            'ScanpathLength': 'mean', 
            'FixationTransitionEntropy': 'mean',
            'TotalFixationCount': 'mean',
            'AverageFixationDuration': 'mean',
            'TaskValidityScore': 'mean',
            'KeyBG_Ratio': 'mean',
            'MMSE_Score': 'first'
        }).reset_index()
        
        # æ·»åŠ ä»»åŠ¡è¡¨ç°åˆ†æ•°
        for task_id in ['q1', 'q2', 'q3', 'q4', 'q5']:
            task_data = master_features[master_features['TaskID'] == task_id]
            task_scores = task_data.groupby('SubjectID')['TaskValidityScore'].first()
            subject_summary[f'Task{task_id[1:]}_Score'] = subject_summary['SubjectID'].map(task_scores)
        
        # æ·»åŠ RQAæ±‡æ€»æŒ‡æ ‡
        if rqa_features is not None and not rqa_features.empty:
            rqa_summary_cols = [col for col in rqa_features.columns 
                              if col.endswith('_CoV') or col.endswith('_Stability_Index') or 'Variability' in col]
            for col in rqa_summary_cols:
                if col in master_features.columns:
                    subject_summary[col] = master_features.groupby('SubjectID')[col].first()
        
        print(f"âœ… ç‰¹å¾æ•´åˆå®Œæˆ:")
        print(f"   - ä¸»ç‰¹å¾è¡¨: {len(master_features)} æ¡ä»»åŠ¡è®°å½•")
        print(f"   - è¢«è¯•æ±‡æ€»è¡¨: {len(subject_summary)} ä¸ªè¢«è¯•")
        
        return master_features, subject_summary
    
    def _calculate_transition_entropy(self, sequence):
        """è®¡ç®—è½¬ç§»ç†µ"""
        if len(sequence) < 2:
            return 0
        
        # æ„å»ºè½¬ç§»å¯¹
        transitions = [(sequence[i], sequence[i+1]) for i in range(len(sequence)-1)]
        
        # è®¡ç®—è½¬ç§»æ¦‚ç‡
        transition_counts = {}
        for trans in transitions:
            transition_counts[trans] = transition_counts.get(trans, 0) + 1
        
        total_transitions = len(transitions)
        probs = [count / total_transitions for count in transition_counts.values()]
        
        # è®¡ç®—Shannonç†µ
        return entropy(probs, base=2)
    
    def save_results(self, master_features, subject_summary, timestamp=None):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        if timestamp is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        try:
            # ä¿å­˜ä¸»ç‰¹å¾è¡¨
            master_file = os.path.join(self.results_dir, f'Master_Features_{timestamp}.csv')
            master_features.to_csv(master_file, index=False, encoding='utf-8-sig')
            
            # ä¿å­˜è¢«è¯•æ±‡æ€»è¡¨
            summary_file = os.path.join(self.results_dir, f'Subject_Features_Summary_{timestamp}.csv')
            subject_summary.to_csv(summary_file, index=False, encoding='utf-8-sig')
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                'timestamp': timestamp,
                'master_features_shape': master_features.shape,
                'subject_summary_shape': subject_summary.shape,
                'feature_columns': master_features.columns.tolist(),
                'summary_columns': subject_summary.columns.tolist()
            }
            
            metadata_file = os.path.join(self.results_dir, f'metadata_{timestamp}.json')
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            return {
                'master_file': master_file,
                'summary_file': summary_file,
                'metadata_file': metadata_file
            }
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
            traceback.print_exc()
            return None

# åˆ›å»ºå…¨å±€ç‰¹å¾æå–å™¨å®ä¾‹
feature_extractor = FeatureExtractor()

# APIè·¯ç”±å®šä¹‰
@feature_extraction_bp.route('/api/feature-extraction/extract', methods=['POST'])
def extract_features():
    """æ‰§è¡Œå®Œæ•´çš„ç‰¹å¾æå–æµç¨‹"""
    try:
        print("ğŸš€ å¼€å§‹ç»¼åˆç‰¹å¾æå–...")
        
        # åŠ è½½æ•°æ®
        data = feature_extractor.load_all_data()
        if not data:
            return jsonify({
                'success': False,
                'message': 'æ•°æ®åŠ è½½å¤±è´¥'
            }), 500
        
        # æå–å„ç±»ç‰¹å¾
        task_features = feature_extractor.extract_task_level_features(data)
        roi_features = feature_extractor.extract_roi_level_features(data)
        rqa_features = feature_extractor.extract_rqa_enhanced_features(data)
        
        # æ•´åˆç‰¹å¾
        mmse_data = data.get('mmse')
        master_features, subject_summary = feature_extractor.integrate_features_with_mmse(
            task_features, roi_features, rqa_features, mmse_data
        )
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        files = feature_extractor.save_results(master_features, subject_summary, timestamp)
        
        if files:
            return jsonify({
                'success': True,
                'message': 'ç‰¹å¾æå–å®Œæˆ',
                'timestamp': timestamp,
                'files': files,
                'statistics': {
                    'total_subjects': len(subject_summary),
                    'total_tasks': len(master_features),
                    'feature_count': len(master_features.columns),
                    'summary_feature_count': len(subject_summary.columns)
                }
            })
        else:
            return jsonify({
                'success': False,
                'message': 'ç»“æœä¿å­˜å¤±è´¥'
            }), 500
            
    except Exception as e:
        print(f"âŒ ç‰¹å¾æå–é”™è¯¯: {e}")
        traceback.print_exc()
        return jsonify({
            'success': False,
            'message': f'ç‰¹å¾æå–å¤±è´¥: {str(e)}'
        }), 500

@feature_extraction_bp.route('/api/feature-extraction/history', methods=['GET'])
def get_extraction_history():
    """è·å–ç‰¹å¾æå–å†å²"""
    try:
        history = []
        
        if os.path.exists(FEATURE_RESULTS_DIR):
            files = os.listdir(FEATURE_RESULTS_DIR)
            metadata_files = [f for f in files if f.startswith('metadata_') and f.endswith('.json')]
            
            for metadata_file in metadata_files:
                try:
                    with open(os.path.join(FEATURE_RESULTS_DIR, metadata_file), 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                        history.append(metadata)
                except Exception as e:
                    print(f"è¯»å–å…ƒæ•°æ®æ–‡ä»¶å¤±è´¥ {metadata_file}: {e}")
        
        # æŒ‰æ—¶é—´æˆ³å€’åºæ’åˆ—
        history.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
        
        return jsonify({
            'success': True,
            'history': history
        })
        
    except Exception as e:
        print(f"âŒ è·å–å†å²è®°å½•é”™è¯¯: {e}")
        traceback.print_exc()
        return jsonify({
            'success': False,
            'message': f'è·å–å†å²è®°å½•å¤±è´¥: {str(e)}'
        }), 500

@feature_extraction_bp.route('/api/feature-extraction/download/<timestamp>', methods=['GET'])
def download_results(timestamp):
    """ä¸‹è½½æŒ‡å®šæ—¶é—´æˆ³çš„ç»“æœæ–‡ä»¶"""
    try:
        file_type = request.args.get('type', 'master')  # master æˆ– summary
        
        if file_type == 'master':
            filename = f'Master_Features_{timestamp}.csv'
        elif file_type == 'summary':
            filename = f'Subject_Features_Summary_{timestamp}.csv'
        else:
            return jsonify({'success': False, 'message': 'æ— æ•ˆçš„æ–‡ä»¶ç±»å‹'}), 400
        
        file_path = os.path.join(FEATURE_RESULTS_DIR, filename)
        
        if os.path.exists(file_path):
            from flask import send_file
            return send_file(file_path, as_attachment=True, download_name=filename)
        else:
            return jsonify({'success': False, 'message': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
            
    except Exception as e:
        print(f"âŒ æ–‡ä»¶ä¸‹è½½é”™è¯¯: {e}")
        traceback.print_exc()
        return jsonify({
            'success': False,
            'message': f'æ–‡ä»¶ä¸‹è½½å¤±è´¥: {str(e)}'
        }), 500

@feature_extraction_bp.route('/api/feature-extraction/preview/<timestamp>', methods=['GET'])
def preview_results(timestamp):
    """é¢„è§ˆæŒ‡å®šæ—¶é—´æˆ³çš„ç»“æœ"""
    try:
        file_type = request.args.get('type', 'master')
        limit = int(request.args.get('limit', 20))
        
        if file_type == 'master':
            filename = f'Master_Features_{timestamp}.csv'
        elif file_type == 'summary':
            filename = f'Subject_Features_Summary_{timestamp}.csv'
        else:
            return jsonify({'success': False, 'message': 'æ— æ•ˆçš„æ–‡ä»¶ç±»å‹'}), 400
        
        file_path = os.path.join(FEATURE_RESULTS_DIR, filename)
        
        if os.path.exists(file_path):
            df = pd.read_csv(file_path)
            
            # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            stats = {
                'shape': df.shape,
                'columns': df.columns.tolist(),
                'dtypes': {col: str(dtype) for col, dtype in df.dtypes.to_dict().items()},
                'missing_values': {col: int(count) for col, count in df.isnull().sum().to_dict().items()}
            }
            
            # æ•°æ®é¢„è§ˆ - å¤„ç†NaNå€¼å’Œæ•°æ®ç±»å‹
            preview_df = df.head(limit).fillna('')  # å°†NaNæ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
            preview_data = preview_df.to_dict('records')
            
            return jsonify({
                'success': True,
                'statistics': stats,
                'preview': preview_data
            })
        else:
            return jsonify({'success': False, 'message': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
            
    except Exception as e:
        print(f"âŒ é¢„è§ˆé”™è¯¯: {e}")
        traceback.print_exc()
        return jsonify({
            'success': False,
            'message': f'é¢„è§ˆå¤±è´¥: {str(e)}'
        }), 500

print("âœ… ç»¼åˆç‰¹å¾æå–ä¸æ•´åˆæ¨¡å—å·²åŠ è½½") 