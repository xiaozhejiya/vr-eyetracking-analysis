"""
æ¨¡å—9ï¼šæœºå™¨å­¦ä¹ é¢„æµ‹API
å®ç°åŸºäºçœ¼åŠ¨ç‰¹å¾çš„MMSEå­åˆ†æ•°é¢„æµ‹
"""

import os
import pandas as pd
import numpy as np
import joblib
from flask import Blueprint, request, jsonify
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import json
from datetime import datetime
import traceback

# æœºå™¨å­¦ä¹ ç›¸å…³å¯¼å…¥
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
    TENSORFLOW_AVAILABLE = True
    print("âœ… TensorFlowå·²åŠ è½½æˆåŠŸ")
except ImportError as e:
    TENSORFLOW_AVAILABLE = False
    print(f"âš ï¸ TensorFlowä¸å¯ç”¨: {e}")
    print("ğŸ’¡ å¦‚éœ€ä½¿ç”¨MLPè®­ç»ƒåŠŸèƒ½ï¼Œè¯·å®‰è£…: pip install tensorflow")

# åˆ›å»ºè“å›¾
ml_prediction_bp = Blueprint('ml_prediction', __name__)

class MLDataProcessor:
    """æœºå™¨å­¦ä¹ æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self):
        self.base_path = os.path.join(os.path.dirname(__file__), '..')
        self.module8_results_path = os.path.join(self.base_path, 'data', 'module8_analysis_results')
        self.mmse_data_path = os.path.join(self.base_path, 'data', 'MMSE_Score')
        self.module9_path = os.path.join(self.base_path, 'data', 'module9_ml_results')
        
        # ç¡®ä¿æ¨¡å—9ç›®å½•å­˜åœ¨
        os.makedirs(self.module9_path, exist_ok=True)
        
        # ç‰¹å¾æ–¹å‘é…ç½®
        self.feature_direction_config = None
        self.load_feature_direction_config()
        
        print("âœ… MLæ•°æ®å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_feature_direction_config(self):
        """åŠ è½½ç‰¹å¾æ–¹å‘é…ç½®æ–‡ä»¶"""
        config_path = os.path.join(self.module9_path, 'feature_direction_config.json')
        
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                self.feature_direction_config = json.load(f)
                print("âœ… æˆåŠŸåŠ è½½ç‰¹å¾æ–¹å‘é…ç½®æ–‡ä»¶")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°ç‰¹å¾æ–¹å‘é…ç½®æ–‡ä»¶: {config_path}")
            # è¿”å›é»˜è®¤é…ç½®
            self.feature_direction_config = {
                "feature_transforms": {
                    "game_duration": {"transform": "reciprocal"},
                    "KW_ROI_time": {"transform": "negate"},
                    "INST_ROI_time": {"transform": "negate"},
                    "BG_ROI_time": {"transform": "negate"},
                    "RR_1D": {"transform": "identity"},
                    "DET_1D": {"transform": "identity"},
                    "ENT_1D": {"transform": "identity"},
                    "RR_2D": {"transform": "identity"},
                    "DET_2D": {"transform": "identity"},
                    "ENT_2D": {"transform": "identity"}
                }
            }
    
    def apply_feature_transform(self, series, transform_method, epsilon=1e-6, outlier_percentile=(1, 99)):
        """
        åº”ç”¨ç‰¹å¾å˜æ¢ï¼Œç»Ÿä¸€æ–¹å‘ä¸º"æ•°å€¼è¶Šé«˜=è®¤çŸ¥è¶Šå¥½"
        
        Args:
            series: pandas Seriesï¼Œç‰¹å¾æ•°æ®
            transform_method: strï¼Œå˜æ¢æ–¹æ³• ('negate', 'reciprocal', 'identity')
            epsilon: floatï¼Œé¿å…é™¤é›¶çš„å°å¸¸æ•°
            outlier_percentile: tupleï¼Œå¼‚å¸¸å€¼æˆªæ–­çš„åˆ†ä½æ•°èŒƒå›´ (ä¸‹ç•Œ, ä¸Šç•Œ)
        
        Returns:
            pandas Seriesï¼Œå˜æ¢åçš„ç‰¹å¾æ•°æ®
        """
        if transform_method == "negate":
            return -series
        elif transform_method == "reciprocal":
            # ä¸“å®¶å»ºè®®ï¼šåœ¨å€’æ•°å˜æ¢å‰è¿›è¡Œå¼‚å¸¸å€¼æˆªæ–­ï¼Œé˜²æ­¢æç«¯å€¼
            if outlier_percentile:
                lower_bound = series.quantile(outlier_percentile[0] / 100.0)
                upper_bound = series.quantile(outlier_percentile[1] / 100.0)
                clipped_series = series.clip(lower=lower_bound, upper=upper_bound)
            else:
                clipped_series = series
            
            # ä½¿ç”¨å€’æ•°å˜æ¢ï¼Œæ·»åŠ å°å¸¸æ•°é¿å…é™¤é›¶
            return 1.0 / (clipped_series + epsilon)
        elif transform_method == "identity":
            return series
        else:
            print(f"âš ï¸ æœªçŸ¥çš„å˜æ¢æ–¹æ³•: {transform_method}ï¼Œä½¿ç”¨identity")
            return series
    
    def validate_feature_directions(self, df, feature_cols, mmse_cols):
        """
        éªŒè¯ç‰¹å¾æ–¹å‘ä¸€è‡´æ€§ - æ‰€æœ‰ç‰¹å¾ä¸MMSEæ€»åˆ†åº”æ­£ç›¸å…³
        
        Args:
            df: DataFrameï¼ŒåŒ…å«ç‰¹å¾å’ŒMMSEæ•°æ®
            feature_cols: listï¼Œç‰¹å¾åˆ—å
            mmse_cols: listï¼ŒMMSEå­åˆ†æ•°åˆ—å
        
        Returns:
            dictï¼ŒéªŒè¯ç»“æœ
        """
        # è®¡ç®—MMSEæ€»åˆ†
        mmse_total = df[mmse_cols].sum(axis=1)
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾ä¸MMSEæ€»åˆ†çš„ç›¸å…³æ€§
        correlations = {}
        negative_features = []
        
        for col in feature_cols:
            if col in df.columns:
                corr = df[col].corr(mmse_total)
                correlations[col] = corr
                if corr < 0:
                    negative_features.append(col)
        
        validation_result = {
            'correlations': correlations,
            'negative_features': negative_features,
            'all_positive': len(negative_features) == 0,
            'mmse_total_stats': {
                'mean': float(mmse_total.mean()),
                'std': float(mmse_total.std()),
                'min': float(mmse_total.min()),
                'max': float(mmse_total.max())
            }
        }
        
        return validation_result
    
    def get_available_rqa_configs(self):
        """è·å–å¯ç”¨çš„RQAé…ç½®åˆ—è¡¨"""
        try:
            if not os.path.exists(self.module8_results_path):
                return []
            
            configs = []
            for config_dir in os.listdir(self.module8_results_path):
                config_path = os.path.join(self.module8_results_path, config_dir)
                if os.path.isdir(config_path):
                    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸ªäººå¯¹æ¯”æ–‡ä»¶
                    individual_files = [f for f in os.listdir(config_path) 
                                      if f.startswith('individual_comparison_')]
                    if individual_files:
                        configs.append({
                            'id': config_dir,
                            'display_name': self._format_config_display_name(config_dir),
                            'file_count': len(individual_files)
                        })
            
            return sorted(configs, key=lambda x: x['id'])
            
        except Exception as e:
            print(f"âŒ è·å–RQAé…ç½®å¤±è´¥: {str(e)}")
            return []
    
    def _format_config_display_name(self, config_id):
        """æ ¼å¼åŒ–é…ç½®æ˜¾ç¤ºåç§°"""
        try:
            # è§£æç±»ä¼¼ m2_tau1_eps0.055_lmin2 çš„æ ¼å¼
            parts = config_id.split('_')
            display_parts = []
            
            for part in parts:
                if part.startswith('m'):
                    display_parts.append(f"m={part[1:]}")
                elif part.startswith('tau'):
                    display_parts.append(f"Ï„={part[3:]}")
                elif part.startswith('eps'):
                    display_parts.append(f"Îµ={part[3:]}")
                elif part.startswith('lmin'):
                    display_parts.append(f"l_min={part[4:]}")
            
            return ", ".join(display_parts)
        except:
            return config_id
    
    def load_eye_movement_data(self, rqa_config):
        """åŠ è½½æ¨¡å—8çš„çœ¼åŠ¨ç‰¹å¾æ•°æ®"""
        try:
            config_path = os.path.join(self.module8_results_path, rqa_config)
            if not os.path.exists(config_path):
                raise FileNotFoundError(f"RQAé…ç½®ç›®å½•ä¸å­˜åœ¨: {rqa_config}")
            
            # æŸ¥æ‰¾æœ€æ–°çš„individual_comparisonæ–‡ä»¶
            individual_files = [f for f in os.listdir(config_path) 
                              if f.startswith('individual_comparison_')]
            
            if not individual_files:
                raise FileNotFoundError(f"æœªæ‰¾åˆ°ä¸ªäººå¯¹æ¯”æ•°æ®æ–‡ä»¶")
            
            # é€‰æ‹©æœ€æ–°çš„æ–‡ä»¶
            latest_file = sorted(individual_files)[-1]
            file_path = os.path.join(config_path, latest_file)
            
            print(f"ğŸ“‚ åŠ è½½çœ¼åŠ¨æ•°æ®æ–‡ä»¶: {latest_file}")
            
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(file_path)
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—
            required_columns = ['Subject_ID', 'Task_ID', 'Group_Type', 'Eye_Movement_Coefficient']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                raise ValueError(f"ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")
            
            print(f"âœ… æˆåŠŸåŠ è½½çœ¼åŠ¨æ•°æ®: {len(df)} æ¡è®°å½•")
            print(f"ğŸ“Š æ•°æ®åˆ—: {list(df.columns)}")
            
            return df, latest_file
            
        except Exception as e:
            print(f"âŒ åŠ è½½çœ¼åŠ¨æ•°æ®å¤±è´¥: {str(e)}")
            raise e
    
    def load_mmse_data(self):
        """åŠ è½½MMSEå­åˆ†æ•°æ•°æ®"""
        try:
            mmse_files = {
                'control': 'æ§åˆ¶ç»„.csv',
                'mci': 'è½»åº¦è®¤çŸ¥éšœç¢ç»„.csv', 
                'ad': 'é˜¿å°”å…¹æµ·é»˜ç—‡ç»„.csv'
            }
            
            all_mmse_data = []
            
            for group_type, filename in mmse_files.items():
                file_path = os.path.join(self.mmse_data_path, filename)
                
                if not os.path.exists(file_path):
                    print(f"âš ï¸ MMSEæ–‡ä»¶ä¸å­˜åœ¨: {filename}")
                    continue
                
                df = pd.read_csv(file_path)
                print(f"ğŸ“‚ åŠ è½½MMSEæ•°æ®: {filename}, è®°å½•æ•°: {len(df)}")
                
                # æ·»åŠ ç»„åˆ«æ ‡è¯†(ä¿æŒä¸çœ¼åŠ¨æ•°æ®ä¸€è‡´çš„å°å†™æ ¼å¼)
                df['Group_Type'] = group_type.lower()
                
                # å¤„ç†å—è¯•è€…IDåˆ—ï¼ˆå¯èƒ½æ˜¯'è¯•è€…'æˆ–'å—è¯•è€…'ï¼‰
                subject_col = None
                for col in ['è¯•è€…', 'å—è¯•è€…', 'Subject_ID']:
                    if col in df.columns:
                        subject_col = col
                        break
                
                if subject_col:
                    df['Subject_ID'] = df[subject_col]
                else:
                    raise ValueError(f"åœ¨{filename}ä¸­æœªæ‰¾åˆ°å—è¯•è€…IDåˆ—")
                
                all_mmse_data.append(df)
            
            if not all_mmse_data:
                raise FileNotFoundError("æœªæ‰¾åˆ°ä»»ä½•MMSEæ•°æ®æ–‡ä»¶")
            
            # åˆå¹¶æ‰€æœ‰ç»„çš„æ•°æ®
            combined_mmse = pd.concat(all_mmse_data, ignore_index=True)
            
            print(f"âœ… æˆåŠŸåŠ è½½MMSEæ•°æ®: {len(combined_mmse)} æ¡è®°å½•")
            print(f"ğŸ“Š æ•°æ®åˆ—: {list(combined_mmse.columns)}")
            
            return combined_mmse
            
        except Exception as e:
            print(f"âŒ åŠ è½½MMSEæ•°æ®å¤±è´¥: {str(e)}")
            raise e
    
    def extract_mmse_subscores(self, mmse_df):
        """æå–MMSEäº”ä¸ªå­åˆ†æ•°"""
        try:
            # å®šä¹‰MMSEå­åˆ†æ•°çš„åˆ—æ˜ å°„å’Œæƒé‡
            subscore_mapping = {
                'Q1': {  # æ—¶é—´å®šå‘ (5åˆ†æ€»è®¡)
                    'å¹´ä»½': 1, 'å­£èŠ‚': 1, 'æœˆä»½': 1, 'æ˜ŸæœŸ': 2
                },
                'Q2': {  # ç©ºé—´å®šå‘ (5åˆ†æ€»è®¡) 
                    'çœå¸‚åŒº': 2, 'è¡—é“': 1, 'å»ºç­‘': 1, 'æ¥¼å±‚': 1
                },
                'Q3': {  # å³åˆ»è®°å¿† (3åˆ†)
                    'å³åˆ»è®°å¿†': 3
                },
                'Q4': {  # æ³¨æ„åŠ›ä¸è®¡ç®— (5åˆ†)
                    '100-7': 1, '93-7': 1, '86-7': 1, '79-7': 1, '72-7': 1
                },
                'Q5': {  # å»¶è¿Ÿå›å¿† (3åˆ†)
                    'è¯1': 1, 'è¯2': 1, 'è¯3': 1
                }
            }
            
            # ä¸ºæ¯ä¸ªå—è¯•è€…è®¡ç®—å­åˆ†æ•°
            processed_data = []
            
            for _, row in mmse_df.iterrows():
                subject_data = {
                    'Subject_ID': row['Subject_ID'],
                    'Group_Type': row['Group_Type']
                }
                
                # è®¡ç®—æ¯ä¸ªå­åˆ†æ•°(æŒ‰æƒé‡)
                for task_id, column_weights in subscore_mapping.items():
                    subscore = 0
                    for col, weight in column_weights.items():
                        if col in mmse_df.columns:
                            subscore += row.get(col, 0) * weight
                    subject_data[f'{task_id}_subscore'] = subscore
                
                processed_data.append(subject_data)
            
            result_df = pd.DataFrame(processed_data)
            
            print(f"âœ… æˆåŠŸæå–MMSEå­åˆ†æ•°")
            print(f"ğŸ“Š å­åˆ†æ•°åˆ—: {[col for col in result_df.columns if 'subscore' in col]}")
            print(f"ğŸ“‹ MMSE Subject_ID ç¤ºä¾‹: {result_df['Subject_ID'].head().tolist()}")
            
            return result_df
            
        except Exception as e:
            print(f"âŒ æå–MMSEå­åˆ†æ•°å¤±è´¥: {str(e)}")
            raise e
    
    def create_subject_aggregated_features(self, eye_movement_df):
        """åˆ›å»ºå—è¯•è€…çº§åˆ«çš„èšåˆçœ¼åŠ¨ç‰¹å¾ï¼ˆæ”¯æŒç¼ºå¤±ä»»åŠ¡å®¹é”™ï¼‰"""
        try:
            # ä¸“å®¶å»ºè®®ï¼šå¢å¼ºç¼ºå¤±ä»»åŠ¡å®¹é”™å¤„ç†
            expected_tasks = ['Q1', 'Q2', 'Q3', 'Q4', 'Q5']
            
            # æ£€æµ‹ç¼ºå¤±ä»»åŠ¡
            missing_task_info = []
            for subject_id in eye_movement_df['Subject_ID'].unique():
                subject_data = eye_movement_df[eye_movement_df['Subject_ID'] == subject_id]
                actual_tasks = set(subject_data['Task_ID'].unique())
                missing_tasks = set(expected_tasks) - actual_tasks
                
                if missing_tasks:
                    missing_task_info.append({
                        'Subject_ID': subject_id,
                        'missing_tasks': list(missing_tasks),
                        'missing_count': len(missing_tasks)
                    })
            
            if missing_task_info:
                print(f"âš ï¸ å‘ç° {len(missing_task_info)} ä¸ªå—è¯•è€…æœ‰ç¼ºå¤±ä»»åŠ¡:")
                for info in missing_task_info[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"  {info['Subject_ID']}: ç¼ºå¤± {info['missing_tasks']}")
                if len(missing_task_info) > 5:
                    print(f"  ... è¿˜æœ‰ {len(missing_task_info) - 5} ä¸ª")
            
            # æŒ‰å—è¯•è€…èšåˆç‰¹å¾ï¼ˆä»…ä½¿ç”¨ç°æœ‰ä»»åŠ¡ï¼‰
            subject_features = eye_movement_df.groupby(['Subject_ID', 'Group_Type']).agg({
                'Eye_Movement_Coefficient': ['mean', 'std', 'min', 'max']
            }).reset_index()
            
            # å±•å¹³åˆ—å
            subject_features.columns = ['Subject_ID', 'Group_Type', 
                                      'eye_coeff_mean', 'eye_coeff_std', 
                                      'eye_coeff_min', 'eye_coeff_max']
            
            # å¡«å……NaNå€¼ï¼ˆå¦‚æœåªæœ‰ä¸€ä¸ªä»»åŠ¡çš„è¯ï¼Œstdä¼šæ˜¯NaNï¼‰
            subject_features['eye_coeff_std'] = subject_features['eye_coeff_std'].fillna(0)
            
            # æ·»åŠ ä»»åŠ¡è®¡æ•°ç‰¹å¾
            task_counts = eye_movement_df.groupby('Subject_ID')['Task_ID'].nunique().reset_index()
            task_counts.columns = ['Subject_ID', 'task_count']
            
            subject_features = subject_features.merge(task_counts, on='Subject_ID')
            
            # ä¸“å®¶å»ºè®®ï¼šæ·»åŠ ç¼ºå¤±ä»»åŠ¡æ ‡å¿—ç‰¹å¾
            missing_task_flags = pd.DataFrame(missing_task_info) if missing_task_info else pd.DataFrame(columns=['Subject_ID', 'missing_count'])
            if not missing_task_flags.empty:
                missing_task_flags = missing_task_flags[['Subject_ID', 'missing_count']]
            else:
                # ä¸ºæ‰€æœ‰å—è¯•è€…åˆ›å»ºç¼ºå¤±è®¡æ•°ä¸º0çš„è®°å½•
                missing_task_flags = pd.DataFrame({
                    'Subject_ID': subject_features['Subject_ID'],
                    'missing_count': 0
                })
            
            subject_features = subject_features.merge(missing_task_flags, on='Subject_ID', how='left')
            subject_features['missing_count'] = subject_features['missing_count'].fillna(0)
            
            # æ·»åŠ ç¼ºå¤±ä»»åŠ¡æ ‡å¿—ï¼ˆäºŒè¿›åˆ¶ç‰¹å¾ï¼‰
            subject_features['flag_missing_task'] = (subject_features['missing_count'] > 0).astype(int)
            
            # è®¡ç®—æ›´å¤šèšåˆç‰¹å¾
            task_features = eye_movement_df.pivot_table(
                index=['Subject_ID', 'Group_Type'], 
                columns='Task_ID', 
                values='Eye_Movement_Coefficient',
                fill_value=0  # ç¼ºå¤±ä»»åŠ¡ç”¨0å¡«å……
            ).reset_index()
            
            # é‡å‘½åä»»åŠ¡ç‰¹å¾åˆ—
            task_feature_cols = {}
            for col in task_features.columns:
                if col not in ['Subject_ID', 'Group_Type']:
                    task_feature_cols[col] = f'eye_coeff_{col.lower()}'
            
            task_features = task_features.rename(columns=task_feature_cols)
            
            # åˆå¹¶æ‰€æœ‰ç‰¹å¾
            final_features = subject_features.merge(task_features, on=['Subject_ID', 'Group_Type'])
            
            print(f"âœ… æˆåŠŸåˆ›å»ºå—è¯•è€…èšåˆç‰¹å¾")
            print(f"ğŸ“Š ç‰¹å¾æ•°é‡: {len(final_features.columns) - 2}")  # å‡å»IDå’ŒGroupåˆ—
            print(f"ğŸ‘¥ å—è¯•è€…æ•°é‡: {len(final_features)}")
            print(f"ğŸ“‹ çœ¼åŠ¨æ•°æ® Subject_ID ç¤ºä¾‹: {final_features['Subject_ID'].head().tolist()}")
            
            # è¯¦ç»†çš„å—è¯•è€…åˆ†å¸ƒç»Ÿè®¡
            group_distribution = final_features['Group_Type'].value_counts()
            print(f"ğŸ“Š å—è¯•è€…ç»„åˆ«åˆ†å¸ƒ:")
            for group, count in group_distribution.items():
                print(f"  {group}: {count} å—è¯•è€…")
            
            # ç¼ºå¤±ä»»åŠ¡ç»Ÿè®¡
            if missing_task_info:
                print(f"âš ï¸ ç¼ºå¤±ä»»åŠ¡ç»Ÿè®¡:")
                print(f"  æœ‰ç¼ºå¤±ä»»åŠ¡çš„å—è¯•è€…: {len(missing_task_info)}")
                print(f"  å®Œæ•´ä»»åŠ¡çš„å—è¯•è€…: {len(final_features) - len(missing_task_info)}")
                missing_counts = final_features['missing_count'].value_counts().sort_index()
                for count, num_subjects in missing_counts.items():
                    if count > 0:
                        print(f"  ç¼ºå¤±{count}ä¸ªä»»åŠ¡: {num_subjects}åå—è¯•è€…")
                        
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤æ•°æ®å¯¼è‡´çš„æ ·æœ¬å‡å°‘
            print(f"âš ï¸ æ•°æ®èšåˆè¯´æ˜: åŸå§‹çœ¼åŠ¨æ•°æ®æŒ‰Subject_IDèšåˆä¸ºå—è¯•è€…çº§åˆ«ç‰¹å¾")
            print(f"   æ¯ä¸ªå—è¯•è€…å¯èƒ½æœ‰å¤šä¸ªä»»åŠ¡è®°å½•ï¼Œèšåˆåå˜ä¸º1ä¸ªå—è¯•è€…1æ¡è®°å½•")
            print(f"   ç¼ºå¤±ä»»åŠ¡é€šè¿‡0å¡«å……å’Œæ ‡å¿—å˜é‡è¿›è¡Œå¤„ç†")
            
            return final_features
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºèšåˆç‰¹å¾å¤±è´¥: {str(e)}")
            raise e
    
    def convert_eye_movement_id_to_mmse_id(self, subject_id, group_type):
        """å°†çœ¼åŠ¨æ•°æ®çš„Subject_IDè½¬æ¢ä¸ºMMSEæ•°æ®æ ¼å¼"""
        try:
            # å»æ‰å¯èƒ½çš„'q'åç¼€
            base_id = subject_id
            if base_id.endswith('q'):
                base_id = base_id[:-1]
            
            if group_type.lower() == 'control':
                # n1 -> n01, n2 -> n02, ..., n20 -> n20
                if base_id.startswith('n'):
                    num = int(base_id[1:])
                    if 1 <= num <= 20:
                        return f"n{num:02d}"
                    else:
                        print(f"âš ï¸ Controlç»„IDè¶…å‡ºèŒƒå›´: {subject_id} (æœŸæœ›1-20)")
                        return None
            elif group_type.lower() == 'mci':
                # m1 -> M01, m2 -> M02, ..., m20 -> M20 (æ³¨æ„å¤§å†™M)
                if base_id.startswith('m'):
                    num = int(base_id[1:])
                    if 1 <= num <= 20:
                        return f"M{num:02d}"
                    else:
                        print(f"âš ï¸ MCIç»„IDè¶…å‡ºèŒƒå›´: {subject_id} (æœŸæœ›1-20)")
                        return None
            elif group_type.lower() == 'ad':
                # ad3 -> ad01, ad4 -> ad02, ..., ad22 -> ad20 (çœ¼åŠ¨æ•°æ®ä»3å¼€å§‹åˆ°22ï¼ŒMMSEä»1å¼€å§‹åˆ°20ï¼Œoffset=-2)
                if base_id.startswith('ad'):
                    num = int(base_id[2:])
                    if 3 <= num <= 22:
                        mmse_num = num - 2  # ad3->ad01, ad4->ad02, ..., ad22->ad20
                        return f"ad{mmse_num:02d}"
                    else:
                        print(f"âš ï¸ ADç»„IDè¶…å‡ºèŒƒå›´: {subject_id} (æœŸæœ›3-22)")
                        return None
            
            print(f"âš ï¸ æ— æ³•è¯†åˆ«çš„IDæ ¼å¼: {subject_id} ({group_type})")
            return None
        except Exception as e:
            print(f"âš ï¸ IDè½¬æ¢å¤±è´¥: {subject_id} -> {str(e)}")
            return None
    
    def merge_eye_movement_and_mmse(self, eye_features_df, mmse_subscores_df):
        """åˆå¹¶çœ¼åŠ¨ç‰¹å¾å’ŒMMSEå­åˆ†æ•°"""
        try:
            # è½¬æ¢çœ¼åŠ¨æ•°æ®çš„Subject_IDæ ¼å¼ä»¥åŒ¹é…MMSEæ•°æ®
            eye_features_df_copy = eye_features_df.copy()
            eye_features_df_copy['MMSE_Subject_ID'] = eye_features_df_copy.apply(
                lambda row: self.convert_eye_movement_id_to_mmse_id(row['Subject_ID'], row['Group_Type']), 
                axis=1
            )
            
            print(f"ğŸ“‹ IDè½¬æ¢ç¤ºä¾‹:")
            for i in range(min(10, len(eye_features_df_copy))):
                row = eye_features_df_copy.iloc[i]
                print(f"  {row['Subject_ID']} ({row['Group_Type']}) -> {row['MMSE_Subject_ID']}")
            
            # è¿‡æ»¤æ‰è½¬æ¢å¤±è´¥çš„è®°å½•ï¼ˆMMSE_Subject_IDä¸ºNoneï¼‰
            valid_conversions = eye_features_df_copy['MMSE_Subject_ID'].notna()
            eye_features_valid = eye_features_df_copy[valid_conversions]
            
            print(f"ğŸ“Š è½¬æ¢ç»Ÿè®¡:")
            print(f"  æ€»çœ¼åŠ¨è®°å½•: {len(eye_features_df_copy)}")
            print(f"  æˆåŠŸè½¬æ¢: {len(eye_features_valid)}")
            print(f"  è½¬æ¢å¤±è´¥: {len(eye_features_df_copy) - len(eye_features_valid)}")
            
            if len(eye_features_valid) == 0:
                print("âŒ æ²¡æœ‰ä»»ä½•çœ¼åŠ¨æ•°æ®IDèƒ½å¤Ÿè½¬æ¢ä¸ºMMSEæ ¼å¼")
                return pd.DataFrame()
            
            # æ˜¾ç¤ºå¾…åˆå¹¶çš„æ•°æ®æ ·æœ¬
            print(f"ğŸ“‹ å¾…åˆå¹¶çš„çœ¼åŠ¨æ•°æ®æ ·æœ¬ (MMSE_Subject_ID):")
            sample_eye_ids = eye_features_valid['MMSE_Subject_ID'].unique()[:10]
            for group in ['control', 'mci', 'ad']:
                group_sample = eye_features_valid[eye_features_valid['Group_Type'] == group]['MMSE_Subject_ID'].unique()[:5]
                print(f"  {group}: {list(group_sample)}")
            
            print(f"ğŸ“‹ MMSEæ•°æ®æ ·æœ¬ (Subject_ID):")
            for group in ['control', 'mci', 'ad']:
                group_sample = mmse_subscores_df[mmse_subscores_df['Group_Type'] == group]['Subject_ID'].unique()[:5]
                print(f"  {group}: {list(group_sample)}")
            
            # åŸºäºè½¬æ¢åçš„Subject_IDå’ŒGroup_Typeåˆå¹¶æ•°æ®
            merged_df = eye_features_valid.merge(
                mmse_subscores_df, 
                left_on=['MMSE_Subject_ID', 'Group_Type'],
                right_on=['Subject_ID', 'Group_Type'],
                how='inner'
            )
            
            # ä¿ç•™åŸå§‹çš„Subject_IDï¼ˆçœ¼åŠ¨æ•°æ®æ ¼å¼ï¼‰
            if len(merged_df) > 0:
                merged_df['Subject_ID'] = merged_df['Subject_ID_x']
                merged_df = merged_df.drop(['Subject_ID_x', 'Subject_ID_y', 'MMSE_Subject_ID'], axis=1)
            
            print(f"âœ… æˆåŠŸåˆå¹¶çœ¼åŠ¨ç‰¹å¾å’ŒMMSEæ•°æ®")
            print(f"ğŸ‘¥ åˆå¹¶åå—è¯•è€…æ•°é‡: {len(merged_df)}")
            print(f"ğŸ“Š æ€»åˆ—æ•°: {len(merged_df.columns)}")
            
            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            missing_data = merged_df.isnull().sum()
            if missing_data.any():
                print("âš ï¸ å‘ç°ç¼ºå¤±æ•°æ®:")
                for col, count in missing_data[missing_data > 0].items():
                    print(f"  {col}: {count} ç¼ºå¤±å€¼")
            
            return merged_df
            
        except Exception as e:
            print(f"âŒ åˆå¹¶æ•°æ®å¤±è´¥: {str(e)}")
            raise e
    
    def prepare_ml_dataset(self, merged_df):
        """å‡†å¤‡æœºå™¨å­¦ä¹ æ•°æ®é›†"""
        try:
            # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡å˜é‡
            feature_columns = [col for col in merged_df.columns 
                             if col not in ['Subject_ID', 'Group_Type'] 
                             and not col.endswith('_subscore')]
            
            target_columns = [col for col in merged_df.columns if col.endswith('_subscore')]
            
            print(f"ğŸ“Š ç‰¹å¾åˆ— ({len(feature_columns)}): {feature_columns}")
            print(f"ğŸ¯ ç›®æ ‡åˆ— ({len(target_columns)}): {target_columns}")
            
            # æå–ç‰¹å¾å’Œç›®æ ‡
            X = merged_df[feature_columns].copy()
            y = merged_df[target_columns].copy()
            
            # æ£€æŸ¥å¹¶å¤„ç†æ— é™å€¼å’Œç¼ºå¤±å€¼
            X = X.replace([np.inf, -np.inf], np.nan)
            X = X.fillna(X.mean())
            
            y = y.replace([np.inf, -np.inf], np.nan)
            y = y.fillna(y.mean())
            
            # åº”ç”¨ç‰¹å¾æ–¹å‘ä¸€è‡´æ€§æ ¡æ­£
            print(f"ğŸ”„ å¼€å§‹ç‰¹å¾æ–¹å‘ä¸€è‡´æ€§æ ¡æ­£...")
            X_transformed = X.copy()
            
            if self.feature_direction_config and 'feature_transforms' in self.feature_direction_config:
                transforms = self.feature_direction_config['feature_transforms']
                applied_transforms = []
                
                for feature_col in feature_columns:
                    if feature_col in transforms:
                        transform_method = transforms[feature_col]['transform']
                        original_values = X_transformed[feature_col].copy()
                        
                        # åº”ç”¨å˜æ¢
                        X_transformed[feature_col] = self.apply_feature_transform(
                            X_transformed[feature_col], transform_method
                        )
                        
                        # è®°å½•å˜æ¢ä¿¡æ¯
                        applied_transforms.append({
                            'feature': feature_col,
                            'method': transform_method,
                            'original_mean': float(original_values.mean()),
                            'transformed_mean': float(X_transformed[feature_col].mean()),
                            'original_std': float(original_values.std()),
                            'transformed_std': float(X_transformed[feature_col].std())
                        })
                        
                        print(f"  âœ… {feature_col}: {transform_method} å˜æ¢")
                    else:
                        print(f"  âš ï¸ {feature_col}: æœªé…ç½®å˜æ¢æ–¹æ³•ï¼Œä½¿ç”¨identity")
                
                print(f"âœ… ç‰¹å¾æ–¹å‘æ ¡æ­£å®Œæˆï¼Œå…±å¤„ç† {len(applied_transforms)} ä¸ªç‰¹å¾")
            else:
                print(f"âš ï¸ æœªåŠ è½½ç‰¹å¾æ–¹å‘é…ç½®ï¼Œè·³è¿‡æ–¹å‘æ ¡æ­£")
                applied_transforms = []
            
            # æ ‡å‡†åŒ–ç‰¹å¾ (ä½¿ç”¨å˜æ¢åçš„ç‰¹å¾)
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X_transformed)
            X_scaled_df = pd.DataFrame(X_scaled, columns=feature_columns, index=X.index)
            
            print(f"âœ… ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ")
            print(f"ğŸ“ˆ ç‰¹å¾èŒƒå›´: {X_scaled.min():.3f} åˆ° {X_scaled.max():.3f}")
            
            # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled_df, y, 
                test_size=0.2, 
                random_state=42,
                stratify=merged_df['Group_Type']  # æŒ‰ç»„åˆ«åˆ†å±‚æŠ½æ ·
            )
            
            # ä¸“å®¶å»ºè®®ï¼šåªç”¨è®­ç»ƒé›†åšæ–¹å‘éªŒè¯ï¼ˆé¿å…æµ‹è¯•é›†ä¿¡æ¯æ³„æ¼ï¼‰
            validation_result = None
            if self.feature_direction_config and 'feature_transforms' in self.feature_direction_config:
                # åœ¨è®­ç»ƒé›†ä¸ŠéªŒè¯ç‰¹å¾æ–¹å‘ä¸€è‡´æ€§
                print(f"ğŸ” ç‰¹å¾æ–¹å‘éªŒè¯ï¼ˆä»…åŸºäºè®­ç»ƒé›†ï¼‰:")
                validation_result = self.validate_feature_directions(
                    pd.concat([X_train, y_train], axis=1), 
                    feature_columns, 
                    [col for col in y_train.columns]
                )
                
                if validation_result['all_positive']:
                    print(f"  âœ… æ‰€æœ‰ç‰¹å¾ä¸MMSEæ€»åˆ†æ­£ç›¸å…³")
                else:
                    print(f"  âš ï¸ å‘ç°è´Ÿç›¸å…³ç‰¹å¾: {validation_result['negative_features']}")
                
                # æ˜¾ç¤ºç›¸å…³æ€§ä¿¡æ¯
                for feature, corr in validation_result['correlations'].items():
                    status = "âœ…" if corr >= 0 else "âŒ"
                    print(f"    {status} {feature}: {corr:.3f}")
            else:
                validation_result = None
            
            print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆ")
            print(f"ğŸš‚ è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
            print(f"ğŸ§ª æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
            
            # è¯¦ç»†çš„ç‰¹å¾å’Œç›®æ ‡ä¿¡æ¯
            print(f"ğŸ” ç‰¹å¾æ ‡å‡†åŒ–æƒ…å†µ:")
            print(f"  ç‰¹å¾æ•°é‡: {len(feature_columns)}")
            print(f"  æ ‡å‡†åŒ–æ–¹æ³•: Z-score (StandardScaler)")
            print(f"  æ ‡å‡†åŒ–åç‰¹å¾èŒƒå›´: [{X_scaled.min():.3f}, {X_scaled.max():.3f}]")
            
            print(f"ğŸ¯ MMSEå­åˆ†æ•°è¯¦æƒ…:")
            max_scores = {'Q1_subscore': 5, 'Q2_subscore': 5, 'Q3_subscore': 3, 'Q4_subscore': 5, 'Q5_subscore': 3}
            for i, col in enumerate(target_columns, 1):
                subscore_info = {
                    'Q1_subscore': 'æ—¶é—´å®šå‘(Q1) [å¹´ä»½1+å­£èŠ‚1+æœˆä»½1+æ˜ŸæœŸ2=5åˆ†]',
                    'Q2_subscore': 'ç©ºé—´å®šå‘(Q2) [çœå¸‚åŒº2+è¡—é“1+å»ºç­‘1+æ¥¼å±‚1=5åˆ†]', 
                    'Q3_subscore': 'å³æ—¶è®°å¿†(Q3) [å³åˆ»è®°å¿†3åˆ†]',
                    'Q4_subscore': 'æ³¨æ„/è®¡ç®—(Q4) [è¿ç»­å‡æ³•5Ã—1=5åˆ†]',
                    'Q5_subscore': 'å»¶è¿Ÿå›å¿†(Q5) [è¯æ±‡å›å¿†3Ã—1=3åˆ†]'
                }.get(col, col)
                max_score = max_scores.get(col, '?')
                print(f"  {subscore_info}")
                print(f"    å®é™…: å‡å€¼={y[col].mean():.2f}, æ ‡å‡†å·®={y[col].std():.2f}, èŒƒå›´=[{y[col].min():.0f}, {y[col].max():.0f}], æ»¡åˆ†={max_score}")
            
            # è®¡ç®—æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
            dataset_stats = {
                'total_samples': len(merged_df),
                'train_samples': len(X_train),
                'test_samples': len(X_test),
                'feature_count': len(feature_columns),
                'target_count': len(target_columns),
                'group_distribution': merged_df['Group_Type'].value_counts().to_dict(),
                'feature_names': feature_columns,
                'target_names': target_columns,
                'target_stats': {
                    col: {
                        'mean': float(y[col].mean()),
                        'std': float(y[col].std()),
                        'min': float(y[col].min()),
                        'max': float(y[col].max())
                    } for col in target_columns
                }
            }
            
            return {
                'X_train': X_train,
                'X_test': X_test,
                'y_train': y_train,
                'y_test': y_test,
                'scaler': scaler,
                'merged_data': merged_df,
                'stats': dataset_stats,
                'applied_transforms': applied_transforms if 'applied_transforms' in locals() else [],
                'validation_result': validation_result if 'validation_result' in locals() else None
            }
            
        except Exception as e:
            print(f"âŒ å‡†å¤‡MLæ•°æ®é›†å¤±è´¥: {str(e)}")
            raise e
    
    def save_preprocessed_data(self, dataset, rqa_config):
        """ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®"""
        try:
            # åˆ›å»ºé…ç½®ä¸“ç”¨ç›®å½•
            config_dir = os.path.join(self.module9_path, rqa_config)
            os.makedirs(config_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ä¿å­˜è®­ç»ƒé›†å’Œæµ‹è¯•é›†
            train_data = pd.concat([dataset['X_train'], dataset['y_train']], axis=1)
            test_data = pd.concat([dataset['X_test'], dataset['y_test']], axis=1)
            
            train_file = os.path.join(config_dir, f'train_dataset_{timestamp}.csv')
            test_file = os.path.join(config_dir, f'test_dataset_{timestamp}.csv')
            merged_file = os.path.join(config_dir, f'merged_dataset_{timestamp}.csv')
            stats_file = os.path.join(config_dir, f'dataset_stats_{timestamp}.json')
            
            train_data.to_csv(train_file, index=False)
            test_data.to_csv(test_file, index=False)
            dataset['merged_data'].to_csv(merged_file, index=False)
            
            # ä¿å­˜å¢å¼ºçš„ç»Ÿè®¡ä¿¡æ¯
            stats_data = dataset['stats'].copy()
            
            # æ·»åŠ ç‰¹å¾å˜æ¢å’ŒéªŒè¯ä¿¡æ¯
            if 'applied_transforms' in dataset and dataset['applied_transforms']:
                stats_data['feature_transforms'] = dataset['applied_transforms']
            
            if 'validation_result' in dataset and dataset['validation_result']:
                stats_data['direction_validation'] = dataset['validation_result']
            
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats_data, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜StandardScaler
            scaler_file = os.path.join(config_dir, f'scaler_{timestamp}.pkl')
            joblib.dump(dataset['scaler'], scaler_file)
            
            # ä¿å­˜ç‰¹å¾æ–¹å‘é…ç½®æ–‡ä»¶å‰¯æœ¬
            config_copy_file = os.path.join(config_dir, f'feature_direction_config_{timestamp}.json')
            if self.feature_direction_config:
                with open(config_copy_file, 'w', encoding='utf-8') as f:
                    json.dump(self.feature_direction_config, f, indent=2, ensure_ascii=False)
            
            # åˆ›å»ºæœ€æ–°æ–‡ä»¶çš„å‰¯æœ¬ï¼ˆç”¨äºè®­ç»ƒï¼‰
            latest_scaler = os.path.join(config_dir, 'latest_scaler.pkl')
            latest_config = os.path.join(config_dir, 'latest_feature_config.json')
            
            try:
                # åˆ é™¤æ—§çš„æ–‡ä»¶
                if os.path.exists(latest_scaler):
                    os.remove(latest_scaler)
                if os.path.exists(latest_config):
                    os.remove(latest_config)
                
                # å¤åˆ¶æœ€æ–°æ–‡ä»¶
                import shutil
                shutil.copy2(scaler_file, latest_scaler)
                if self.feature_direction_config:
                    shutil.copy2(config_copy_file, latest_config)
                
            except Exception as e:
                print(f"âš ï¸ åˆ›å»ºæœ€æ–°æ–‡ä»¶å‰¯æœ¬å¤±è´¥: {e}")
            
            print(f"ğŸ’¾ æ•°æ®ä¿å­˜å®Œæˆ:")
            print(f"  è®­ç»ƒé›†: {train_file}")
            print(f"  æµ‹è¯•é›†: {test_file}")
            print(f"  åˆå¹¶æ•°æ®: {merged_file}")
            print(f"  ç»Ÿè®¡ä¿¡æ¯: {stats_file}")
            print(f"  æ ‡å‡†åŒ–å™¨: {scaler_file}")
            print(f"  ç‰¹å¾é…ç½®: {config_copy_file}")
            
            return {
                'train_file': train_file,
                'test_file': test_file,
                'merged_file': merged_file,
                'stats_file': stats_file,
                'scaler_file': scaler_file,
                'config_file': config_copy_file,
                'latest_scaler': latest_scaler,
                'latest_config': latest_config
            }
            
        except Exception as e:
            print(f"âŒ ä¿å­˜é¢„å¤„ç†æ•°æ®å¤±è´¥: {str(e)}")
            raise e

# åˆ›å»ºå…¨å±€å¤„ç†å™¨å®ä¾‹
ml_processor = MLDataProcessor()

@ml_prediction_bp.route('/api/ml/available-configs', methods=['GET'])
def get_available_configs():
    """è·å–å¯ç”¨çš„RQAé…ç½®"""
    try:
        configs = ml_processor.get_available_rqa_configs()
        return jsonify({
            'success': True,
            'configs': configs,
            'count': len(configs)
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@ml_prediction_bp.route('/api/ml/preprocess-data', methods=['POST'])
def preprocess_data():
    """æ‰§è¡Œæ•°æ®é¢„å¤„ç†ï¼ˆå­æ¨¡å—9.1ï¼‰"""
    try:
        data = request.get_json()
        rqa_config = data.get('rqa_config')
        enable_direction_correction = data.get('enable_direction_correction', True)  # é»˜è®¤å¯ç”¨
        
        if not rqa_config:
            return jsonify({
                'success': False,
                'error': 'è¯·æä¾›RQAé…ç½®å‚æ•°'
            }), 400
        
        print(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ¨¡å—9.1æ•°æ®é¢„å¤„ç†ï¼ŒRQAé…ç½®: {rqa_config}")
        
        # æ­¥éª¤1: åŠ è½½çœ¼åŠ¨æ•°æ®
        print("ğŸ“‚ æ­¥éª¤1: åŠ è½½çœ¼åŠ¨æ•°æ®")
        eye_movement_df, eye_file = ml_processor.load_eye_movement_data(rqa_config)
        
        # æ­¥éª¤2: åŠ è½½MMSEæ•°æ®
        print("ğŸ“‚ æ­¥éª¤2: åŠ è½½MMSEæ•°æ®")
        mmse_df = ml_processor.load_mmse_data()
        
        # æ­¥éª¤3: æå–MMSEå­åˆ†æ•°
        print("ğŸ”¢ æ­¥éª¤3: æå–MMSEå­åˆ†æ•°")
        mmse_subscores_df = ml_processor.extract_mmse_subscores(mmse_df)
        
        # æ­¥éª¤4: åˆ›å»ºå—è¯•è€…çº§åˆ«çš„çœ¼åŠ¨ç‰¹å¾
        print("ğŸ§® æ­¥éª¤4: åˆ›å»ºå—è¯•è€…çº§åˆ«çœ¼åŠ¨ç‰¹å¾")
        eye_features_df = ml_processor.create_subject_aggregated_features(eye_movement_df)
        
        # æ­¥éª¤5: åˆå¹¶çœ¼åŠ¨ç‰¹å¾å’ŒMMSEæ•°æ®
        print("ğŸ”— æ­¥éª¤5: åˆå¹¶çœ¼åŠ¨ç‰¹å¾å’ŒMMSEæ•°æ®")
        merged_df = ml_processor.merge_eye_movement_and_mmse(eye_features_df, mmse_subscores_df)
        
        # æ­¥éª¤6: å‡†å¤‡æœºå™¨å­¦ä¹ æ•°æ®é›†
        print("âš™ï¸ æ­¥éª¤6: å‡†å¤‡æœºå™¨å­¦ä¹ æ•°æ®é›†")
        dataset = ml_processor.prepare_ml_dataset(merged_df)
        
        # æ­¥éª¤7: ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®
        print("ğŸ’¾ æ­¥éª¤7: ä¿å­˜é¢„å¤„ç†æ•°æ®")
        saved_files = ml_processor.save_preprocessed_data(dataset, rqa_config)
        
        print(f"âœ… æ¨¡å—9.1æ•°æ®é¢„å¤„ç†å®Œæˆï¼")
        
        return jsonify({
            'success': True,
            'message': 'æ•°æ®é¢„å¤„ç†å®Œæˆ',
            'stats': dataset['stats'],
            'files': saved_files,
            'source_file': eye_file
        })
        
    except Exception as e:
        error_msg = str(e)
        traceback.print_exc()
        print(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {error_msg}")
        
        return jsonify({
            'success': False,
            'error': error_msg
        }), 500

class CVMLPTrainer:
    """5-foldäº¤å‰éªŒè¯MLPæ¨¡å‹è®­ç»ƒå™¨ï¼ˆä¸“å®¶ä¼˜åŒ–ç‰ˆ + æ ‡ç­¾å½’ä¸€åŒ–ï¼‰"""
    
    def __init__(self, config_name):
        self.config_name = config_name
        self.model_dir = os.path.join(os.path.dirname(__file__), '..', 'data', 'module9_ml_results', config_name)
        os.makedirs(self.model_dir, exist_ok=True)
        
        # CVä¸“ç”¨ç›®å½•
        self.cv_models_dir = os.path.join(self.model_dir, 'cv_models')
        self.cv_histories_dir = os.path.join(self.model_dir, 'cv_histories')
        os.makedirs(self.cv_models_dir, exist_ok=True)
        os.makedirs(self.cv_histories_dir, exist_ok=True)
        
        # ä¸“å®¶å»ºè®®çš„ä¼˜åŒ–å‚æ•°
        self.cv_params = {
            'n_splits': 5,
            'shuffle': True,
            'random_state': 42,
            'epochs': 200,
            'batch_size': 8,
            'patience': 10,
            'dropout': 0.35,  # ä¸“å®¶å»ºè®®
            'l2_reg': 1e-3,   # ä¸“å®¶å»ºè®®
            'hidden_layers': [32],  # ç®€å•ç»“æ„é¿å…è¿‡æ‹Ÿåˆ
            'activation': 'relu',
            'optimizer': 'adam',
            'loss': 'mse',
            'metrics': ['mae']
        }
        
        # æ ‡ç­¾å½’ä¸€åŒ–ï¼šMMSEå­åˆ†æ•°æ»¡åˆ†å‘é‡ [Q1, Q2, Q3, Q4, Q5]
        self.MAX_SCORES = np.array([5, 5, 3, 5, 3], dtype=np.float32)
        print(f"ğŸ¯ æ ‡ç­¾å½’ä¸€åŒ–å¯ç”¨ï¼šMAX_SCORES = {self.MAX_SCORES}")
    
    def build_model(self, input_dim=10, output_dim=5):
        """æ„å»ºå•ä¸ªCVæŠ˜å çš„æ¨¡å‹"""
        if not TENSORFLOW_AVAILABLE:
            raise ImportError("TensorFlowä¸å¯ç”¨ï¼Œæ— æ³•åˆ›å»ºMLPæ¨¡å‹")
        
        l2_reg = keras.regularizers.l2(self.cv_params['l2_reg'])
        
        model = keras.Sequential([
            layers.Dense(
                self.cv_params['hidden_layers'][0], 
                activation=self.cv_params['activation'],
                input_shape=(input_dim,),
                kernel_regularizer=l2_reg,
                kernel_initializer='he_normal',
                name='hidden_1'
            ),
            layers.Dropout(self.cv_params['dropout'], name='dropout_1'),
            layers.Dense(output_dim, activation='linear', name='output')
        ])
        
        model.compile(
            optimizer=self.cv_params['optimizer'],
            loss=self.cv_params['loss'],
            metrics=self.cv_params['metrics']
        )
        
        return model
    
    def train_cv_models(self, X_train, y_train_raw, X_test, y_test_raw):
        """æ‰§è¡Œ5-foldäº¤å‰éªŒè¯è®­ç»ƒ"""
        try:
            print(f"ğŸš€ å¼€å§‹5-foldäº¤å‰éªŒè¯è®­ç»ƒï¼ˆæ ‡ç­¾å½’ä¸€åŒ–ä¼˜åŒ–ç‰ˆï¼‰")
            print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬: {len(X_train)}, æµ‹è¯•æ ·æœ¬: {len(X_test)}")
            print(f"ğŸ¯ åŸå§‹æ ‡ç­¾èŒƒå›´: y_train âˆˆ [{y_train_raw.min():.1f}, {y_train_raw.max():.1f}]")
            
            # æ ‡ç­¾å½’ä¸€åŒ–: y âˆˆ [0,1]
            y_train = y_train_raw / self.MAX_SCORES
            y_test = y_test_raw / self.MAX_SCORES
            print(f"âœ… æ ‡ç­¾å½’ä¸€åŒ–å®Œæˆ: y_norm âˆˆ [{y_train.min():.3f}, {y_train.max():.3f}]")
            
            # å¯¼å…¥sklearn
            from sklearn.model_selection import KFold
            from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
            
            # åˆå§‹åŒ–
            kf = KFold(
                n_splits=self.cv_params['n_splits'],
                shuffle=self.cv_params['shuffle'],
                random_state=self.cv_params['random_state']
            )
            
            fold_predictions = []
            fold_metrics = []
            fold_histories = []
            
            # 5-foldè®­ç»ƒå¾ªç¯
            for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):
                print(f"\nğŸ“‚ è®­ç»ƒ Fold {fold + 1}/{self.cv_params['n_splits']}")
                
                # åˆ’åˆ†å½“å‰æŠ˜å çš„è®­ç»ƒ/éªŒè¯é›†
                X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]
                y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]
                
                print(f"  è®­ç»ƒæ ·æœ¬: {len(X_fold_train)}, éªŒè¯æ ·æœ¬: {len(X_fold_val)}")
                
                # æ„å»ºæ¨¡å‹ - åŠ¨æ€è·å–è¾“å…¥ç»´åº¦ï¼ˆä¸“å®¶å»ºè®®ï¼š10ä¸ªæ ¸å¿ƒç‰¹å¾ï¼‰
                input_dim = X_fold_train.shape[1]
                output_dim = y_fold_train.shape[1]
                print(f"  ğŸ”§ æ¨¡å‹ç»´åº¦: è¾“å…¥={input_dim}ç‰¹å¾, è¾“å‡º={output_dim}å­åˆ†æ•°")
                if input_dim == 10:
                    print(f"  âœ… ç‰¹å¾ç»´åº¦ç¬¦åˆä¸“å®¶å»ºè®® (10ä¸ªæ ¸å¿ƒçœ¼åŠ¨ç‰¹å¾)")
                else:
                    print(f"  âš ï¸ ç‰¹å¾ç»´åº¦å¼‚å¸¸: æœŸæœ›10ä¸ªï¼Œå®é™…{input_dim}ä¸ª")
                model = self.build_model(input_dim=input_dim, output_dim=output_dim)
                
                # æ—©åœå›è°ƒ
                early_stopping = keras.callbacks.EarlyStopping(
                    monitor='val_loss',
                    patience=self.cv_params['patience'],
                    restore_best_weights=True,
                    verbose=1
                )
                
                # è®­ç»ƒæ¨¡å‹
                history = model.fit(
                    X_fold_train, y_fold_train,
                    validation_data=(X_fold_val, y_fold_val),
                    epochs=self.cv_params['epochs'],
                    batch_size=self.cv_params['batch_size'],
                    verbose=0,
                    callbacks=[early_stopping]
                )
                
                # ä¿å­˜æ¨¡å‹
                model_path = os.path.join(self.cv_models_dir, f'fold{fold}.keras')
                model.save(model_path)
                print(f"  âœ… æ¨¡å‹å·²ä¿å­˜: fold{fold}.keras")
                
                # ä¿å­˜è®­ç»ƒå†å²
                history_path = os.path.join(self.cv_histories_dir, f'cv_history_fold{fold}.json')
                with open(history_path, 'w') as f:
                    import json
                    history_dict = {}
                    for key, values in history.history.items():
                        history_dict[key] = [float(v) for v in values]
                    json.dump(history_dict, f, indent=2)
                
                # éªŒè¯é›†é¢„æµ‹å’ŒæŒ‡æ ‡ï¼ˆæ ‡ç­¾å½’ä¸€åŒ–ç‰ˆï¼‰
                y_val_pred_norm = model.predict(X_fold_val, verbose=0)  # é¢„æµ‹å½’ä¸€åŒ–æ ‡ç­¾
                y_val_pred_raw = y_val_pred_norm * self.MAX_SCORES      # ä¹˜å›æ»¡åˆ†è¿›è¡Œè¯„ä¼°
                
                # è·å–åŸå§‹éªŒè¯æ ‡ç­¾ï¼ˆä»ç´¢å¼•ä¸­æå–ï¼‰
                y_fold_val_raw = y_train_raw[val_idx]
                
                # è®¡ç®—åŸå§‹é‡çº²çš„æŒ‡æ ‡
                val_mse = mean_squared_error(y_fold_val_raw, y_val_pred_raw)
                val_rmse = np.sqrt(val_mse)  # å…¼å®¹è€ç‰ˆæœ¬sklearn
                val_mae = mean_absolute_error(y_fold_val_raw, y_val_pred_raw)
                val_r2 = r2_score(y_fold_val_raw, y_val_pred_raw)
                best_epoch = len(history.history['loss'])
                
                fold_metrics.append({
                    'fold': fold,
                    'val_rmse': float(val_rmse),
                    'val_mae': float(val_mae),
                    'val_r2': float(val_r2),
                    'best_epoch': best_epoch,
                    'final_train_loss': float(history.history['loss'][-1]),
                    'final_val_loss': float(history.history['val_loss'][-1])
                })
                
                # æµ‹è¯•é›†é¢„æµ‹ï¼ˆç”¨äºåç»­é›†æˆï¼Œä¿å­˜å½’ä¸€åŒ–ç‰ˆæœ¬ï¼‰
                y_test_pred_norm = model.predict(X_test, verbose=0)  # å½’ä¸€åŒ–é¢„æµ‹
                fold_predictions.append(y_test_pred_norm)            # ä¿å­˜å½’ä¸€åŒ–ç‰ˆæœ¬ç”¨äºé›†æˆ
                
                print(f"  ğŸ“Š éªŒè¯ RMSE: {val_rmse:.4f}, MAE: {val_mae:.4f}, RÂ²: {val_r2:.4f}")
                print(f"  ğŸ æœ€ä½³è½®æ¬¡: {best_epoch}")
            
            # é›†æˆé¢„æµ‹ï¼ˆæ ‡ç­¾å½’ä¸€åŒ–ç‰ˆï¼‰
            print(f"\nğŸ”„ è®¡ç®—é›†æˆé¢„æµ‹...")
            y_pred_ensemble_norm = np.mean(fold_predictions, axis=0)  # å½’ä¸€åŒ–é›†æˆé¢„æµ‹
            y_pred_ensemble_raw = y_pred_ensemble_norm * self.MAX_SCORES  # ä¹˜å›æ»¡åˆ†
            
            print(f"ğŸ“Š é›†æˆé¢„æµ‹èŒƒå›´: norm âˆˆ [{y_pred_ensemble_norm.min():.3f}, {y_pred_ensemble_norm.max():.3f}]")
            print(f"ğŸ“Š é›†æˆé¢„æµ‹èŒƒå›´: raw âˆˆ [{y_pred_ensemble_raw.min():.1f}, {y_pred_ensemble_raw.max():.1f}]")
            
            # é›†æˆè¯„ä¼°ï¼ˆä½¿ç”¨åŸå§‹é‡çº²ï¼‰
            ensemble_mse = mean_squared_error(y_test_raw, y_pred_ensemble_raw)
            ensemble_rmse = np.sqrt(ensemble_mse)  # å…¼å®¹è€ç‰ˆæœ¬sklearn
            ensemble_mae = mean_absolute_error(y_test_raw, y_pred_ensemble_raw)
            ensemble_r2 = r2_score(y_test_raw, y_pred_ensemble_raw)
            
            # æ¯ä¸ªå­åˆ†æ•°çš„è¯¦ç»†è¯„ä¼°
            detailed_metrics = {}
            target_names = ['Q1_subscore', 'Q2_subscore', 'Q3_subscore', 'Q4_subscore', 'Q5_subscore']
            
            for i, target in enumerate(target_names):
                y_true_col = y_test_raw[:, i]         # ä½¿ç”¨åŸå§‹é‡çº²
                y_pred_col = y_pred_ensemble_raw[:, i] # ä½¿ç”¨åŸå§‹é‡çº²
                
                mse_val = mean_squared_error(y_true_col, y_pred_col)
                detailed_metrics[target] = {
                    'mse': float(mse_val),
                    'rmse': float(np.sqrt(mse_val)),  # å…¼å®¹è€ç‰ˆæœ¬sklearn
                    'mae': float(mean_absolute_error(y_true_col, y_pred_col)),
                    'r2': float(r2_score(y_true_col, y_pred_col)),
                    'max_score': float(self.MAX_SCORES[i])  # æ·»åŠ æ»¡åˆ†ä¿¡æ¯
                }
            
            # ä¿å­˜é›†æˆé¢„æµ‹ç»“æœ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # åˆ›å»ºé¢„æµ‹ç»“æœDataFrameï¼ˆåŸå§‹é‡çº²ï¼‰
            predictions_df = pd.DataFrame({
                'Subject_ID': range(len(y_test_raw)),  # ä¸´æ—¶IDï¼Œå®é™…ä½¿ç”¨æ—¶åº”ä¼ å…¥çœŸå®ID
                **{f'y_true_{target}': y_test_raw[:, i] for i, target in enumerate(target_names)},
                **{f'y_pred_{target}': y_pred_ensemble_raw[:, i] for i, target in enumerate(target_names)},
                # é¢å¤–æ·»åŠ å½’ä¸€åŒ–ç‰ˆæœ¬ç”¨äºè°ƒè¯•
                **{f'y_pred_norm_{target}': y_pred_ensemble_norm[:, i] for i, target in enumerate(target_names)}
            })
            
            predictions_path = os.path.join(self.model_dir, f'ensemble_test_predictions_{timestamp}.csv')
            predictions_df.to_csv(predictions_path, index=False)
            
            # ä¿å­˜CVæŒ‡æ ‡ï¼ˆåŒ…å«æ ‡ç­¾å½’ä¸€åŒ–å’Œç‰¹å¾é€‰æ‹©ä¿¡æ¯ï¼‰
            cv_metrics = {
                'fold_stats': fold_metrics,
                'ensemble': {
                    'mse': float(ensemble_mse),
                    'rmse': float(ensemble_rmse),
                    'mae': float(ensemble_mae),
                    'r2': float(ensemble_r2)
                },
                'detailed_metrics': detailed_metrics,
                'cv_params': self.cv_params,
                'label_normalization': {
                    'enabled': True,
                    'max_scores': self.MAX_SCORES.tolist(),
                    'description': 'Labels normalized to [0,1] during training, scaled back for evaluation'
                },
                'feature_selection': {
                    'enabled': True,
                    'total_features': len(X_train[0]),
                    'expected_features': [
                        'game_duration', 'KW_ROI_time', 'INST_ROI_time', 'BG_ROI_time',
                        'RR_1D', 'DET_1D', 'ENT_1D', 'RR_2D', 'DET_2D', 'ENT_2D'
                    ],
                    'actual_features': feature_columns,
                    'description': 'Expert-recommended 10 core eye-movement features: 4 basic + 6 RQA parameters'
                },
                'timestamp': timestamp
            }
            
            metrics_path = os.path.join(self.model_dir, f'cv_metrics_{timestamp}.json')
            with open(metrics_path, 'w') as f:
                json.dump(cv_metrics, f, indent=2, ensure_ascii=False)
            
            print(f"\nâœ… 5-foldäº¤å‰éªŒè¯å®Œæˆ!")
            print(f"ğŸ“Š é›†æˆç»“æœ:")
            print(f"  ğŸ¯ RMSE: {ensemble_rmse:.4f}")
            print(f"  ğŸ“ MAE: {ensemble_mae:.4f}")
            print(f"  ğŸ“ˆ RÂ²: {ensemble_r2:.4f}")
            
            # è®¡ç®—CVç»Ÿè®¡
            fold_rmses = [m['val_rmse'] for m in fold_metrics]
            cv_rmse_mean = np.mean(fold_rmses)
            cv_rmse_std = np.std(fold_rmses)
            
            print(f"  ğŸ“Š CV RMSE: {cv_rmse_mean:.4f} Â± {cv_rmse_std:.4f}")
            
            return {
                'success': True,
                'ensemble_metrics': {
                    'rmse': ensemble_rmse,
                    'mae': ensemble_mae,
                    'r2': ensemble_r2,
                    'mse': ensemble_mse
                },
                'fold_metrics': fold_metrics,
                'detailed_metrics': detailed_metrics,
                'cv_stats': {
                    'cv_rmse_mean': float(cv_rmse_mean),
                    'cv_rmse_std': float(cv_rmse_std),
                    'best_fold': min(fold_metrics, key=lambda x: x['val_rmse'])['fold']
                },
                'files': {
                    'predictions': predictions_path,
                    'metrics': metrics_path,
                    'models_dir': self.cv_models_dir,
                    'histories_dir': self.cv_histories_dir
                }
            }
            
        except Exception as e:
            print(f"âŒ CVè®­ç»ƒå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return {
                'success': False,
                'error': str(e)
            }
    
    def load_ensemble_models(self):
        """åŠ è½½æ‰€æœ‰CVæ¨¡å‹ç”¨äºé›†æˆé¢„æµ‹"""
        try:
            import glob
            model_paths = glob.glob(os.path.join(self.cv_models_dir, 'fold*.keras'))
            if len(model_paths) != self.cv_params['n_splits']:
                raise FileNotFoundError(f"æœŸæœ›{self.cv_params['n_splits']}ä¸ªæ¨¡å‹ï¼Œå®é™…æ‰¾åˆ°{len(model_paths)}ä¸ª")
            
            models = []
            for path in sorted(model_paths):
                model = keras.models.load_model(path)
                models.append(model)
            
            return models
        except Exception as e:
            print(f"âŒ åŠ è½½é›†æˆæ¨¡å‹å¤±è´¥: {str(e)}")
            return None
    
    def predict_ensemble(self, X_input, models=None, return_raw_scale=True):
        """ä½¿ç”¨é›†æˆæ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼ˆæ”¯æŒæ ‡ç­¾å½’ä¸€åŒ–ï¼‰"""
        if models is None:
            models = self.load_ensemble_models()
            if models is None:
                raise RuntimeError("æ— æ³•åŠ è½½é›†æˆæ¨¡å‹")
        
        # è·å–æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹ï¼ˆå½’ä¸€åŒ–ç‰ˆæœ¬ï¼‰
        predictions_norm = []
        for model in models:
            pred_norm = model.predict(X_input, verbose=0)
            predictions_norm.append(pred_norm)
        
        # è®¡ç®—å¹³å‡é¢„æµ‹ï¼ˆå½’ä¸€åŒ–ç‰ˆæœ¬ï¼‰
        ensemble_prediction_norm = np.mean(predictions_norm, axis=0)
        
        if return_raw_scale:
            # ä¹˜å›æ»¡åˆ†å¾—åˆ°åŸå§‹é‡çº²
            ensemble_prediction_raw = ensemble_prediction_norm * self.MAX_SCORES
            individual_predictions_raw = [pred * self.MAX_SCORES for pred in predictions_norm]
            return ensemble_prediction_raw, individual_predictions_raw
        else:
            # è¿”å›å½’ä¸€åŒ–ç‰ˆæœ¬
            return ensemble_prediction_norm, predictions_norm

class MLPTrainer:
    """MLPæ¨¡å‹è®­ç»ƒå™¨ï¼ˆå•æ¨¡å‹ç‰ˆæœ¬ï¼‰"""
    
    def __init__(self, config_name):
        self.config_name = config_name
        self.model_dir = os.path.join(os.path.dirname(__file__), '..', 'data', 'module9_ml_results', config_name)
        os.makedirs(self.model_dir, exist_ok=True)
        
        # é»˜è®¤æ¨¡å‹å‚æ•° - é’ˆå¯¹å°æ ·æœ¬è¿‡æ‹Ÿåˆä¼˜åŒ–
        self.default_params = {
            'model_type': 'simple',  # 'simple', 'moderate', 'complex'
            'hidden_layers': [32],   # ç®€åŒ–ç½‘ç»œç»“æ„ï¼Œå•éšè—å±‚32ç¥ç»å…ƒ
            'activation': 'relu',
            'output_activation': 'linear',
            'optimizer': 'adam',
            'loss': 'mse',
            'metrics': ['mse', 'mae'],
            'epochs': 100,
            'batch_size': 8,  # ä¸“å®¶å»ºè®®ï¼š8æ›´é€‚åˆ48ä¸ªè®­ç»ƒæ ·æœ¬
            'validation_split': 0.2,  # ä¸“å®¶å»ºè®®ï¼š0.2æ›´åˆç†
            'early_stopping_patience': 15,
            'learning_rate': 0.001,
            # æ­£åˆ™åŒ–å‚æ•°
            'use_dropout': True,
            'dropout_rate': 0.3,
            'use_l2_regularization': True,
            'l2_lambda': 0.01,
            'use_batch_normalization': False  # å°æ•°æ®é›†ä¸å»ºè®®ä½¿ç”¨
        }
    
    def get_model_presets(self):
        """è·å–é¢„è®¾æ¨¡å‹é…ç½®"""
        return {
            'simple': {
                'hidden_layers': [32],
                'description': 'å•éšè—å±‚32ç¥ç»å…ƒ - é€‚åˆå°æ ·æœ¬ï¼Œé˜²è¿‡æ‹Ÿåˆ'
            },
            'moderate': {
                'hidden_layers': [64, 32],
                'description': 'åŒéšè—å±‚64+32ç¥ç»å…ƒ - ä¸­ç­‰å¤æ‚åº¦'
            },
            'complex': {
                'hidden_layers': [64, 32, 16],
                'description': 'ä¸‰éšè—å±‚64+32+16ç¥ç»å…ƒ - å¤æ‚æ¨¡å‹ï¼Œéœ€å¤§æ ·æœ¬'
            }
        }

    def create_mlp_model(self, input_dim=10, output_dim=5, params=None):
        """åˆ›å»ºä¼˜åŒ–çš„MLPæ¨¡å‹ï¼Œé’ˆå¯¹å°æ ·æœ¬è¿‡æ‹Ÿåˆé—®é¢˜"""
        if not TENSORFLOW_AVAILABLE:
            raise ImportError("TensorFlowä¸å¯ç”¨ï¼Œæ— æ³•åˆ›å»ºMLPæ¨¡å‹")
        
        if params is None:
            params = self.default_params
        
        model = keras.Sequential()
        
        # L2æ­£åˆ™åŒ–é…ç½®
        l2_reg = keras.regularizers.l2(params.get('l2_lambda', 0.01)) if params.get('use_l2_regularization', False) else None
        
        # è¾“å…¥å±‚å’Œç¬¬ä¸€ä¸ªéšè—å±‚
        model.add(layers.Dense(
            params['hidden_layers'][0], 
            input_dim=input_dim,
            activation=params['activation'],
            kernel_initializer='he_normal',
            kernel_regularizer=l2_reg,
            name='hidden_layer_1'
        ))
        
        # ç¬¬ä¸€å±‚åçš„Dropout
        if params.get('use_dropout', False):
            model.add(layers.Dropout(params.get('dropout_rate', 0.3), name='dropout_1'))
        
        # æ‰¹å½’ä¸€åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if params.get('use_batch_normalization', False):
            model.add(layers.BatchNormalization(name='batch_norm_1'))
        
        # é¢å¤–çš„éšè—å±‚
        for i, units in enumerate(params['hidden_layers'][1:], 2):
            model.add(layers.Dense(
                units,
                activation=params['activation'],
                kernel_initializer='he_normal',
                kernel_regularizer=l2_reg,
                name=f'hidden_layer_{i}'
            ))
            
            # æ¯å±‚åæ·»åŠ Dropout
            if params.get('use_dropout', False):
                model.add(layers.Dropout(params.get('dropout_rate', 0.3), name=f'dropout_{i}'))
            
            # æ‰¹å½’ä¸€åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if params.get('use_batch_normalization', False):
                model.add(layers.BatchNormalization(name=f'batch_norm_{i}'))
        
        # è¾“å‡ºå±‚ï¼ˆæ— æ­£åˆ™åŒ–ï¼‰
        model.add(layers.Dense(
            output_dim,
            activation=params['output_activation'],
            name='output_layer'
        ))
        
        # è¯¦ç»†æ¨¡å‹ä¿¡æ¯
        regularization_info = []
        if params.get('use_dropout', False):
            regularization_info.append(f"Dropout({params.get('dropout_rate', 0.3)})")
        if params.get('use_l2_regularization', False):
            regularization_info.append(f"L2({params.get('l2_lambda', 0.01)})")
        if params.get('use_batch_normalization', False):
            regularization_info.append("BatchNorm")
        
        reg_str = f" + {'+'.join(regularization_info)}" if regularization_info else ""
        
        print(f"ğŸ§  åˆ›å»ºä¼˜åŒ–MLPæ¨¡å‹: {input_dim}ç»´è¾“å…¥ -> {params['hidden_layers']} -> {output_dim}ç»´è¾“å‡º{reg_str}")
        print(f"ğŸ“Š æ¨¡å‹å¤æ‚åº¦: {params.get('model_type', 'custom')}çº§åˆ«")
        
        return model
    
    def compile_model(self, model, params=None):
        """ç¼–è¯‘æ¨¡å‹"""
        if params is None:
            params = self.default_params
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        if params.get('learning_rate'):
            optimizer = keras.optimizers.Adam(learning_rate=params['learning_rate'])
        else:
            optimizer = params['optimizer']
        
        model.compile(
            optimizer=optimizer,
            loss=params['loss'],
            metrics=params['metrics']
        )
        
        print(f"âš™ï¸ æ¨¡å‹ç¼–è¯‘å®Œæˆ: {params['optimizer']}, æŸå¤±å‡½æ•°: {params['loss']}")
        
        return model
    
    def train_model(self, X_train, y_train, X_test=None, y_test=None, params=None):
        """è®­ç»ƒMLPæ¨¡å‹"""
        try:
            if params is None:
                params = self.default_params
            
            print(f"ğŸš€ å¼€å§‹MLPæ¨¡å‹è®­ç»ƒ")
            print(f"ğŸ“Š è®­ç»ƒæ•°æ®: {X_train.shape[0]} æ ·æœ¬, {X_train.shape[1]} ç‰¹å¾")
            print(f"ğŸ¯ ç›®æ ‡æ•°æ®: {y_train.shape[0]} æ ·æœ¬, {y_train.shape[1]} è¾“å‡º")
            
            # åˆ›å»ºå’Œç¼–è¯‘æ¨¡å‹
            model = self.create_mlp_model(
                input_dim=X_train.shape[1],
                output_dim=y_train.shape[1],
                params=params
            )
            
            model = self.compile_model(model, params)
            
            # æ˜¾ç¤ºæ¨¡å‹æ‘˜è¦
            print("ğŸ“‹ æ¨¡å‹ç»“æ„:")
            model.summary(print_fn=lambda x: print(f"  {x}"))
            
            # å‡†å¤‡å›è°ƒå‡½æ•°
            callbacks = []
            
            # æ—©åœå›è°ƒ
            if params.get('early_stopping_patience'):
                early_stop = EarlyStopping(
                    monitor='val_loss',
                    patience=params['early_stopping_patience'],
                    restore_best_weights=True,
                    verbose=1
                )
                callbacks.append(early_stop)
                print(f"ğŸ“Œ æ—©åœç­–ç•¥: éªŒè¯æŸå¤±{params['early_stopping_patience']}è½®æœªæ”¹å–„å³åœæ­¢")
            
            # æ¨¡å‹æ£€æŸ¥ç‚¹
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            checkpoint_path = os.path.join(self.model_dir, f'best_model_{timestamp}.h5')
            checkpoint = ModelCheckpoint(
                checkpoint_path,
                monitor='val_loss',
                save_best_only=True,
                verbose=1
            )
            callbacks.append(checkpoint)
            
            # å¼€å§‹è®­ç»ƒ
            print(f"ğŸ“ å¼€å§‹è®­ç»ƒ: {params['epochs']} epochs, batch_size={params['batch_size']}")
            
            history = model.fit(
                X_train, y_train,
                epochs=params['epochs'],
                batch_size=params['batch_size'],
                validation_split=params['validation_split'],
                callbacks=callbacks,
                verbose=1
            )
            
            # è®­ç»ƒå®Œæˆåçš„è¯„ä¼°
            print(f"âœ… è®­ç»ƒå®Œæˆ!")
            
            # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
            if X_test is not None and y_test is not None:
                print(f"ğŸ§ª æµ‹è¯•é›†è¯„ä¼°:")
                test_loss = model.evaluate(X_test, y_test, verbose=0)
                print(f"  æµ‹è¯•æŸå¤±(MSE): {test_loss[0]:.4f}")
                if len(test_loss) > 1:
                    print(f"  æµ‹è¯•MAE: {test_loss[2]:.4f}")
                
                # è¯¦ç»†æµ‹è¯•é›†æ€§èƒ½åˆ†æ
                print(f"ğŸ“Š è¯¦ç»†æµ‹è¯•é›†æ€§èƒ½åˆ†æ:")
                
                # è·å–é¢„æµ‹ç»“æœ
                from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
                y_pred = model.predict(X_test, verbose=0)
                
                # æ•´ä½“æ€§èƒ½æŒ‡æ ‡
                mse = mean_squared_error(y_test, y_pred)
                mae = mean_absolute_error(y_test, y_pred)
                rmse = mse ** 0.5
                r2 = r2_score(y_test, y_pred)
                
                print(f"ğŸ¯ æ•´ä½“æµ‹è¯•é›†æ€§èƒ½:")
                print(f"  TEST MSE: {mse:.4f}")
                print(f"  TEST RMSE: {rmse:.4f}")
                print(f"  TEST MAE: {mae:.4f}")
                print(f"  TEST RÂ²: {r2:.4f}")
                
                # æ¯ä¸ªMMSEå­åˆ†æ•°çš„è¯¦ç»†åˆ†æ
                print(f"ğŸ“‹ å„å­åˆ†æ•°è¯¦ç»†æ€§èƒ½:")
                subscore_names = ["æ—¶é—´å®šå‘(Q1)", "ç©ºé—´å®šå‘(Q2)", "å³æ—¶è®°å¿†(Q3)", "æ³¨æ„/è®¡ç®—(Q4)", "å»¶è¿Ÿå›å¿†(Q5)"]
                max_scores = [5, 5, 3, 5, 3]  # å„å­åˆ†æ•°çš„æ»¡åˆ†
                
                subscore_results = []
                for i, (name, max_score) in enumerate(zip(subscore_names, max_scores)):
                    sub_mae = mean_absolute_error(y_test[:, i], y_pred[:, i])
                    sub_r2 = r2_score(y_test[:, i], y_pred[:, i])
                    sub_mse = mean_squared_error(y_test[:, i], y_pred[:, i])
                    sub_rmse = sub_mse ** 0.5
                    
                    # è®¡ç®—ç›¸å¯¹è¯¯å·® (MAE/æ»¡åˆ†)
                    relative_error = (sub_mae / max_score) * 100
                    
                    print(f"  {name}:")
                    print(f"    MAE: {sub_mae:.3f} (ç›¸å¯¹è¯¯å·®: {relative_error:.1f}%)")
                    print(f"    RMSE: {sub_rmse:.3f}")
                    print(f"    RÂ²: {sub_r2:.3f}")
                    print(f"    æ»¡åˆ†: {max_score}")
                    
                    subscore_results.append({
                        'name': name,
                        'mae': sub_mae,
                        'rmse': sub_rmse,
                        'r2': sub_r2,
                        'relative_error': relative_error,
                        'max_score': max_score
                    })
                
                # æ€§èƒ½åˆ†çº§è¯„ä¼°
                print(f"ğŸ† æ€§èƒ½åˆ†çº§è¯„ä¼°:")
                avg_relative_error = sum([r['relative_error'] for r in subscore_results]) / len(subscore_results)
                avg_r2 = sum([r['r2'] for r in subscore_results]) / len(subscore_results)
                
                if avg_relative_error < 15 and avg_r2 > 0.7:
                    performance_grade = "ä¼˜ç§€"
                    grade_emoji = "ğŸ†"
                elif avg_relative_error < 25 and avg_r2 > 0.5:
                    performance_grade = "è‰¯å¥½"  
                    grade_emoji = "ğŸ¥ˆ"
                elif avg_relative_error < 35 and avg_r2 > 0.3:
                    performance_grade = "ä¸­ç­‰"
                    grade_emoji = "ğŸ¥‰"
                else:
                    performance_grade = "éœ€æ”¹è¿›"
                    grade_emoji = "âš ï¸"
                
                print(f"  {grade_emoji} ç»¼åˆè¯„çº§: {performance_grade}")
                print(f"  å¹³å‡ç›¸å¯¹è¯¯å·®: {avg_relative_error:.1f}%")
                print(f"  å¹³å‡RÂ²: {avg_r2:.3f}")
                
                # è®­ç»ƒvséªŒè¯vsæµ‹è¯•å¯¹æ¯”
                print(f"ğŸ“Š æ¨¡å‹æ³›åŒ–åˆ†æ:")
                final_train_loss = history.history['loss'][-1]
                final_val_loss = history.history['val_loss'][-1]
                best_val_loss = min(history.history['val_loss'])
                
                # è¿‡æ‹Ÿåˆæ£€æµ‹
                overfitting_ratio = final_val_loss / final_train_loss
                generalization_gap = abs(mse - final_val_loss)
                
                print(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {final_train_loss:.4f}")
                print(f"  æœ€ç»ˆéªŒè¯æŸå¤±: {final_val_loss:.4f}")
                print(f"  æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
                print(f"  æµ‹è¯•é›†æŸå¤±: {mse:.4f}")
                print(f"  è¿‡æ‹Ÿåˆæ¯”ç‡: {overfitting_ratio:.2f}")
                print(f"  æ³›åŒ–å·®è·: {generalization_gap:.4f}")
                
                if overfitting_ratio > 2.0:
                    print(f"âš ï¸  æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆ! å»ºè®®:")
                    print(f"   - å¢åŠ Dropoutç‡ (å½“å‰: {params.get('dropout_rate', 0.3)})")
                    print(f"   - å¢åŠ L2æ­£åˆ™åŒ– (å½“å‰: {params.get('l2_lambda', 0.01)})")
                    print(f"   - ç®€åŒ–ç½‘ç»œç»“æ„")
                    print(f"   - å¢åŠ è®­ç»ƒæ•°æ®")
                elif overfitting_ratio < 1.2:
                    print(f"âœ… æ¨¡å‹æ³›åŒ–è‰¯å¥½")
                else:
                    print(f"ğŸ“ˆ è½»å¾®è¿‡æ‹Ÿåˆï¼Œåœ¨å¯æ¥å—èŒƒå›´å†…")
                
                # ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœåˆ°è®­ç»ƒå†å²
                detailed_evaluation = {
                    'overall': {
                        'mse': float(mse),
                        'mae': float(mae),
                        'rmse': float(rmse),
                        'r2': float(r2)
                    },
                    'subscore_results': subscore_results,
                    'performance_grade': performance_grade,
                    'avg_relative_error': float(avg_relative_error),
                    'avg_r2': float(avg_r2),
                    'overfitting_ratio': float(overfitting_ratio),
                    'generalization_gap': float(generalization_gap)
                }
            
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹ - ä½¿ç”¨æœ‰æ„ä¹‰çš„æ–‡ä»¶å
            # æ„å»ºæ–‡ä»¶å: RQAå‚æ•°_æ¨¡å‹ç±»å‹_éšè—å±‚_æ­£åˆ™åŒ–_æ—¶é—´æˆ³
            model_type = params.get('model_type', 'simple')
            hidden_layers_str = '_'.join(map(str, params.get('hidden_layers', [32])))
            dropout_str = f"dropout{params.get('dropout_rate', 0.3)}" if params.get('use_dropout', False) else "nodropout"
            l2_str = f"l2{params.get('l2_lambda', 0.01)}" if params.get('use_l2_regularization', False) else "nol2"
            
            model_filename = f"{self.config_name}_{model_type}_{hidden_layers_str}_{dropout_str}_{l2_str}_{timestamp}.h5"
            final_model_path = os.path.join(self.model_dir, model_filename)
            model.save(final_model_path)
            
            # åŒæ—¶ä¿å­˜æœ€ä½³æ¨¡å‹çš„å‰¯æœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            best_model_path = os.path.join(self.model_dir, f'best_model_{timestamp}.h5')
            if os.path.exists(best_model_path):
                best_model_filename = f"{self.config_name}_{model_type}_{hidden_layers_str}_{dropout_str}_{l2_str}_best_{timestamp}.h5"
                best_model_final_path = os.path.join(self.model_dir, best_model_filename)
                try:
                    import shutil
                    shutil.copy2(best_model_path, best_model_final_path)
                    print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_model_filename}")
                except Exception as e:
                    print(f"âš ï¸ å¤åˆ¶æœ€ä½³æ¨¡å‹å¤±è´¥: {e}")
            
            # ä¿å­˜è®­ç»ƒå†å²ï¼ˆåŒ…å«è¯¦ç»†è¯„ä¼°ç»“æœï¼‰
            history_path = os.path.join(self.model_dir, f'training_history_{timestamp}.json')
            history_data = {
                'loss': [float(x) for x in history.history['loss']],
                'val_loss': [float(x) for x in history.history['val_loss']],
                'mse': [float(x) for x in history.history.get('mse', [])],
                'val_mse': [float(x) for x in history.history.get('val_mse', [])],
                'mae': [float(x) for x in history.history.get('mae', [])],
                'val_mae': [float(x) for x in history.history.get('val_mae', [])],
                'epochs_trained': len(history.history['loss']),
                'best_val_loss': float(min(history.history['val_loss'])),
                'final_train_loss': float(history.history['loss'][-1]),
                'final_val_loss': float(history.history['val_loss'][-1])
            }
            
            # æ·»åŠ è¯¦ç»†æµ‹è¯•é›†è¯„ä¼°ç»“æœ
            if X_test is not None and y_test is not None and 'detailed_evaluation' in locals():
                history_data['detailed_test_evaluation'] = detailed_evaluation
            
            with open(history_path, 'w', encoding='utf-8') as f:
                json.dump(history_data, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜è®­ç»ƒç»“æœæ‘˜è¦
            model_files_info = {
                'final_model': final_model_path,
                'final_model_filename': model_filename,
                'best_checkpoint': checkpoint_path,
                'training_history': history_path
            }
            
            # å¦‚æœæœ€ä½³æ¨¡å‹å­˜åœ¨ï¼Œæ·»åŠ åˆ°æ–‡ä»¶ä¿¡æ¯ä¸­
            if 'best_model_final_path' in locals():
                model_files_info['best_model'] = best_model_final_path
                model_files_info['best_model_filename'] = best_model_filename
            
            training_summary = {
                'config_name': self.config_name,
                'timestamp': timestamp,
                'model_params': params,
                'training_stats': history_data,
                'model_files': model_files_info,
                'data_info': {
                    'train_samples': X_train.shape[0],
                    'test_samples': X_test.shape[0] if X_test is not None else 0,
                    'input_features': X_train.shape[1],
                    'output_targets': y_train.shape[1]
                }
            }
            
            print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {model_filename}")
            print(f"ğŸ“‹ è®­ç»ƒå†å²å·²ä¿å­˜: training_history_{timestamp}.json")
            print(f"ğŸ“ ä¿å­˜ç›®å½•: {self.model_dir}")
            
            # å‡†å¤‡è¿”å›ç»“æœï¼ŒåŒ…å«è¯¦ç»†è¯„ä¼°
            result = {
                'success': True,
                'model': model,
                'history': history_data,
                'summary': training_summary,
                'files': training_summary['model_files']
            }
            
            # å¦‚æœæœ‰è¯¦ç»†è¯„ä¼°ç»“æœï¼Œæ·»åŠ åˆ°è¿”å›å€¼
            if X_test is not None and y_test is not None and 'detailed_evaluation' in locals():
                result['detailed_evaluation'] = detailed_evaluation
            
            return result
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}")
            raise e

@ml_prediction_bp.route('/api/ml/train-model', methods=['POST'])
def train_mlp_model():
    """è®­ç»ƒMLPæ¨¡å‹"""
    try:
        if not TENSORFLOW_AVAILABLE:
            return jsonify({
                'success': False,
                'error': 'TensorFlowä¸å¯ç”¨ï¼Œè¯·å®‰è£…tensorflowåº“'
            }), 500
        
        data = request.get_json()
        config_name = data.get('config_name')
        model_params = data.get('model_params', {})
        
        if not config_name:
            return jsonify({
                'success': False,
                'error': 'è¯·æä¾›RQAé…ç½®åç§°'
            }), 400
        
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒMLPæ¨¡å‹ï¼Œé…ç½®: {config_name}")
        
        # åŠ è½½é¢„å¤„ç†åçš„æ•°æ®
        config_dir = os.path.join(os.path.dirname(__file__), '..', 'data', 'module9_ml_results', config_name)
        
        # æŸ¥æ‰¾æœ€æ–°çš„æ•°æ®æ–‡ä»¶
        train_files = [f for f in os.listdir(config_dir) if f.startswith('train_dataset_')]
        test_files = [f for f in os.listdir(config_dir) if f.startswith('test_dataset_')]
        
        if not train_files or not test_files:
            return jsonify({
                'success': False,
                'error': f'æœªæ‰¾åˆ°{config_name}çš„é¢„å¤„ç†æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†'
            }), 400
        
        # ä½¿ç”¨æœ€æ–°çš„æ•°æ®æ–‡ä»¶
        latest_train = sorted(train_files)[-1]
        latest_test = sorted(test_files)[-1]
        
        train_path = os.path.join(config_dir, latest_train)
        test_path = os.path.join(config_dir, latest_test)
        
        print(f"ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®: {latest_train}")
        print(f"ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®: {latest_test}")
        
        # åŠ è½½æ•°æ®
        train_df = pd.read_csv(train_path)
        test_df = pd.read_csv(test_path)
        
        # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
        feature_columns = [col for col in train_df.columns 
                          if col not in ['Subject_ID', 'Group_Type'] 
                          and not col.endswith('_subscore')]
        target_columns = [col for col in train_df.columns if col.endswith('_subscore')]
        
        X_train = train_df[feature_columns].values
        y_train = train_df[target_columns].values
        X_test = test_df[feature_columns].values  
        y_test = test_df[target_columns].values
        
        print(f"ğŸ“Š è®­ç»ƒç‰¹å¾: {X_train.shape}")
        print(f"ğŸ¯ è®­ç»ƒæ ‡ç­¾: {y_train.shape}")
        print(f"ğŸ“Š æµ‹è¯•ç‰¹å¾: {X_test.shape}")
        print(f"ğŸ¯ æµ‹è¯•æ ‡ç­¾: {y_test.shape}")
        
        # åˆ›å»ºè®­ç»ƒå™¨å¹¶è®­ç»ƒæ¨¡å‹
        trainer = MLPTrainer(config_name)
        
        # åˆå¹¶ç”¨æˆ·å‚æ•°å’Œé»˜è®¤å‚æ•°
        final_params = trainer.default_params.copy()
        final_params.update(model_params)
        
        print(f"âš™ï¸ è®­ç»ƒå‚æ•°: {final_params}")
        
        # è®­ç»ƒæ¨¡å‹
        result = trainer.train_model(X_train, y_train, X_test, y_test, final_params)
        
        if result['success']:
            print(f"âœ… MLPæ¨¡å‹è®­ç»ƒå®Œæˆ!")
            
            # æå–detailed_evaluationä»¥ä¾¿å‰ç«¯è®¿é—®
            detailed_eval = None
            if 'detailed_evaluation' in result:
                detailed_eval = result['detailed_evaluation']
            elif 'history' in result and 'detailed_test_evaluation' in result['history']:
                detailed_eval = result['history']['detailed_test_evaluation']
            
            return jsonify({
                'success': True,
                'message': 'MLPæ¨¡å‹è®­ç»ƒå®Œæˆ',
                'training_stats': result['history'],
                'detailed_evaluation': detailed_eval,
                'summary': result['summary'],
                'files': result['files']
            })
        else:
            return jsonify({
                'success': False,
                'error': 'æ¨¡å‹è®­ç»ƒå¤±è´¥'
            }), 500
            
    except Exception as e:
        error_msg = str(e)
        traceback.print_exc()
        print(f"âŒ MLPè®­ç»ƒå¤±è´¥: {error_msg}")
        
        return jsonify({
            'success': False,
            'error': error_msg
        }), 500

@ml_prediction_bp.route('/api/ml/cv-train', methods=['POST'])
def cv_train_model():
    """æ‰§è¡Œ5-foldäº¤å‰éªŒè¯è®­ç»ƒï¼ˆä¸“å®¶ä¼˜åŒ–ç‰ˆï¼‰"""
    try:
        data = request.get_json()
        config_name = data.get('config_name')
        cv_params = data.get('cv_params', {})
        
        if not config_name:
            return jsonify({
                'success': False,
                'error': 'è¯·æä¾›RQAé…ç½®åç§°'
            }), 400
        
        print(f"ğŸš€ å¼€å§‹CVè®­ç»ƒï¼Œé…ç½®: {config_name}")
        
        # åŠ è½½é¢„å¤„ç†æ•°æ®
        config_dir = os.path.join(os.path.dirname(__file__), '..', 'data', 'module9_ml_results', config_name)
        
        # æŸ¥æ‰¾æœ€æ–°çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        import glob
        train_files = glob.glob(os.path.join(config_dir, 'train_dataset_*.csv'))
        test_files = glob.glob(os.path.join(config_dir, 'test_dataset_*.csv'))
        
        if not train_files or not test_files:
            return jsonify({
                'success': False,
                'error': 'æœªæ‰¾åˆ°é¢„å¤„ç†æ•°æ®ï¼Œè¯·å…ˆæ‰§è¡Œæ•°æ®é¢„å¤„ç†'
            }), 400
        
        # ä½¿ç”¨æœ€æ–°çš„æ•°æ®æ–‡ä»¶
        train_file = sorted(train_files)[-1]
        test_file = sorted(test_files)[-1]
        
        # åŠ è½½æ•°æ®
        train_df = pd.read_csv(train_file)
        test_df = pd.read_csv(test_file)
        
        # ä¸“å®¶å»ºè®®ï¼šç²¾é€‰10ä¸ªæ ¸å¿ƒçœ¼åŠ¨ç‰¹å¾
        expected_10_features = [
            'game_duration', 'KW_ROI_time', 'INST_ROI_time', 'BG_ROI_time',  # 4ä¸ªåŸºç¡€çœ¼åŠ¨æŒ‡æ ‡
            'RR_1D', 'DET_1D', 'ENT_1D', 'RR_2D', 'DET_2D', 'ENT_2D'        # 6ä¸ªRQAå‚æ•°
        ]
        
        # æ£€æŸ¥æ•°æ®ä¸­å®é™…å­˜åœ¨çš„ç‰¹å¾
        available_cols = list(train_df.columns)
        target_columns = [col for col in train_df.columns if col.endswith('_subscore')]
        
        # æ‰¾å‡ºå®é™…å­˜åœ¨çš„10ä¸ªæ ¸å¿ƒç‰¹å¾
        feature_columns = []
        missing_features = []
        for feature in expected_10_features:
            if feature in available_cols:
                feature_columns.append(feature)
            else:
                missing_features.append(feature)
        
        # å¦‚æœ10ä¸ªæ ¸å¿ƒç‰¹å¾ä¸å…¨ï¼Œä»å¯ç”¨åˆ—ä¸­è¡¥å……ï¼ˆæ’é™¤IDå’Œæ ‡ç­¾åˆ—ï¼‰
        if len(feature_columns) < 10:
            exclude_cols = ['Subject_ID', 'Group_Type'] + target_columns + ['task_count', 'missing_count']
            additional_cols = [col for col in available_cols if col not in exclude_cols + feature_columns]
            needed = 10 - len(feature_columns)
            feature_columns.extend(additional_cols[:needed])
        
        print(f"ğŸ¯ ä¸“å®¶10ç‰¹å¾ç­›é€‰:")
        print(f"  æœŸæœ›ç‰¹å¾: {expected_10_features}")
        print(f"  å®é™…ç‰¹å¾: {feature_columns}")
        print(f"  ç‰¹å¾ç»´åº¦: {len(feature_columns)}")
        if missing_features:
            print(f"  âš ï¸ ç¼ºå¤±ç‰¹å¾: {missing_features}")
        if len(feature_columns) == 10:
            print(f"  âœ… æˆåŠŸè·å¾—10ä¸ªæ ¸å¿ƒç‰¹å¾")
        
        # é‡æ–°æå–ç‰¹å¾æ•°æ®
        X_train_raw = train_df[feature_columns].values
        y_train = train_df[target_columns].values
        X_test_raw = test_df[feature_columns].values
        y_test = test_df[target_columns].values
        
        # é‡æ–°fit scalerï¼ˆä¸“å®¶å»ºè®®ï¼šåŸºäºç²¾é€‰ç‰¹å¾é‡æ–°æ ‡å‡†åŒ–ï¼‰
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train_raw)
        X_test = scaler.transform(X_test_raw)
        
        # ä¿å­˜æ›´æ–°çš„scaler
        scaler_path = os.path.join(config_dir, 'scaler_10features.pkl')
        joblib.dump(scaler, scaler_path)
        print(f"âœ… 10ç‰¹å¾scalerå·²ä¿å­˜: {scaler_path}")
        
        print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"  è®­ç»ƒé›†: {X_train.shape}")
        print(f"  æµ‹è¯•é›†: {X_test.shape}")
        print(f"  ç‰¹å¾æ•°: {len(feature_columns)} (å®é™…ç»´åº¦: {X_train.shape[1]})")
        print(f"  ç›®æ ‡æ•°: {len(target_columns)} (å®é™…ç»´åº¦: {y_train.shape[1]})")
        print(f"  ç‰¹å¾åˆ—è¡¨: {feature_columns}")
        
        # åˆ›å»ºCVè®­ç»ƒå™¨
        cv_trainer = CVMLPTrainer(config_name)
        
        # å¦‚æœæœ‰è‡ªå®šä¹‰å‚æ•°ï¼Œæ›´æ–°CVå‚æ•°
        if cv_params:
            cv_trainer.cv_params.update(cv_params)
            print(f"ğŸ”§ ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°: {cv_params}")
        
        # æ‰§è¡ŒCVè®­ç»ƒï¼ˆä¼ é€’åŸå§‹æ ‡ç­¾ï¼Œç”±CVè®­ç»ƒå™¨å†…éƒ¨å½’ä¸€åŒ–ï¼‰
        result = cv_trainer.train_cv_models(X_train, y_train, X_test, y_test)
        
        if result['success']:
            return jsonify({
                'success': True,
                'message': '5-foldäº¤å‰éªŒè¯è®­ç»ƒå®Œæˆ',
                'ensemble_metrics': result['ensemble_metrics'],
                'fold_metrics': result['fold_metrics'],
                'cv_stats': result['cv_stats'],
                'detailed_metrics': result['detailed_metrics'],
                'files': result['files']
            })
        else:
            return jsonify({
                'success': False,
                'error': result['error']
            }), 500
            
    except Exception as e:
        error_msg = str(e)
        import traceback
        traceback.print_exc()
        print(f"âŒ CVè®­ç»ƒå¤±è´¥: {error_msg}")
        
        return jsonify({
            'success': False,
            'error': error_msg
        }), 500

@ml_prediction_bp.route('/api/ml/ensemble-predict', methods=['POST'])
def ensemble_predict():
    """ä½¿ç”¨é›†æˆæ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    try:
        data = request.get_json()
        config_name = data.get('config_name')
        features = data.get('features')  # åŸå§‹ç‰¹å¾å­—å…¸æˆ–æ•°ç»„
        
        if not config_name or not features:
            return jsonify({
                'success': False,
                'error': 'è¯·æä¾›é…ç½®åç§°å’Œç‰¹å¾æ•°æ®'
            }), 400
        
        # åŠ è½½CVè®­ç»ƒå™¨
        cv_trainer = CVMLPTrainer(config_name)
        
        # å¦‚æœfeaturesæ˜¯å­—å…¸ï¼Œéœ€è¦è½¬æ¢ä¸ºæ•°ç»„
        if isinstance(features, dict):
            # åŠ è½½ç‰¹å¾æ–¹å‘é…ç½®å’Œscaler
            config_dir = os.path.join(os.path.dirname(__file__), '..', 'data', 'module9_ml_results', config_name)
            
            try:
                # åŠ è½½ç‰¹å¾é…ç½®
                import glob
                config_files = glob.glob(os.path.join(config_dir, 'latest_feature_config.json'))
                if config_files:
                    with open(config_files[0], 'r') as f:
                        feature_config = json.load(f)
                else:
                    feature_config = None
                
                # åŠ è½½scalerï¼ˆä¼˜å…ˆä½¿ç”¨10ç‰¹å¾ç‰ˆæœ¬ï¼‰
                scaler_10_files = glob.glob(os.path.join(config_dir, 'scaler_10features.pkl'))
                scaler_files = glob.glob(os.path.join(config_dir, 'latest_scaler.pkl'))
                
                if scaler_10_files:
                    scaler = joblib.load(scaler_10_files[0])
                    print(f"âœ… ä½¿ç”¨10ç‰¹å¾ä¸“ç”¨scaler: {scaler_10_files[0]}")
                elif scaler_files:
                    scaler = joblib.load(scaler_files[0])
                    print(f"âš ï¸ ä½¿ç”¨é€šç”¨scaler: {scaler_files[0]}")
                else:
                    return jsonify({
                        'success': False,
                        'error': 'æœªæ‰¾åˆ°ç‰¹å¾æ ‡å‡†åŒ–å™¨'
                    }), 400
                
                # é¢„å¤„ç†ç‰¹å¾
                feature_array = []
                for feature_name in sorted(features.keys()):
                    value = features[feature_name]
                    
                    # åº”ç”¨ç‰¹å¾å˜æ¢
                    if feature_config and feature_name in feature_config.get('feature_transforms', {}):
                        transform = feature_config['feature_transforms'][feature_name]['transform']
                        if transform == 'negate':
                            value = -value
                        elif transform == 'reciprocal':
                            value = 1.0 / (value + 1e-6)
                    
                    feature_array.append(value)
                
                # æ ‡å‡†åŒ–
                X_input = scaler.transform(np.array(feature_array).reshape(1, -1))
                
            except Exception as e:
                return jsonify({
                    'success': False,
                    'error': f'ç‰¹å¾é¢„å¤„ç†å¤±è´¥: {str(e)}'
                }), 400
        else:
            # å‡è®¾featureså·²ç»æ˜¯é¢„å¤„ç†åçš„æ•°ç»„
            X_input = np.array(features).reshape(1, -1)
        
        # æ‰§è¡Œé›†æˆé¢„æµ‹ï¼ˆè¿”å›åŸå§‹é‡çº²ï¼‰
        ensemble_pred, individual_preds = cv_trainer.predict_ensemble(X_input, return_raw_scale=True)
        
        # æ ¼å¼åŒ–ç»“æœ
        target_names = ['Q1_subscore', 'Q2_subscore', 'Q3_subscore', 'Q4_subscore', 'Q5_subscore']
        
        result = {
            'ensemble_prediction': {
                target_names[i]: float(ensemble_pred[0][i]) for i in range(len(target_names))
            },
            'individual_predictions': [
                {target_names[i]: float(pred[0][i]) for i in range(len(target_names))}
                for pred in individual_preds
            ],
            'prediction_stats': {
                'mean': {target_names[i]: float(np.mean([pred[0][i] for pred in individual_preds])) 
                        for i in range(len(target_names))},
                'std': {target_names[i]: float(np.std([pred[0][i] for pred in individual_preds])) 
                       for i in range(len(target_names))}
            }
        }
        
        return jsonify({
            'success': True,
            'result': result
        })
        
    except Exception as e:
        error_msg = str(e)
        import traceback
        traceback.print_exc()
        print(f"âŒ é›†æˆé¢„æµ‹å¤±è´¥: {error_msg}")
        
        return jsonify({
            'success': False,
            'error': error_msg
        }), 500

def register_ml_prediction_routes(app):
    """æ³¨å†Œæœºå™¨å­¦ä¹ é¢„æµ‹è·¯ç”±"""
    app.register_blueprint(ml_prediction_bp)
    print("âœ… æœºå™¨å­¦ä¹ é¢„æµ‹APIè·¯ç”±å·²æ³¨å†Œ")