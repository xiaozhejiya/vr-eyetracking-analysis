# -*- coding: utf-8 -*-
"""
çœŸå®æ•°æ®æ•´åˆAPIæ‰©å±•
ä»æ ¡å‡†æ•°æ®ã€ROIäº‹ä»¶ã€RQAç»“æœä¸­æå–å¹¶æ•´åˆ10ä¸ªå½’ä¸€åŒ–å±æ€§
"""

import os
import json
import pandas as pd
import numpy as np
from datetime import datetime
from flask import Blueprint, jsonify, request
import glob

# åˆ›å»ºBlueprint
real_data_bp = Blueprint('real_data', __name__)

class RealDataIntegrator:
    """çœŸå®æ•°æ®æ•´åˆå™¨"""
    
    def __init__(self):
        self.base_path = os.path.join(os.path.dirname(__file__), '..', 'data')
        self.output_path = os.path.join(self.base_path, 'module7_integrated_results')
        self.ensure_output_directory()
    
    def ensure_output_directory(self):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        os.makedirs(self.output_path, exist_ok=True)
    
    def get_available_rqa_configs(self):
        """è·å–å¯ç”¨çš„RQAé…ç½®"""
        rqa_path = os.path.join(self.base_path, 'rqa_pipeline_results')
        configs = []
        
        if os.path.exists(rqa_path):
            for config_dir in os.listdir(rqa_path):
                metadata_file = os.path.join(rqa_path, config_dir, 'metadata.json')
                if os.path.exists(metadata_file):
                    try:
                        with open(metadata_file, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                        configs.append({
                            'signature': metadata['signature'],
                            'parameters': metadata['parameters'],
                            'last_updated': metadata.get('last_updated'),
                            'completed': all([
                                metadata.get('step_1_completed', False),
                                metadata.get('step_2_completed', False),
                                metadata.get('step_3_completed', False),
                                metadata.get('step_4_completed', False),
                                metadata.get('step_5_completed', False)
                            ])
                        })
                    except Exception as e:
                        print(f"âš ï¸ è¯»å–é…ç½®å¤±è´¥ {config_dir}: {e}")
        
        return configs
    
    def load_calibrated_data(self):
        """åŠ è½½æ ¡å‡†æ•°æ®ä»¥è®¡ç®—æ¸¸æˆæ—¶é•¿"""
        print("ğŸ“Š åŠ è½½æ ¡å‡†æ•°æ®...")
        game_durations = {}
        
        # éå†æ‰€æœ‰ç»„åˆ«çš„æ ¡å‡†æ•°æ®
        for group in ['control', 'mci', 'ad']:
            calibrated_path = os.path.join(self.base_path, f'{group}_calibrated')
            if not os.path.exists(calibrated_path):
                continue
                
            for group_dir in os.listdir(calibrated_path):
                group_path = os.path.join(calibrated_path, group_dir)
                if not os.path.isdir(group_path):
                    continue
                    
                # è¯»å–è¯¥ç»„çš„æ‰€æœ‰CSVæ–‡ä»¶
                csv_files = glob.glob(os.path.join(group_path, '*_preprocessed_calibrated.csv'))
                for csv_file in csv_files:
                    try:
                        # ä»æ–‡ä»¶åæå–å—è¯•è€…å’Œä»»åŠ¡ä¿¡æ¯
                        filename = os.path.basename(csv_file)
                        # ä¾‹å¦‚: n1q1_preprocessed_calibrated.csv -> n1q1
                        session_id = filename.replace('_preprocessed_calibrated.csv', '')
                        
                        # è¯»å–CSVè®¡ç®—æ—¶é•¿
                        df = pd.read_csv(csv_file)
                        if len(df) > 0 and 'time_diff' in df.columns:
                            # æ¸¸æˆæ—¶é•¿ = æœ€å¤§æ—¶é—´å·® (æ¯«ç§’è½¬ç§’)
                            duration_ms = df['time_diff'].max()
                            duration_s = duration_ms / 1000.0 if duration_ms > 0 else 0
                            game_durations[session_id] = duration_s
                            
                    except Exception as e:
                        print(f"âš ï¸ å¤„ç†æ ¡å‡†æ–‡ä»¶å¤±è´¥ {csv_file}: {e}")
        
        print(f"âœ… åŠ è½½äº† {len(game_durations)} ä¸ªä¼šè¯çš„æ¸¸æˆæ—¶é•¿")
        return game_durations
    
    def load_roi_features(self):
        """åŠ è½½ROIç‰¹å¾æ•°æ®"""
        print("ğŸ“Š åŠ è½½ROIç‰¹å¾æ•°æ®...")
        roi_features = {}
        
        roi_file = os.path.join(self.base_path, 'event_analysis_results', 'All_ROI_Summary.csv')
        if not os.path.exists(roi_file):
            print(f"âŒ ROIæ–‡ä»¶ä¸å­˜åœ¨: {roi_file}")
            return roi_features
        
        try:
            df = pd.read_csv(roi_file)
            
            # æŒ‰ADQ_IDåˆ†ç»„èšåˆROIæ•°æ®
            for session_id in df['ADQ_ID'].unique():
                session_data = df[df['ADQ_ID'] == session_id]
                
                # æŒ‰ROIç±»å‹èšåˆFixTime
                roi_times = {
                    'KW': 0,    # å…³é”®è¯
                    'INST': 0,  # æŒ‡ç¤º  
                    'BG': 0     # èƒŒæ™¯
                }
                
                for _, row in session_data.iterrows():
                    roi_name = row['ROI']
                    fix_time = row['FixTime']
                    
                    if roi_name.startswith('KW_'):
                        roi_times['KW'] += fix_time
                    elif roi_name.startswith('INST_'):
                        roi_times['INST'] += fix_time
                    elif roi_name.startswith('BG_'):
                        roi_times['BG'] += fix_time
                
                roi_features[session_id] = roi_times
                
        except Exception as e:
            print(f"âŒ å¤„ç†ROIæ–‡ä»¶å¤±è´¥: {e}")
        
        print(f"âœ… åŠ è½½äº† {len(roi_features)} ä¸ªä¼šè¯çš„ROIç‰¹å¾")
        return roi_features
    
    def load_rqa_features(self, rqa_config):
        """åŠ è½½RQAç‰¹å¾æ•°æ®"""
        print(f"ğŸ“Š åŠ è½½RQAç‰¹å¾æ•°æ® ({rqa_config})...")
        rqa_features = {}
        
        rqa_path = os.path.join(self.base_path, 'rqa_pipeline_results', rqa_config, 'step1_rqa_calculation')
        
        # è¯»å–ä¸‰ä¸ªç»„åˆ«çš„RQAç»“æœ
        for group in ['control', 'mci', 'ad']:
            rqa_file = os.path.join(rqa_path, f'RQA_1D2D_summary_{group}.csv')
            if not os.path.exists(rqa_file):
                print(f"âš ï¸ RQAæ–‡ä»¶ä¸å­˜åœ¨: {rqa_file}")
                continue
                
            try:
                df = pd.read_csv(rqa_file)
                
                for _, row in df.iterrows():
                    # ä»filenameæå–session_id
                    filename = row['filename']
                    session_id = filename.replace('_preprocessed_calibrated.csv', '')
                    
                    rqa_features[session_id] = {
                        'rr_2d': row['RR-2D-xy'],
                        'rr_1d': row['RR-1D-x'], 
                        'det_2d': row['DET-2D-xy'],
                        'det_1d': row['DET-1D-x'],
                        'ent_2d': row['ENT-2D-xy'],
                        'ent_1d': row['ENT-1D-x'],
                        'group': group
                    }
                    
            except Exception as e:
                print(f"âŒ å¤„ç†RQAæ–‡ä»¶å¤±è´¥ {rqa_file}: {e}")
        
        print(f"âœ… åŠ è½½äº† {len(rqa_features)} ä¸ªä¼šè¯çš„RQAç‰¹å¾")
        return rqa_features
    
    def normalize_features(self, integrated_data):
        """å½’ä¸€åŒ–æ‰€æœ‰ç‰¹å¾ - ä½¿ç”¨ä¼˜åŒ–çš„å½’ä¸€åŒ–ç­–ç•¥"""
        print("ğŸ“Š å½’ä¸€åŒ–ç‰¹å¾æ•°æ®...")
        
        df = pd.DataFrame(integrated_data)
        
        # å®šä¹‰ç‰¹å¾å½’ä¸€åŒ–ç­–ç•¥
        normalization_strategies = {
            # æ¸¸æˆæ—¶é•¿: ä½¿ç”¨95ç™¾åˆ†ä½æ•°æˆªæ–­ï¼Œé¿å…æç«¯å¼‚å¸¸å€¼å½±å“
            'game_duration': {'method': 'percentile_clip', 'percentile': 95},
            
            # ROIæ—¶é—´: ä½¿ç”¨æ ‡å‡†Min-Maxï¼Œä½†è®¾ç½®åˆç†ä¸Šé™
            'roi_kw_time': {'method': 'percentile_clip', 'percentile': 98},
            'roi_inst_time': {'method': 'percentile_clip', 'percentile': 98}, 
            'roi_bg_time': {'method': 'percentile_clip', 'percentile': 98},
            
            # RQAç‰¹å¾: ç†è®ºä¸Šæœ‰å›ºå®šèŒƒå›´ï¼Œä½¿ç”¨æ ‡å‡†Min-Max
            'rr_1d': {'method': 'minmax'},
            'det_1d': {'method': 'minmax'},
            'ent_1d': {'method': 'minmax'},
            'rr_2d': {'method': 'minmax'},
            'det_2d': {'method': 'minmax'},
            'ent_2d': {'method': 'minmax'}
        }
        
        # åº”ç”¨å½’ä¸€åŒ–ç­–ç•¥
        for feature, strategy in normalization_strategies.items():
            if feature in df.columns:
                if strategy['method'] == 'percentile_clip':
                    # ç™¾åˆ†ä½æ•°æˆªæ–­å½’ä¸€åŒ–
                    percentile = strategy['percentile']
                    min_val = df[feature].quantile(0.05)  # ä½¿ç”¨5%åˆ†ä½æ•°ä½œä¸ºæœ€å°å€¼
                    max_val = df[feature].quantile(percentile / 100)  # ä½¿ç”¨æŒ‡å®šåˆ†ä½æ•°ä½œä¸ºæœ€å¤§å€¼
                    
                    # æˆªæ–­å¼‚å¸¸å€¼
                    clipped_values = df[feature].clip(min_val, max_val)
                    
                    if max_val > min_val:
                        df[f'{feature}_norm'] = (clipped_values - min_val) / (max_val - min_val)
                    else:
                        df[f'{feature}_norm'] = 0.5  # å¦‚æœå€¼éƒ½ç›¸åŒï¼Œè®¾ä¸ºä¸­é—´å€¼
                        
                elif strategy['method'] == 'minmax':
                    # æ ‡å‡†Min-Maxå½’ä¸€åŒ–
                    min_val = df[feature].min()
                    max_val = df[feature].max()
                    if max_val > min_val:
                        df[f'{feature}_norm'] = (df[feature] - min_val) / (max_val - min_val)
                    else:
                        df[f'{feature}_norm'] = 0.0
                        
        return df.to_dict('records')
    
    def integrate_features(self, rqa_config='m2_tau1_eps0.055_lmin2'):
        """æ•´åˆæ‰€æœ‰ç‰¹å¾æ•°æ®"""
        print(f"ğŸ”„ å¼€å§‹æ•´åˆçœŸå®æ•°æ®ç‰¹å¾ ({rqa_config})...")
        
        try:
            # 1. åŠ è½½å„ç±»æ•°æ®
            game_durations = self.load_calibrated_data()
            roi_features = self.load_roi_features() 
            rqa_features = self.load_rqa_features(rqa_config)
            
            # 2. åˆå¹¶æ•°æ®
            integrated_data = []
            all_session_ids = set(game_durations.keys()) | set(roi_features.keys()) | set(rqa_features.keys())
            
            for session_id in all_session_ids:
                # è§£æsubject_idå’Œtask_id
                if 'q' in session_id:
                    # ä¾‹å¦‚: n1q1 -> subject_id=n1q, task_id=Q1
                    parts = session_id.split('q')
                    if len(parts) == 2:
                        subject_id = f"{parts[0]}q"
                        task_id = f"Q{parts[1]}"
                    else:
                        continue
                else:
                    # ä¾‹å¦‚: m10 -> éœ€è¦ä»å…¶ä»–æ•°æ®æºæ¨æ–­ä»»åŠ¡
                    # è¿™ç§æƒ…å†µæ¯”è¾ƒå¤æ‚ï¼Œå…ˆè·³è¿‡
                    continue
                
                # ç¡®å®šç»„åˆ«
                if session_id.startswith('n'):
                    group_type = 'control'
                elif session_id.startswith('m'):
                    group_type = 'mci'
                elif session_id.startswith('ad'):
                    group_type = 'ad'
                else:
                    continue
                
                # æ•´åˆç‰¹å¾
                record = {
                    'session_id': session_id,
                    'subject_id': subject_id,
                    'task_id': task_id,
                    'group_type': group_type,
                    
                    # æ¸¸æˆæ—¶é•¿
                    'game_duration': game_durations.get(session_id, 0),
                    
                    # ROIç‰¹å¾
                    'roi_kw_time': roi_features.get(session_id, {}).get('KW', 0),
                    'roi_inst_time': roi_features.get(session_id, {}).get('INST', 0),
                    'roi_bg_time': roi_features.get(session_id, {}).get('BG', 0),
                    
                    # RQAç‰¹å¾
                    'rr_1d': rqa_features.get(session_id, {}).get('rr_1d', 0),
                    'det_1d': rqa_features.get(session_id, {}).get('det_1d', 0),
                    'ent_1d': rqa_features.get(session_id, {}).get('ent_1d', 0),
                    'rr_2d': rqa_features.get(session_id, {}).get('rr_2d', 0),
                    'det_2d': rqa_features.get(session_id, {}).get('det_2d', 0),
                    'ent_2d': rqa_features.get(session_id, {}).get('ent_2d', 0)
                }
                
                integrated_data.append(record)
            
            # 3. å½’ä¸€åŒ–
            normalized_data = self.normalize_features(integrated_data)
            
            # 4. ä¿å­˜ç»“æœ
            self.save_integrated_results(normalized_data, rqa_config)
            
            print(f"âœ… ç‰¹å¾æ•´åˆå®Œæˆ: {len(normalized_data)} æ¡è®°å½•")
            return normalized_data
            
        except Exception as e:
            print(f"âŒ ç‰¹å¾æ•´åˆå¤±è´¥: {e}")
            raise e
    
    def save_integrated_results(self, data, rqa_config):
        """ä¿å­˜æ•´åˆç»“æœ"""
        config_dir = os.path.join(self.output_path, rqa_config)
        os.makedirs(config_dir, exist_ok=True)
        
        # ä¿å­˜æ•°æ®
        df = pd.DataFrame(data)
        data_file = os.path.join(config_dir, 'integrated_features_summary.csv')
        df.to_csv(data_file, index=False, encoding='utf-8')
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'rqa_config': rqa_config,
            'generated_at': datetime.now().isoformat(),
            'record_count': len(data),
            'features': list(df.columns),
            'data_sources': {
                'calibrated_data': 'game_duration',
                'roi_events': 'roi_*_time',
                'rqa_results': 'rr_*, det_*, ent_*'
            }
        }
        
        metadata_file = os.path.join(config_dir, 'metadata.json')
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {config_dir}")
    
    def load_cached_results(self, rqa_config):
        """åŠ è½½ç¼“å­˜çš„æ•´åˆç»“æœ"""
        config_dir = os.path.join(self.output_path, rqa_config)
        data_file = os.path.join(config_dir, 'integrated_features_summary.csv')
        
        if os.path.exists(data_file):
            try:
                df = pd.read_csv(data_file)
                return df.to_dict('records')
            except Exception as e:
                print(f"âŒ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
        
        return None
    
    def get_available_rqa_configs(self):
        """è·å–æ‰€æœ‰å¯ç”¨çš„RQAé…ç½®"""
        configs = []
        rqa_results_path = os.path.join('data', 'rqa_pipeline_results')
        
        if not os.path.exists(rqa_results_path):
            return configs
        
        try:
            # æ‰«ææ‰€æœ‰RQAé…ç½®ç›®å½•
            config_dirs = [d for d in os.listdir(rqa_results_path) 
                          if os.path.isdir(os.path.join(rqa_results_path, d))]
            
            for config_dir in config_dirs:
                # æ£€æŸ¥æ˜¯å¦æœ‰RQAç»“æœæ–‡ä»¶
                config_path = os.path.join(rqa_results_path, config_dir, 'step1_rqa_calculation')
                if os.path.exists(config_path):
                    rqa_files = glob.glob(os.path.join(config_path, 'RQA_1D2D_summary_*.csv'))
                    if rqa_files:
                        # è§£æé…ç½®å‚æ•°
                        config_info = self.parse_rqa_config(config_dir)
                        config_info['id'] = config_dir
                        config_info['file_count'] = len(rqa_files)
                        configs.append(config_info)
        
        except Exception as e:
            print(f"âŒ æ‰«æRQAé…ç½®å¤±è´¥: {e}")
        
        return configs
    
    def parse_rqa_config(self, config_str):
        """è§£æRQAé…ç½®å­—ç¬¦ä¸²"""
        # ä¾‹å¦‚: m2_tau1_eps0.055_lmin2
        parts = config_str.split('_')
        config = {
            'name': config_str,
            'display_name': config_str,
            'm': 'Unknown',
            'tau': 'Unknown', 
            'eps': 'Unknown',
            'lmin': 'Unknown'
        }
        
        try:
            display_parts = []
            for part in parts:
                if part.startswith('m') and len(part) > 1 and part[1:].isdigit():
                    config['m'] = part[1:]
                    display_parts.append(f"m={part[1:]}")
                elif part.startswith('tau') and len(part) > 3:
                    config['tau'] = part[3:]
                    display_parts.append(f"Ï„={part[3:]}")
                elif part.startswith('eps') and len(part) > 3:
                    config['eps'] = part[3:]
                    display_parts.append(f"Îµ={part[3:]}")
                elif part.startswith('lmin') and len(part) > 4 and part[4:].isdigit():
                    config['lmin'] = part[4:]
                    display_parts.append(f"l_min={part[4:]}")
            
            if display_parts:
                config['display_name'] = ', '.join(display_parts)
        
        except Exception as e:
            print(f"âŒ è§£æRQAé…ç½®å¤±è´¥ {config_str}: {e}")
        
        return config
    
    def get_data_statistics(self):
        """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_subjects': 0,
            'total_sessions': 0,
            'total_tasks': 5,  # Q1-Q5
            'normalized_features': 0
        }
        
        try:
            subjects = set()
            sessions = 0
            
            # ç»Ÿè®¡æ ¡å‡†æ•°æ®
            for group in ['control_calibrated', 'mci_calibrated', 'ad_calibrated']:
                group_path = os.path.join('data', group)
                if os.path.exists(group_path):
                    for subject_dir in os.listdir(group_path):
                        subject_path = os.path.join(group_path, subject_dir)
                        if os.path.isdir(subject_path):
                            # æå–å—è¯•è€…ID
                            if group.startswith('control'):
                                subject_id = subject_dir.replace('control_group_', 'n')
                            elif group.startswith('mci'):
                                subject_id = subject_dir.replace('mci_group_', 'm')
                            elif group.startswith('ad'):
                                subject_id = subject_dir.replace('ad_group_', 'ad')
                            
                            subjects.add(subject_id)
                            
                            # ç»Ÿè®¡ä¼šè¯æ•°ï¼ˆCSVæ–‡ä»¶æ•°ï¼‰
                            csv_files = [f for f in os.listdir(subject_path) if f.endswith('.csv')]
                            sessions += len(csv_files)
            
            stats['total_subjects'] = len(subjects)
            stats['total_sessions'] = sessions
            
            # ç»Ÿè®¡å½’ä¸€åŒ–ç‰¹å¾æ•°ï¼ˆåŸå§‹+å½’ä¸€åŒ–ï¼‰
            feature_names = [
                'game_duration', 'roi_kw_time', 'roi_inst_time', 'roi_bg_time',
                'rr_1d', 'det_1d', 'ent_1d', 'rr_2d', 'det_2d', 'ent_2d'
            ]
            # åŸå§‹ç‰¹å¾ + å½’ä¸€åŒ–ç‰¹å¾ + æ ‡è¯†å­—æ®µ
            stats['normalized_features'] = len(feature_names) * 2 + 4  # session_id, subject_id, task_id, group_type
            
        except Exception as e:
            print(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        
        return stats

# åˆ›å»ºå…¨å±€æ•´åˆå™¨å®ä¾‹
integrator = RealDataIntegrator()

@real_data_bp.route('/api/available-rqa-configs', methods=['GET'])
def get_available_rqa_configs():
    """è·å–å¯ç”¨çš„RQAé…ç½®"""
    try:
        configs = integrator.get_available_rqa_configs()
        return jsonify({
            'success': True,
            'configs': configs
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@real_data_bp.route('/api/data-statistics', methods=['GET'])
def get_data_statistics():
    """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = integrator.get_data_statistics()
        return jsonify({
            'success': True,
            'statistics': stats
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@real_data_bp.route('/api/integrated-features/<rqa_config>', methods=['GET'])
def get_integrated_features(rqa_config):
    """è·å–æŒ‡å®šRQAé…ç½®çš„æ•´åˆç‰¹å¾æ•°æ®"""
    try:
        data = integrator.load_cached_results(rqa_config)
        
        if data is None:
            return jsonify({
                'success': False,
                'error': 'ç¼“å­˜æ•°æ®ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®æ•´åˆ'
            }), 404
        
        return jsonify(data)
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@real_data_bp.route('/api/integrate-real-features', methods=['POST'])
def integrate_real_features():
    """æ•´åˆçœŸå®ç‰¹å¾æ•°æ®"""
    try:
        data = request.get_json()
        rqa_config = data.get('rqa_config', 'm2_tau1_eps0.055_lmin2')
        
        # æ‰§è¡Œæ•´åˆ
        integrated_data = integrator.integrate_features(rqa_config)
        
        return jsonify(integrated_data)
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@real_data_bp.route('/api/save-module8-results', methods=['POST'])
def save_module8_results():
    """ä¿å­˜æ¨¡å—8åˆ†æç»“æœ"""
    try:
        data = request.get_json()
        
        # è·å–å‚æ•°
        csv_content = data.get('data', '')
        rqa_config = data.get('rqa_config', '')
        filename = data.get('filename', '')
        content_type = data.get('content_type', 'text/csv')
        
        if not csv_content or not rqa_config or not filename:
            return jsonify({
                'success': False,
                'error': 'ç¼ºå°‘å¿…è¦å‚æ•°: data, rqa_config, filename'
            }), 400
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        base_dir = os.path.join(os.path.dirname(__file__), '..', 'data', 'module8_analysis_results')
        rqa_dir = os.path.join(base_dir, rqa_config)
        os.makedirs(rqa_dir, exist_ok=True)
        
        # ä¿å­˜æ–‡ä»¶
        file_path = os.path.join(rqa_dir, filename)
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(csv_content)
        
        print(f"âœ… æ¨¡å—8æ–‡ä»¶å·²ä¿å­˜: {file_path}")
        
        return jsonify({
            'success': True,
            'message': f'æ–‡ä»¶å·²ä¿å­˜: {filename}',
            'file_path': file_path
        })
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æ¨¡å—8æ–‡ä»¶å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

def register_real_data_routes(app):
    """æ³¨å†ŒçœŸå®æ•°æ®æ•´åˆè·¯ç”±"""
    app.register_blueprint(real_data_bp)
    print("âœ… çœŸå®æ•°æ®æ•´åˆAPIè·¯ç”±å·²æ³¨å†Œ")