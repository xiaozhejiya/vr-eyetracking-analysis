#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
äº‹ä»¶åˆ†æAPIæ‰©å±•æ¨¡å—
ä¸ºenhanced_web_visualizer.pyæ·»åŠ äº‹ä»¶åˆ†ææ•°æ®æŸ¥è¯¢åŠŸèƒ½
"""

import os
import pandas as pd
import numpy as np
from flask import request, jsonify
from typing import Dict, Any

def clean_nan_values(obj):
    """
    é€’å½’æ¸…ç†å¯¹è±¡ä¸­çš„NaNå€¼ï¼Œå°†å…¶æ›¿æ¢ä¸ºNoneï¼ˆJSONä¸­çš„nullï¼‰
    """
    if isinstance(obj, dict):
        return {k: clean_nan_values(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_nan_values(item) for item in obj]
    elif isinstance(obj, (float, np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj):
            return None
        return float(obj)
    elif isinstance(obj, (np.int64, np.int32)):
        return int(obj)
    else:
        return obj

def add_event_analysis_routes(app, visualizer_instance):
    """
    ä¸ºFlaskåº”ç”¨æ·»åŠ äº‹ä»¶åˆ†æè·¯ç”±
    
    Args:
        app: Flaskåº”ç”¨å®ä¾‹
        visualizer_instance: EnhancedWebVisualizerå®ä¾‹
    """
    
    @app.route('/api/event-analysis/data', methods=['GET'])
    def get_event_analysis_data():
        """è·å–äº‹ä»¶åˆ†ææ•°æ®"""
        try:
            data_type = request.args.get('type', 'events')  # 'events' or 'roi'
            group = request.args.get('group', 'all')  # 'all', 'control', 'mci', 'ad'
            event_type = request.args.get('event_type', 'all')  # 'all', 'fixation', 'saccade'
            page = int(request.args.get('page', 1))
            page_size = int(request.args.get('page_size', 100))
            
            # ç¡®å®šæ•°æ®æ–‡ä»¶è·¯å¾„
            data_dir = "data/event_analysis_results"
            
            if data_type == 'events':
                if group == 'all':
                    file_path = os.path.join(data_dir, "All_Events.csv")
                else:
                    file_path = os.path.join(data_dir, f"{group}_All_Events.csv")
            else:  # roi
                if group == 'all':
                    file_path = os.path.join(data_dir, "All_ROI_Summary.csv")
                else:
                    file_path = os.path.join(data_dir, f"{group}_All_ROI_Summary.csv")
            
            if not os.path.exists(file_path):
                return jsonify({
                    'success': False,
                    'error': f'æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}'
                }), 404
            
            # è¯»å–æ•°æ®
            df = pd.read_csv(file_path)
            
            # åº”ç”¨ç­›é€‰
            if data_type == 'events' and event_type != 'all':
                df = df[df['EventType'] == event_type]
            
            # åˆ†é¡µ
            total_count = len(df)
            start_idx = (page - 1) * page_size
            end_idx = start_idx + page_size
            paginated_df = df.iloc[start_idx:end_idx]
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            data_list = paginated_df.to_dict('records')
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats = {}
            if data_type == 'events':
                if 'EventType' in df.columns:
                    stats['event_counts'] = df['EventType'].value_counts().to_dict()
                if 'Group' in df.columns:
                    stats['group_counts'] = df['Group'].value_counts().to_dict()
                if 'ROI' in df.columns:
                    roi_counts = df.dropna(subset=['ROI'])['ROI'].value_counts().head(10).to_dict()
                    stats['top_rois'] = roi_counts
            else:  # roi
                if 'Group' in df.columns:
                    stats['group_counts'] = df['Group'].value_counts().to_dict()
                if 'ROI' in df.columns:
                    stats['unique_rois'] = df['ROI'].nunique()
                    total_fix_time = df['FixTime'].sum()
                    avg_fix_time = df['FixTime'].mean()
                    stats['total_fix_time'] = total_fix_time if not pd.isna(total_fix_time) else 0
                    stats['avg_fix_time'] = avg_fix_time if not pd.isna(avg_fix_time) else 0
            
            # æ¸…ç†æ‰€æœ‰NaNå€¼
            response_data = {
                'success': True,
                'data': clean_nan_values(data_list),
                'pagination': {
                    'page': page,
                    'page_size': page_size,
                    'total_count': total_count,
                    'total_pages': (total_count + page_size - 1) // page_size
                },
                'stats': clean_nan_values(stats)
            }
            
            return jsonify(response_data)
            
        except Exception as e:
            print(f"äº‹ä»¶åˆ†ææ•°æ®æŸ¥è¯¢é”™è¯¯: {str(e)}")
            return jsonify({
                'success': False,
                'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'
            }), 500
    
    @app.route('/api/event-analysis/summary', methods=['GET'])
    def get_event_analysis_summary():
        """è·å–äº‹ä»¶åˆ†ææ•°æ®æ‘˜è¦"""
        try:
            data_dir = "data/event_analysis_results"
            
            summary = {
                'groups': ['control', 'mci', 'ad'],
                'data_files': [],
                'stats': {}
            }
            
            # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            for group in summary['groups']:
                events_file = os.path.join(data_dir, f"{group}_All_Events.csv")
                roi_file = os.path.join(data_dir, f"{group}_All_ROI_Summary.csv")
                
                file_info = {
                    'group': group,
                    'events_file_exists': os.path.exists(events_file),
                    'roi_file_exists': os.path.exists(roi_file)
                }
                
                # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œè·å–åŸºæœ¬ç»Ÿè®¡
                if file_info['events_file_exists']:
                    df_events = pd.read_csv(events_file)
                    file_info['event_count'] = len(df_events)
                    file_info['fixation_count'] = len(df_events[df_events['EventType'] == 'fixation'])
                    file_info['saccade_count'] = len(df_events[df_events['EventType'] == 'saccade'])
                
                if file_info['roi_file_exists']:
                    df_roi = pd.read_csv(roi_file)
                    file_info['roi_count'] = len(df_roi)
                    file_info['unique_roi_count'] = df_roi['ROI'].nunique()
                
                summary['data_files'].append(file_info)
            
            # å…¨å±€ç»Ÿè®¡
            all_events_file = os.path.join(data_dir, "All_Events.csv")
            all_roi_file = os.path.join(data_dir, "All_ROI_Summary.csv")
            
            if os.path.exists(all_events_file):
                df_all_events = pd.read_csv(all_events_file)
                summary['stats']['total_events'] = len(df_all_events)
                summary['stats']['total_fixations'] = len(df_all_events[df_all_events['EventType'] == 'fixation'])
                summary['stats']['total_saccades'] = len(df_all_events[df_all_events['EventType'] == 'saccade'])
                
                if 'Group' in df_all_events.columns:
                    summary['stats']['group_distribution'] = df_all_events['Group'].value_counts().to_dict()
            
            if os.path.exists(all_roi_file):
                df_all_roi = pd.read_csv(all_roi_file)
                summary['stats']['total_roi_records'] = len(df_all_roi)
                summary['stats']['unique_rois'] = df_all_roi['ROI'].nunique()
                total_fix_time = df_all_roi['FixTime'].sum()
                summary['stats']['total_fixation_time'] = total_fix_time if not pd.isna(total_fix_time) else 0
            
            # æ¸…ç†NaNå€¼åè¿”å›
            return jsonify({
                'success': True,
                'summary': clean_nan_values(summary)
            })
            
        except Exception as e:
            print(f"äº‹ä»¶åˆ†ææ‘˜è¦æŸ¥è¯¢é”™è¯¯: {str(e)}")
            return jsonify({
                'success': False,
                'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'
            }), 500
    
    @app.route('/api/event-analysis/regenerate', methods=['POST'])
    def regenerate_event_analysis_data():
        """é‡æ–°ç”Ÿæˆäº‹ä»¶åˆ†ææ•°æ®"""
        try:
            # è¿è¡Œæ•°æ®ç”Ÿæˆè„šæœ¬
            import subprocess
            import sys
            
            result = subprocess.run([
                sys.executable, 'generate_event_analysis_data.py'
            ], capture_output=True, text=True, cwd=os.getcwd())
            
            if result.returncode == 0:
                return jsonify({
                    'success': True,
                    'message': 'äº‹ä»¶åˆ†ææ•°æ®é‡æ–°ç”Ÿæˆå®Œæˆ',
                    'output': result.stdout
                })
            else:
                return jsonify({
                    'success': False,
                    'error': 'æ•°æ®ç”Ÿæˆå¤±è´¥',
                    'details': result.stderr
                }), 500
                
        except Exception as e:
            print(f"é‡æ–°ç”Ÿæˆäº‹ä»¶åˆ†ææ•°æ®é”™è¯¯: {str(e)}")
            return jsonify({
                'success': False,
                'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'
            }), 500


def setup_event_analysis_integration(app, visualizer):
    """
    è®¾ç½®äº‹ä»¶åˆ†æé›†æˆ
    
    ä½¿ç”¨æ–¹æ³•:
    åœ¨enhanced_web_visualizer.pyä¸­æ·»åŠ :
    
    try:
        from .event_api_extension import setup_event_analysis_integration
        setup_event_analysis_integration(app, self)
        print("âœ… äº‹ä»¶åˆ†æåŠŸèƒ½å·²å¯ç”¨")
    except ImportError:
        print("âš ï¸  äº‹ä»¶åˆ†æåŠŸèƒ½ä¸å¯ç”¨")
    """
    # æ·»åŠ è·¯ç”±
    add_event_analysis_routes(app, visualizer)
    
    print("ğŸ“Š äº‹ä»¶åˆ†æAPIè·¯ç”±å·²æ·»åŠ :")
    print("  - GET  /api/event-analysis/data")
    print("  - GET  /api/event-analysis/summary") 
    print("  - POST /api/event-analysis/regenerate") 