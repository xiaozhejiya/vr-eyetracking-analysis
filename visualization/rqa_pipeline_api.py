#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
RQAåˆ†ææµç¨‹APIæ¨¡å—
å®Œæ•´çš„çœ¼åŠ¨æ•°æ®é€’å½’é‡åŒ–åˆ†ææµç¨‹ï¼Œä»æ•°æ®å¤„ç†åˆ°ç»Ÿè®¡åˆ†æå†åˆ°å¯è§†åŒ–
"""

import os
import math
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import base64
import io
from datetime import datetime
from flask import Blueprint, request, jsonify
import traceback

# åˆ›å»ºè“å›¾
rqa_pipeline_bp = Blueprint('rqa_pipeline', __name__)

# å…¨å±€é…ç½®
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# åŸºç¡€æ•°æ®ç›®å½•
BASE_DATA_DIR = 'data'
PIPELINE_RESULTS_DIR = os.path.join(BASE_DATA_DIR, 'rqa_pipeline_results')

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(PIPELINE_RESULTS_DIR, exist_ok=True)


###############################################################################
# å‚æ•°ç®¡ç†å’Œç›®å½•ç»“æ„
###############################################################################

def generate_param_signature(params):
    """ç”Ÿæˆå‚æ•°ç­¾å"""
    m = params.get('m', 2)
    tau = params.get('tau', params.get('delay', 1))  # å…¼å®¹delayå’Œtau
    eps = params.get('eps', 0.05)
    lmin = params.get('lmin', 2)
    return f"m{m}_tau{tau}_eps{eps}_lmin{lmin}"


def get_param_directory(params):
    """è·å–å‚æ•°å¯¹åº”çš„ç›®å½•è·¯å¾„"""
    signature = generate_param_signature(params)
    return os.path.join(PIPELINE_RESULTS_DIR, signature)


def get_step_directory(params, step_name):
    """è·å–ç‰¹å®šæ­¥éª¤çš„ç›®å½•è·¯å¾„"""
    param_dir = get_param_directory(params)
    step_dir = os.path.join(param_dir, step_name)
    os.makedirs(step_dir, exist_ok=True)
    return step_dir


def save_param_metadata(params, step_completed):
    """ä¿å­˜å‚æ•°å…ƒæ•°æ®"""
    param_dir = get_param_directory(params)
    os.makedirs(param_dir, exist_ok=True)
    
    metadata_file = os.path.join(param_dir, 'metadata.json')
    
    # è¯»å–ç°æœ‰å…ƒæ•°æ®
    metadata = {}
    if os.path.exists(metadata_file):
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
        except:
            pass
    
    # æ›´æ–°å…ƒæ•°æ®
    metadata.update({
        'signature': generate_param_signature(params),
        'parameters': params,
        'last_updated': datetime.now().isoformat(),
        f'step_{step_completed}_completed': True
    })
    
    # ä¿å­˜å…ƒæ•°æ®
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)


def get_param_history():
    """è·å–æ‰€æœ‰å‚æ•°å†å²è®°å½•"""
    history = []
    
    if not os.path.exists(PIPELINE_RESULTS_DIR):
        return history
    
    for param_folder in os.listdir(PIPELINE_RESULTS_DIR):
        param_path = os.path.join(PIPELINE_RESULTS_DIR, param_folder)
        if os.path.isdir(param_path):
            metadata_file = os.path.join(param_path, 'metadata.json')
            if os.path.exists(metadata_file):
                try:
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    
                    # è®¡ç®—å®Œæˆæ­¥éª¤æ•°
                    completed_steps = sum(1 for i in range(1, 6) 
                                        if metadata.get(f'step_{i}_completed', False))
                    
                    history.append({
                        'signature': metadata.get('signature', param_folder),
                        'params': metadata.get('parameters', {}),
                        'completed_steps': completed_steps,
                        'progress': (completed_steps / 5) * 100,
                        'last_updated': metadata.get('last_updated', '')
                    })
                except:
                    # å¦‚æœå…ƒæ•°æ®æŸåï¼Œå°è¯•è§£ææ–‡ä»¶å¤¹å
                    try:
                        # è§£ææ–‡ä»¶å¤¹å: m2_tau1_eps0.05_lmin2
                        parts = param_folder.split('_')
                        params = {}
                        for part in parts:
                            if part.startswith('m'):
                                params['m'] = int(part[1:])
                            elif part.startswith('tau'):
                                params['tau'] = int(part[3:])
                            elif part.startswith('eps'):
                                params['eps'] = float(part[3:])
                            elif part.startswith('lmin'):
                                params['lmin'] = int(part[4:])
                        
                        # æ£€æŸ¥å®Œæˆçš„æ­¥éª¤
                        step_dirs = ['step1_rqa_calculation', 'step2_data_merging', 
                                   'step3_feature_enrichment', 'step4_statistical_analysis', 
                                   'step5_visualization']
                        completed_steps = sum(1 for step_dir in step_dirs 
                                            if os.path.exists(os.path.join(param_path, step_dir)))
                        
                        history.append({
                            'signature': param_folder,
                            'params': params,
                            'completed_steps': completed_steps,
                            'progress': (completed_steps / 5) * 100,
                            'last_updated': ''
                        })
                    except:
                        pass
    
    # æŒ‰æœ€åæ›´æ–°æ—¶é—´æ’åº
    history.sort(key=lambda x: x.get('last_updated', ''), reverse=True)
    return history


###############################################################################
# RQAè®¡ç®—æ¨¡å— - æ ¸å¿ƒç®—æ³•å®ç°
###############################################################################

def load_xy_time_series(csv_path):
    """
    è¯»å– CSVï¼ˆéœ€åŒ…å« 'x','y' åˆ—ï¼›å¯å¸¦ 'milliseconds','ROI','SequenceID' ç­‰ï¼‰ï¼Œ
    è¿”å›: x_, y_, t_, df
    """
    df = pd.read_csv(csv_path)
    if 'x' not in df.columns or 'y' not in df.columns:
        raise ValueError(f"{csv_path} ç¼ºå°‘ x æˆ– y åˆ—!")
    
    x_ = df['x'].to_numpy()
    y_ = df['y'].to_numpy()
    if 'milliseconds' in df.columns:
        t_ = df['milliseconds'].to_numpy()
    else:
        t_ = np.arange(len(x_))
    return x_, y_, t_, df


def embed_seq_1d(x_, m=2, delay=1):
    """1D: åªç”¨ x åºåˆ— => shape=(N-(m-1)*delay, m)"""
    N = len(x_)
    rows = N - (m-1)*delay
    if rows <= 0:
        return np.empty((0, m))
    emb = []
    for i in range(rows):
        emb.append([x_[i+k*delay] for k in range(m)])
    return np.array(emb)


def embed_seq_2d(x_, y_, m=2, delay=1):
    """2D: ç”¨ (x,y)ï¼Œshape=(N-(m-1)*delay, 2*m)"""
    N = len(x_)
    if len(y_) != N:
        raise ValueError("x_ å’Œ y_ é•¿åº¦ä¸åŒ¹é…!")
    rows = N - (m-1)*delay
    if rows <= 0:
        return np.empty((0, 2*m))
    emb = np.zeros((rows, 2*m), dtype=float)
    for i in range(rows):
        for k in range(m):
            emb[i, 2*k] = x_[i + k*delay]
            emb[i, 2*k+1] = y_[i + k*delay]
    return emb


def compute_rp_1dabs(emb_data, eps=0.05):
    """å½“åªç”¨ 1D embedding æ—¶ï¼Œç”¨ç»å¯¹å·®ä¹‹å’Œ â‰¤ eps => 1"""
    M = emb_data.shape[0]
    m = emb_data.shape[1]
    RP = np.zeros((M, M), dtype=int)
    for i in range(M):
        for j in range(M):
            dist = sum(abs(emb_data[i, k] - emb_data[j, k]) for k in range(m))
            if dist <= eps:
                RP[i, j] = 1
    return RP


def compute_rp_euclid(emb_data, eps=0.05):
    """å¯¹ 2D embedding ç”¨æ¬§å‡ é‡Œå¾—è·ç¦» â‰¤ eps => 1"""
    M = emb_data.shape[0]
    RP = np.zeros((M, M), dtype=int)
    for i in range(M):
        for j in range(M):
            dist = math.sqrt(np.sum((emb_data[i]-emb_data[j])**2))
            if dist <= eps:
                RP[i, j] = 1
    return RP


def extract_diag_lengths(RP):
    """ç»Ÿè®¡å¯¹è§’çº¿ä¸­ '1' çš„è¿ç»­æ®µ => {é•¿åº¦:å‡ºç°æ¬¡æ•°}"""
    N = RP.shape[0]
    length_counts = {}
    for d in range(-(N-1), N):
        line_vals = []
        for i in range(N):
            j = i + d
            if 0 <= j < N:
                line_vals.append(RP[i, j])
        
        idx = 0
        L = len(line_vals)
        while idx < L:
            if line_vals[idx] == 1:
                seg_len = 1
                idx2 = idx + 1
                while idx2 < L and line_vals[idx2] == 1:
                    seg_len += 1
                    idx2 += 1
                length_counts[seg_len] = length_counts.get(seg_len, 0) + 1
                idx = idx2
            else:
                idx += 1
    return length_counts


def compute_rqa_measures(RP, lmin=2):
    """è¿”å› RR, DET, ENT"""
    N = RP.shape[0]
    sum_ones = RP.sum()
    RR = sum_ones/(N*N)
    
    length_dict = extract_diag_lengths(RP)
    denom_all = 0
    for l, c_ in length_dict.items():
        denom_all += l*c_
    
    numer_det = 0
    denom_ent = 0
    for l, c_ in length_dict.items():
        if l >= lmin:
            numer_det += l*c_
            denom_ent += c_
    
    DET = numer_det/denom_all if denom_all > 0 else 0
    
    ENT = 0
    if denom_ent > 0:
        sum_counts_lmin = sum(c_ for (ll, c_) in length_dict.items() if ll >= lmin)
        for l, c_ in length_dict.items():
            if l >= lmin:
                p_l = c_ / sum_counts_lmin
                if p_l > 1e-12:
                    ENT += - p_l * math.log2(p_l)
    return RR, DET, ENT


def process_single_rqa_file(csv_path, m=2, delay=1, eps=0.05, lmin=2):
    """å¤„ç†å•ä¸ªæ–‡ä»¶çš„RQAè®¡ç®—"""
    try:
        x_, y_, t_, df = load_xy_time_series(csv_path)
        
        if len(x_) < 5:
            return None
        
        # æå–æ–‡ä»¶åä¸­çš„folderå’Œqä¿¡æ¯
        filename = os.path.basename(csv_path)
        # å‡è®¾æ–‡ä»¶åæ ¼å¼: n3q1_preprocessed_calibrated.csv
        parts = filename.replace('_preprocessed_calibrated.csv', '').replace('.csv', '')
        
        result = {
            "filename": filename,
            "folder": None,
            "q": None,
            "RR-2D-xy": np.nan,
            "RR-1D-x": np.nan,
            "DET-2D-xy": np.nan,
            "DET-1D-x": np.nan,
            "ENT-2D-xy": np.nan,
            "ENT-1D-x": np.nan,
        }
        
        # è§£æfolderå’Œq
        if parts.startswith('n') and 'q' in parts:
            # Controlç»„: n3q1 -> folder=3, q=1
            folder_q = parts[1:]  # å»æ‰'n'
            if 'q' in folder_q:
                folder_str, q_str = folder_q.split('q')
                try:
                    result["folder"] = int(folder_str)
                    result["q"] = int(q_str)
                except:
                    pass
        elif parts.startswith('m') and 'q' in parts:
            # MCIç»„: m1q1 -> folder=1, q=1
            folder_q = parts[1:]  # å»æ‰'m'
            if 'q' in folder_q:
                folder_str, q_str = folder_q.split('q')
                try:
                    result["folder"] = int(folder_str)
                    result["q"] = int(q_str)
                except:
                    pass
        elif parts.startswith('ad') and 'q' in parts:
            # ADç»„: ad1q1 -> folder=1, q=1
            folder_q = parts[2:]  # å»æ‰'ad'
            if 'q' in folder_q:
                folder_str, q_str = folder_q.split('q')
                try:
                    result["folder"] = int(folder_str)
                    result["q"] = int(q_str)
                except:
                    pass
        
        # 2D-xyåˆ†æ
        emb_2d = embed_seq_2d(x_, y_, m=m, delay=delay)
        if emb_2d.shape[0] >= 2:
            RP_2d = compute_rp_euclid(emb_2d, eps=eps)
            rr2d, det2d, ent2d = compute_rqa_measures(RP_2d, lmin=lmin)
            result["RR-2D-xy"] = rr2d
            result["DET-2D-xy"] = det2d
            result["ENT-2D-xy"] = ent2d
        
        # 1D-xåˆ†æ
        emb_1d = embed_seq_1d(x_, m=m, delay=delay)
        if emb_1d.shape[0] >= 2:
            RP_1d = compute_rp_1dabs(emb_1d, eps=eps)
            rr1d, det1d, ent1d = compute_rqa_measures(RP_1d, lmin=lmin)
            result["RR-1D-x"] = rr1d
            result["DET-1D-x"] = det1d
            result["ENT-1D-x"] = ent1d
        
        return result
        
    except Exception as e:
        print(f"å¤„ç†æ–‡ä»¶ {csv_path} å‡ºé”™: {e}")
        return None


###############################################################################
# æ•°æ®å¤„ç†å‡½æ•°
###############################################################################

def merge_group_data(control_csv, mci_csv, ad_csv=None):
    """åˆå¹¶ä¸‰ç»„æ•°æ®"""
    try:
        all_data_list = []
        
        # è¯»å–Controlç»„
        if os.path.exists(control_csv):
            ctg = pd.read_csv(control_csv)
            ctg["Group"] = "Control"
            ctg["ID"] = ctg.apply(lambda row: f"n{row['folder']}q{row['q']}", axis=1)
            all_data_list.append(ctg)
        
        # è¯»å–MCIç»„
        if os.path.exists(mci_csv):
            mci = pd.read_csv(mci_csv)
            mci["Group"] = "MCI"
            mci["ID"] = mci.apply(lambda row: f"m{row['folder']}q{row['q']}", axis=1)
            all_data_list.append(mci)
        
        # è¯»å–ADç»„ï¼ˆå¦‚æœæä¾›ï¼‰
        if ad_csv and os.path.exists(ad_csv):
            ad = pd.read_csv(ad_csv)
            ad["Group"] = "AD"
            ad["ID"] = ad.apply(lambda row: f"ad{row['folder']}q{row['q']}", axis=1)
            all_data_list.append(ad)
        
        if not all_data_list:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®æ–‡ä»¶")
        
        # åˆå¹¶æ•°æ®
        all_data = pd.concat(all_data_list, ignore_index=True)
        
        # è°ƒæ•´åˆ—é¡ºåº
        cols = [
            "ID", "Group", "folder", "q",
            "RR-2D-xy", "RR-1D-x", "DET-2D-xy", "DET-1D-x", "ENT-2D-xy", "ENT-1D-x"
        ]
        all_data = all_data[cols]
        
        return all_data
        
    except Exception as e:
        print(f"åˆå¹¶æ•°æ®å‡ºé”™: {e}")
        raise


def build_event_aggregates(events_csv_path):
    """æ„é€ äº‹ä»¶çº§èšåˆ"""
    if not os.path.exists(events_csv_path):
        return pd.DataFrame()

    df_evt = pd.read_csv(events_csv_path)
    df_evt["EventType"] = df_evt["EventType"].astype(str).str.lower().str.strip()
    df_evt["ADQ_ID"] = df_evt["ADQ_ID"].astype(str)
    
    # å¤„ç†å¯èƒ½å­˜åœ¨çš„åˆ—åï¼Œå…¼å®¹ä¸åŒçš„æ•°æ®æ ¼å¼
    available_cols = df_evt.columns.tolist()
    
    # æ˜ å°„åˆ—åï¼ˆå¤„ç†ä¸åŒçš„å‘½åçº¦å®šï¼‰
    col_mapping = {
        'Duration_ms': ['Duration_ms', 'duration_ms', 'Duration'],
        'Amplitude': ['Amplitude_deg', 'SaccadeAmplitude', 'amplitude', 'Amplitude'],
        'MaxVel': ['MaxVel', 'SaccadeMaxVel', 'max_vel', 'MaxVelocity']
    }
    
    # æ‰¾åˆ°å®é™…å­˜åœ¨çš„åˆ—å
    actual_cols = {}
    for key, possible_names in col_mapping.items():
        for name in possible_names:
            if name in available_cols:
                actual_cols[key] = name
                break
        if key not in actual_cols:
            actual_cols[key] = None
    
    # æ•°å€¼åŒ–å¤„ç†å­˜åœ¨çš„åˆ—
    for key, col_name in actual_cols.items():
        if col_name and col_name in df_evt.columns:
            df_evt[col_name] = pd.to_numeric(df_evt[col_name], errors="coerce")

    # fixationèšåˆ
    fix_data = df_evt[df_evt["EventType"] == "fixation"].copy()
    if not fix_data.empty and actual_cols['Duration_ms']:
        fix_agg = fix_data.groupby("ADQ_ID").agg({actual_cols['Duration_ms']: "sum"})
        fix_agg["FixCount"] = fix_data.groupby("ADQ_ID")[actual_cols['Duration_ms']].count()
        fix_agg.rename(columns={actual_cols['Duration_ms']: "FixDurSum"}, inplace=True)
        fix_agg.reset_index(inplace=True)
    else:
        # å¦‚æœæ²¡æœ‰fixationæ•°æ®æˆ–Durationåˆ—ï¼Œåˆ›å»ºç©ºçš„èšåˆ
        fix_agg = pd.DataFrame(columns=["ADQ_ID", "FixDurSum", "FixCount"])

    # saccadeèšåˆ
    sacc_data = df_evt[df_evt["EventType"] == "saccade"].copy()
    if not sacc_data.empty and actual_cols['Amplitude'] and actual_cols['MaxVel']:
        sacc_agg = sacc_data.groupby("ADQ_ID").agg({
            actual_cols['Amplitude']: "mean",
            actual_cols['MaxVel']: "max"
        })
        sacc_agg["SaccCount"] = sacc_data.groupby("ADQ_ID")[actual_cols['Amplitude']].count()
        sacc_agg.rename(columns={
            actual_cols['Amplitude']: "SaccAmpMean",
            actual_cols['MaxVel']: "SaccMaxVelPeak"
        }, inplace=True)
        sacc_agg.reset_index(inplace=True)
    else:
        # å¦‚æœæ²¡æœ‰saccadeæ•°æ®æˆ–ç›¸å…³åˆ—ï¼Œåˆ›å»ºç©ºçš„èšåˆ
        sacc_agg = pd.DataFrame(columns=["ADQ_ID", "SaccAmpMean", "SaccCount", "SaccMaxVelPeak"])

    # åˆå¹¶
    if not fix_agg.empty and not sacc_agg.empty:
        df_evt_agg = pd.merge(fix_agg, sacc_agg, on="ADQ_ID", how="outer")
    elif not fix_agg.empty:
        df_evt_agg = fix_agg
        # æ·»åŠ ç¼ºå¤±çš„saccadeåˆ—
        for col in ["SaccAmpMean", "SaccCount", "SaccMaxVelPeak"]:
            df_evt_agg[col] = 0
    elif not sacc_agg.empty:
        df_evt_agg = sacc_agg
        # æ·»åŠ ç¼ºå¤±çš„fixationåˆ—
        for col in ["FixDurSum", "FixCount"]:
            df_evt_agg[col] = 0
    else:
        # å¦‚æœéƒ½æ²¡æœ‰æ•°æ®ï¼Œè¿”å›ç©ºDataFrame
        return pd.DataFrame()
    
    # å¡«å……ç¼ºå¤±å€¼
    for c in ["FixDurSum", "FixCount", "SaccAmpMean", "SaccCount", "SaccMaxVelPeak"]:
        if c in df_evt_agg.columns:
            df_evt_agg[c] = df_evt_agg[c].fillna(0)

    return df_evt_agg


def build_roi_aggregates(roi_csv_path):
    """æ„é€ ROIçº§èšåˆ"""
    if not os.path.exists(roi_csv_path):
        return pd.DataFrame()

    df_roi = pd.read_csv(roi_csv_path)
    df_roi["ADQ_ID"] = df_roi["ADQ_ID"].astype(str)
    
    # å¤„ç†å¯èƒ½å­˜åœ¨çš„åˆ—åï¼Œå…¼å®¹ä¸åŒçš„æ•°æ®æ ¼å¼
    available_cols = df_roi.columns.tolist()
    
    col_mapping = {
        'RegressionCount': ['RegressionCount', 'regression_count', 'RegCount'],
        'FixationDuration': ['FixationDuration', 'FixTime', 'fixation_duration', 'FixDur']
    }
    
    # æ‰¾åˆ°å®é™…å­˜åœ¨çš„åˆ—å
    actual_cols = {}
    for key, possible_names in col_mapping.items():
        for name in possible_names:
            if name in available_cols:
                actual_cols[key] = name
                break
        if key not in actual_cols:
            actual_cols[key] = None

    # æ•°å€¼åŒ–å¤„ç†å­˜åœ¨çš„åˆ—
    for key, col_name in actual_cols.items():
        if col_name and col_name in df_roi.columns:
            df_roi[col_name] = pd.to_numeric(df_roi[col_name], errors="coerce")

    # æ‰§è¡Œèšåˆæ“ä½œï¼ˆå¦‚æœç›¸å…³åˆ—å­˜åœ¨ï¼‰
    agg_dict = {}
    if actual_cols['RegressionCount']:
        agg_dict[actual_cols['RegressionCount']] = "sum"
    if actual_cols['FixationDuration']:
        agg_dict[actual_cols['FixationDuration']] = "sum"

    if agg_dict:
        grp_agg = df_roi.groupby("ADQ_ID").agg(agg_dict)
        
        # é‡å‘½ååˆ—ä¸ºæ ‡å‡†åŒ–åç§°
        if actual_cols['RegressionCount'] and actual_cols['RegressionCount'] in grp_agg.columns:
            grp_agg.rename(columns={actual_cols['RegressionCount']: "RegCountSum"}, inplace=True)
        else:
            grp_agg["RegCountSum"] = 0 # å¦‚æœç¼ºå¤±åˆ™æ·»åŠ 
            
        if actual_cols['FixationDuration'] and actual_cols['FixationDuration'] in grp_agg.columns:
            grp_agg.rename(columns={actual_cols['FixationDuration']: "ROIFixDurSum"}, inplace=True)
        else:
            grp_agg["ROIFixDurSum"] = 0 # å¦‚æœç¼ºå¤±åˆ™æ·»åŠ 

        grp_agg.reset_index(inplace=True)
    else:
        # å¦‚æœæ²¡æœ‰ç›¸å…³åˆ—ç”¨äºèšåˆï¼Œè¿”å›å¸¦æœ‰æœŸæœ›åˆ—çš„ç©ºDataFrame
        grp_agg = pd.DataFrame(columns=["ADQ_ID", "RegCountSum", "ROIFixDurSum"])

    # å¡«å……ä»»ä½•å‰©ä½™çš„NaNå€¼ä¸º0
    for c in ["RegCountSum", "ROIFixDurSum"]:
        if c in grp_agg.columns:
            grp_agg[c] = grp_agg[c].fillna(0)

    return grp_agg


###############################################################################
# å¯è§†åŒ–å‡½æ•°
###############################################################################

def create_group_bar_charts(df, metrics=["RR-2D-xy", "DET-2D-xy", "ENT-2D-xy"]):
    """åˆ›å»ºç»„çº§æ¡å½¢å›¾"""
    colors = {'Control': '#ADD8E6', 'MCI': '#FFB6A4', 'AD': '#98FB98'}
    charts = []
    
    for metric in metrics:
        if metric not in df.columns:
            continue
            
        try:
            plt.figure(figsize=(8, 6))
            
            # ç§»é™¤NaNå€¼
            df_clean = df.dropna(subset=[metric])
            if df_clean.empty:
                print(f"è­¦å‘Š: {metric} åˆ—ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
                continue
            
            # è®¡ç®—ç»„åˆ«ç»Ÿè®¡
            group_stats = df_clean.groupby('Group')[metric].agg(['mean', 'std']).reset_index()
            
            # å¤„ç†NaNå€¼
            group_stats['mean'] = group_stats['mean'].fillna(0)
            group_stats['std'] = group_stats['std'].fillna(0)
            
            # ç»˜åˆ¶æ¡å½¢å›¾
            bars = plt.bar(group_stats['Group'], group_stats['mean'], 
                          color=[colors.get(g, '#cccccc') for g in group_stats['Group']],
                          yerr=group_stats['std'], capsize=5, alpha=0.8)
            
            plt.title(f'Group-level {metric} (mean Â± std)', fontsize=14, fontweight='bold')
            plt.xlabel('Group', fontsize=12, fontweight='bold')
            plt.ylabel(f'{metric} Value', fontsize=12, fontweight='bold')
            plt.grid(True, alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, (bar, mean_val, std_val) in enumerate(zip(bars, group_stats['mean'], group_stats['std'])):
                if not (np.isnan(mean_val) or np.isnan(std_val)):
                    height = bar.get_height()
                    plt.text(bar.get_x() + bar.get_width()/2., height + std_val + height*0.01,
                            f'{mean_val:.4f}', ha='center', va='bottom', fontweight='bold')
            
            plt.tight_layout()
            
            # ä¿å­˜ä¸ºbase64
            buffer = io.BytesIO()
            plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight', facecolor='white')
            buffer.seek(0)
            image_base64 = base64.b64encode(buffer.getvalue()).decode()
            buffer.close()
            plt.close()
            
            charts.append({
                'title': f'{metric} ç»„åˆ«å¯¹æ¯”',
                'metric': metric,
                'image': image_base64
            })
            
        except Exception as e:
            print(f"ç”Ÿæˆ {metric} æ¡å½¢å›¾æ—¶å‡ºé”™: {e}")
            plt.close()  # ç¡®ä¿å…³é—­å›¾å½¢
            continue
    
    return charts


def create_task_trend_chart(df, metric="RR-2D-xy"):
    """åˆ›å»ºä»»åŠ¡é—´å˜åŒ–æŠ˜çº¿å›¾ - Average RR (2D-xy) across tasks by Group"""
    colors = {'Control': '#4472C4', 'MCI': '#E15759', 'AD': '#70AD47'}  # æ›´æ¸…æ™°çš„é¢œè‰²
    
    try:
        print(f"ğŸ“Š å¼€å§‹ç”Ÿæˆ {metric} è¶‹åŠ¿å›¾...")
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_cols = [metric, 'Group', 'q']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            print(f"âŒ ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
            return None
        
        print(f"âœ… æ•°æ®åˆ—æ£€æŸ¥é€šè¿‡ï¼ŒåŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"ğŸ“ˆ åŒ…å«çš„ç»„: {df['Group'].unique()}")
        print(f"ğŸ“Š åŒ…å«çš„qå€¼: {sorted(df['q'].unique())}")
        
        # ç§»é™¤NaNå€¼
        df_clean = df.dropna(subset=required_cols)
        print(f"ğŸ§¹ æ¸…ç†åæ•°æ®å½¢çŠ¶: {df_clean.shape}")
        
        if df_clean.empty:
            print(f"âŒ è­¦å‘Š: æ¸…ç†åçš„æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆè¶‹åŠ¿å›¾")
            return None
        
        # æ£€æŸ¥æ¯ä¸ªç»„çš„æ•°æ®
        for group in ['Control', 'MCI', 'AD']:
            group_data = df_clean[df_clean['Group'] == group]
            print(f"ğŸ‘¥ ç»„ {group}: {len(group_data)} æ¡è®°å½•")
            if not group_data.empty:
                print(f"   - qå€¼èŒƒå›´: {sorted(group_data['q'].unique())}")
                print(f"   - {metric}å€¼èŒƒå›´: {group_data[metric].min():.6f} - {group_data[metric].max():.6f}")
        
        # å¼€å§‹åˆ›å»ºå›¾è¡¨
        plt.figure(figsize=(12, 8))
        
        # è®¡ç®—æ¯ä¸ªç»„åœ¨æ¯ä¸ªqå€¼çš„ç»Ÿè®¡
        print("ğŸ“Š å¼€å§‹è®¡ç®—ç»„çº§ç»Ÿè®¡...")
        avg_by_group = df_clean.groupby(['Group', 'q'])[metric].agg(['mean', 'std', 'count']).reset_index()
        print(f"ğŸ“‹ èšåˆåæ•°æ®å½¢çŠ¶: {avg_by_group.shape}")
        print(f"ğŸ“‹ èšåˆæ•°æ®é¢„è§ˆ:\n{avg_by_group.head(10)}")
        
        # å¤„ç†NaNå€¼
        avg_by_group['mean'] = avg_by_group['mean'].fillna(0)
        avg_by_group['std'] = avg_by_group['std'].fillna(0)
        
        lines_plotted = 0
        legend_labels = []
        
        # ä¸ºæ¯ä¸ªç»„ç”»çº¿
        for group in ['Control', 'MCI', 'AD']:
            if group not in avg_by_group['Group'].values:
                print(f"âš ï¸  è·³è¿‡ç»„ {group}ï¼šæ•°æ®ä¸­ä¸å­˜åœ¨")
                continue
                
            group_data = avg_by_group[avg_by_group['Group'] == group].sort_values('q')
            if group_data.empty:
                print(f"âš ï¸  è·³è¿‡ç»„ {group}ï¼šæ— æœ‰æ•ˆæ•°æ®")
                continue
            
            # è®¡ç®—æ€»æ ·æœ¬æ•°ï¼ˆæ‰€æœ‰qå€¼çš„å¹³å‡ï¼‰
            total_count = group_data['count'].mean()
            print(f"ğŸ¨ ç»˜åˆ¶ç»„ {group}ï¼Œæ•°æ®ç‚¹æ•°: {len(group_data)}ï¼Œå¹³å‡æ ·æœ¬æ•°: {total_count:.1f}")
            
            # ç»˜åˆ¶ä¸»çº¿
            line = plt.plot(group_data['q'], group_data['mean'], 
                    marker='o', label=f'{group} (nâ‰ˆ{total_count:.0f})', 
                    color=colors[group], linewidth=3, markersize=8, 
                    markerfacecolor='white', markeredgewidth=2, markeredgecolor=colors[group])
            
            # æ·»åŠ æ ‡å‡†å·®åŒºåŸŸ
            plt.fill_between(group_data['q'],
                            group_data['mean'] - group_data['std'],
                            group_data['mean'] + group_data['std'],
                            color=colors[group], alpha=0.2)
            
            lines_plotted += 1
            legend_labels.append(f'{group} (nâ‰ˆ{total_count:.0f})')
            
            # æ‰“å°æ¯ä¸ªæ•°æ®ç‚¹çš„è¯¦ç»†ä¿¡æ¯
            for _, row in group_data.iterrows():
                print(f"   Q{int(row['q'])}: mean={row['mean']:.6f}, std={row['std']:.6f}, count={row['count']}")
        
        if lines_plotted == 0:
            print(f"âŒ æ²¡æœ‰ä»»ä½•ç»„çš„æ•°æ®å¯ä»¥ç»˜åˆ¶")
            plt.close()
            return None
        
        print(f"âœ… æˆåŠŸç»˜åˆ¶äº† {lines_plotted} ä¸ªç»„çš„è¶‹åŠ¿çº¿")
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.title(f"Average {metric} across tasks by Group", fontsize=16, fontweight='bold', pad=20)
        plt.xlabel("Task (Q)", fontsize=14, fontweight='bold')
        plt.ylabel(f"Mean {metric} Value", fontsize=14, fontweight='bold')
        
        # è®¾ç½®xè½´åˆ»åº¦
        all_q_values = sorted(df_clean['q'].unique())
        plt.xticks(all_q_values, [f'Q{int(q)}' for q in all_q_values])
        
        # è®¾ç½®ç½‘æ ¼
        plt.grid(True, alpha=0.3, linestyle='--')
        
        # æ˜¾ç¤ºå›¾ä¾‹
        legend = plt.legend(title="Cognitive Groups", 
                          loc='best', frameon=True, fancybox=True, shadow=True)
        # è®¾ç½®å›¾ä¾‹æ ‡é¢˜åŠ ç²—
        legend.get_title().set_fontweight('bold')
        
        # æ ¼å¼åŒ–yè½´
        plt.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))
        
        plt.tight_layout()
        
        # ä¿å­˜ä¸ºbase64
        buffer = io.BytesIO()
        plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight', facecolor='white')
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode()
        buffer.close()
        plt.close()
        
        print(f"âœ… æˆåŠŸç”Ÿæˆ {metric} è¶‹åŠ¿å›¾")
        return {
            'title': f'Average {metric} across tasks by Group',
            'metric': metric,
            'image': image_base64,
            'description': f'è·¨ä»»åŠ¡çš„å¹³å‡{metric}å˜åŒ–è¶‹åŠ¿ (æŒ‰è®¤çŸ¥ç»„åˆ†ç»„)'
        }
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆ {metric} è¶‹åŠ¿å›¾æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        plt.close()  # ç¡®ä¿å…³é—­å›¾å½¢
        return None


###############################################################################
# APIè·¯ç”±
###############################################################################

@rqa_pipeline_bp.route('/api/rqa-pipeline/calculate', methods=['POST'])
def rqa_calculate():
    """æ­¥éª¤1: RQAè®¡ç®—"""
    try:
        data = request.get_json()
        parameters = data.get('parameters', {})
        
        # é»˜è®¤å‚æ•°
        m = parameters.get('m', 2)
        delay = parameters.get('delay', parameters.get('tau', 1))  # å…¼å®¹tauå’Œdelay
        eps = parameters.get('eps', 0.05)
        lmin = parameters.get('lmin', 2)
        
        # è·å–å‚æ•°å¯¹åº”çš„ç›®å½•
        step_dir = get_step_directory(parameters, 'step1_rqa_calculation')
        
        # æ•°æ®ç›®å½• - éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
        data_dirs = [
            'data/control_calibrated',
            'data/mci_calibrated', 
            'data/ad_calibrated'
        ]
        
        results = []
        
        # å¤„ç†æ‰€æœ‰æ•°æ®ç›®å½•
        for data_dir in data_dirs:
            if os.path.exists(data_dir):
                for root, dirs, files in os.walk(data_dir):
                    for file in files:
                        if file.endswith('_calibrated.csv'):
                            csv_path = os.path.join(root, file)
                            result = process_single_rqa_file(csv_path, m, delay, eps, lmin)
                            if result:
                                results.append(result)
        
        # æ ¹æ®ç»„åˆ«ä¿å­˜ç»“æœåˆ°å‚æ•°ç‰¹å®šç›®å½•
        control_results = [r for r in results if r['filename'].startswith('n')]
        mci_results = [r for r in results if r['filename'].startswith('m')]
        ad_results = [r for r in results if r['filename'].startswith('ad')]
        
        # ä¿å­˜åˆ°å‚æ•°ç‰¹å®šç›®å½•çš„CSVæ–‡ä»¶
        if control_results:
            control_df = pd.DataFrame(control_results)
            control_path = os.path.join(step_dir, 'RQA_1D2D_summary_control.csv')
            control_df.to_csv(control_path, index=False)
        
        if mci_results:
            mci_df = pd.DataFrame(mci_results)
            mci_path = os.path.join(step_dir, 'RQA_1D2D_summary_mci.csv')
            mci_df.to_csv(mci_path, index=False)
        
        if ad_results:
            ad_df = pd.DataFrame(ad_results)
            ad_path = os.path.join(step_dir, 'RQA_1D2D_summary_ad.csv')
            ad_df.to_csv(ad_path, index=False)
        
        # ä¿å­˜å‚æ•°å…ƒæ•°æ®
        save_param_metadata(parameters, 1)
        
        return jsonify({
            'status': 'success',
            'message': 'RQAè®¡ç®—å®Œæˆ',
            'data': {
                'param_signature': generate_param_signature(parameters),
                'total_files': len(results),
                'control_files': len(control_results),
                'mci_files': len(mci_results),
                'ad_files': len(ad_results),
                'output_directory': step_dir
            }
        })
        
    except Exception as e:
        print(f"RQAè®¡ç®—é”™è¯¯: {e}")
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': f'RQAè®¡ç®—å¤±è´¥: {str(e)}'
        }), 500


@rqa_pipeline_bp.route('/api/rqa-pipeline/merge', methods=['POST'])
def data_merge():
    """æ­¥éª¤2: æ•°æ®åˆå¹¶"""
    try:
        data = request.get_json()
        parameters = data.get('parameters', {})
        
        # è·å–å‚æ•°å¯¹åº”çš„ç›®å½•
        step1_dir = get_step_directory(parameters, 'step1_rqa_calculation')
        step2_dir = get_step_directory(parameters, 'step2_data_merging')
        
        # ä»æ­¥éª¤1çš„ç»“æœè¯»å–æ•°æ®
        control_path = os.path.join(step1_dir, 'RQA_1D2D_summary_control.csv')
        mci_path = os.path.join(step1_dir, 'RQA_1D2D_summary_mci.csv')
        ad_path = os.path.join(step1_dir, 'RQA_1D2D_summary_ad.csv')
        
        # åˆå¹¶ä¸‰ç»„æ•°æ®
        merged_data = merge_group_data(control_path, mci_path, ad_path)
        
        # ä¿å­˜åˆå¹¶ç»“æœåˆ°æ­¥éª¤2ç›®å½•
        output_path = os.path.join(step2_dir, 'All_Subjects_RQA_EyeMetrics.csv')
        merged_data.to_csv(output_path, index=False)
        
        # ä¿å­˜å‚æ•°å…ƒæ•°æ®
        save_param_metadata(parameters, 2)
        
        return jsonify({
            'status': 'success',
            'message': 'æ•°æ®åˆå¹¶å®Œæˆ',
            'data': {
                'param_signature': generate_param_signature(parameters),
                'output_file': output_path,
                'total_records': len(merged_data),
                'groups': merged_data['Group'].value_counts().to_dict()
            }
        })
        
    except Exception as e:
        print(f"æ•°æ®åˆå¹¶é”™è¯¯: {e}")
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': f'æ•°æ®åˆå¹¶å¤±è´¥: {str(e)}'
        }), 500


@rqa_pipeline_bp.route('/api/rqa-pipeline/enrich', methods=['POST'])
def feature_enrichment():
    """æ­¥éª¤3: ç‰¹å¾è¡¥å……"""
    try:
        data = request.get_json()
        parameters = data.get('parameters', {})
        
        # è·å–å‚æ•°å¯¹åº”çš„ç›®å½•
        step2_dir = get_step_directory(parameters, 'step2_data_merging')
        step3_dir = get_step_directory(parameters, 'step3_feature_enrichment')
        
        # è¯»å–åŸºç¡€RQAæ•°æ®
        rqa_path = os.path.join(step2_dir, 'All_Subjects_RQA_EyeMetrics.csv')
        if not os.path.exists(rqa_path):
            raise FileNotFoundError("æœªæ‰¾åˆ°RQAæ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆæ‰§è¡Œæ•°æ®åˆå¹¶æ­¥éª¤")
        
        df_rqa = pd.read_csv(rqa_path, dtype={"ID": str})
        df_rqa.rename(columns={"ID": "ADQ_ID"}, inplace=True)
        df_rqa["ADQ_ID"] = df_rqa["ADQ_ID"].str.replace(r"\.0$", "", regex=True)
        
        # æ„å»ºäº‹ä»¶èšåˆ
        events_path = 'data/event_analysis_results/All_Events.csv'
        df_evt_agg = build_event_aggregates(events_path)
        
        # æ„å»ºROIèšåˆ
        roi_path = 'data/event_analysis_results/All_ROI_Summary.csv'
        df_roi_agg = build_roi_aggregates(roi_path)
        
        # åˆå¹¶ç‰¹å¾
        if not df_evt_agg.empty and not df_roi_agg.empty:
            df_agg = pd.merge(df_evt_agg, df_roi_agg, on="ADQ_ID", how="outer")
        elif not df_evt_agg.empty:
            df_agg = df_evt_agg
        elif not df_roi_agg.empty:
            df_agg = df_roi_agg
        else:
            df_agg = pd.DataFrame()
        
        # åˆå¹¶åˆ°RQAæ•°æ®
        if not df_agg.empty:
            df_final = pd.merge(df_rqa, df_agg, on="ADQ_ID", how="left")
        else:
            df_final = df_rqa
        
        # æ¢å¤IDåˆ—å
        df_final.rename(columns={"ADQ_ID": "ID"}, inplace=True)
        
        # ä¿å­˜ç»“æœåˆ°æ­¥éª¤3ç›®å½•
        output_path = os.path.join(step3_dir, 'All_Subjects_RQA_EyeMetrics_Filled.csv')
        df_final.to_csv(output_path, index=False)
        
        # ä¿å­˜å‚æ•°å…ƒæ•°æ®
        save_param_metadata(parameters, 3)
        
        return jsonify({
            'status': 'success',
            'message': 'ç‰¹å¾è¡¥å……å®Œæˆ',
            'data': {
                'param_signature': generate_param_signature(parameters),
                'output_file': output_path,
                'total_records': len(df_final),
                'added_features': list(df_agg.columns) if not df_agg.empty else []
            }
        })
        
    except Exception as e:
        print(f"ç‰¹å¾è¡¥å……é”™è¯¯: {e}")
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': f'ç‰¹å¾è¡¥å……å¤±è´¥: {str(e)}'
        }), 500


@rqa_pipeline_bp.route('/api/rqa-pipeline/analyze', methods=['POST'])
def statistical_analysis():
    """æ­¥éª¤4: ç»Ÿè®¡åˆ†æ"""
    try:
        data = request.get_json()
        parameters = data.get('parameters', {})
        
        # è·å–å‚æ•°å¯¹åº”çš„ç›®å½•
        step3_dir = get_step_directory(parameters, 'step3_feature_enrichment')
        step4_dir = get_step_directory(parameters, 'step4_statistical_analysis')
        
        # è¯»å–å¡«å……åçš„æ•°æ®
        filled_path = os.path.join(step3_dir, 'All_Subjects_RQA_EyeMetrics_Filled.csv')
        if not os.path.exists(filled_path):
            raise FileNotFoundError("æœªæ‰¾åˆ°å¡«å……æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆæ‰§è¡Œç‰¹å¾è¡¥å……æ­¥éª¤")
        
        df = pd.read_csv(filled_path)
        
        # RQAæŒ‡æ ‡
        rqa_vars = ["RR-2D-xy", "DET-2D-xy", "ENT-2D-xy"]
        
        # ç»„çº§ç»Ÿè®¡
        group_stats = df.groupby("Group")[rqa_vars].describe()
        group_stats_path = os.path.join(step4_dir, 'group_stats_output.csv')
        group_stats.to_csv(group_stats_path)
        
        # å¤šå±‚æ¬¡ç»Ÿè®¡
        multi_level_stats = df.groupby(["Group", "folder", "q"])[rqa_vars].agg(["mean", "std"])
        multi_level_path = os.path.join(step4_dir, 'multi_level_stats_output.csv')
        multi_level_stats.to_csv(multi_level_path)
        
        # å‡†å¤‡è¿”å›æ•°æ®
        group_summary = []
        for group in df['Group'].unique():
            group_data = df[df['Group'] == group]
            group_summary.append({
                'Group': group,
                'Count': len(group_data),
                'RR_mean': group_data['RR-2D-xy'].mean(),
                'RR_std': group_data['RR-2D-xy'].std(),
                'DET_mean': group_data['DET-2D-xy'].mean(),
                'DET_std': group_data['DET-2D-xy'].std(),
                'ENT_mean': group_data['ENT-2D-xy'].mean(),
                'ENT_std': group_data['ENT-2D-xy'].std(),
            })
        
        # ä¿å­˜å‚æ•°å…ƒæ•°æ®
        save_param_metadata(parameters, 4)
        
        return jsonify({
            'status': 'success',
            'message': 'ç»Ÿè®¡åˆ†æå®Œæˆ',
            'data': {
                'param_signature': generate_param_signature(parameters),
                'group_stats_file': group_stats_path,
                'multi_level_stats_file': multi_level_path,
                'group_summary': group_summary
            }
        })
        
    except Exception as e:
        print(f"ç»Ÿè®¡åˆ†æé”™è¯¯: {e}")
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': f'ç»Ÿè®¡åˆ†æå¤±è´¥: {str(e)}'
        }), 500


@rqa_pipeline_bp.route('/api/rqa-pipeline/visualize', methods=['POST'])
def create_visualization():
    """æ­¥éª¤5: å¯è§†åŒ–"""
    try:
        data = request.get_json()
        parameters = data.get('parameters', {})
        
        print(f"å¼€å§‹å¯è§†åŒ–æ­¥éª¤ï¼Œå‚æ•°: {parameters}")
        
        # è·å–å‚æ•°å¯¹åº”çš„ç›®å½•
        step4_dir = get_step_directory(parameters, 'step4_statistical_analysis')
        step5_dir = get_step_directory(parameters, 'step5_visualization')
        
        print(f"å¯è§†åŒ–è¾“å‡ºç›®å½•: {step5_dir}")
        
        # è¯»å–å¡«å……åçš„æ•°æ®ï¼ˆä»æ­¥éª¤3ç›®å½•ï¼‰
        step3_dir = get_step_directory(parameters, 'step3_feature_enrichment')
        filled_path = os.path.join(step3_dir, 'All_Subjects_RQA_EyeMetrics_Filled.csv')
        if not os.path.exists(filled_path):
            raise FileNotFoundError("æœªæ‰¾åˆ°å¡«å……æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆæ‰§è¡Œç‰¹å¾è¡¥å……æ­¥éª¤")
        
        print(f"è¯»å–æ•°æ®æ–‡ä»¶: {filled_path}")
        df = pd.read_csv(filled_path)
        print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"æ•°æ®åˆ—: {df.columns.tolist()}")
        print(f"æ•°æ®ç»„åˆ«: {df['Group'].value_counts()}")
        print(f"æ•°æ®qå€¼: {df['q'].value_counts()}")
        
        # ç”Ÿæˆæ¡å½¢å›¾
        print("\n=== å¼€å§‹ç”Ÿæˆç»„çº§æ¡å½¢å›¾ ===")
        bar_charts = create_group_bar_charts(df, ["RR-2D-xy", "DET-2D-xy", "ENT-2D-xy"])
        print(f"æˆåŠŸç”Ÿæˆ {len(bar_charts)} ä¸ªæ¡å½¢å›¾")
        
        # ä¿å­˜æ¡å½¢å›¾åˆ°æ–‡ä»¶
        for i, chart in enumerate(bar_charts):
            chart_filename = f"bar_chart_{chart['metric'].replace('-', '_')}.png"
            chart_path = os.path.join(step5_dir, chart_filename)
            
            # è§£ç base64å¹¶ä¿å­˜å›¾ç‰‡
            image_data = base64.b64decode(chart['image'])
            with open(chart_path, 'wb') as f:
                f.write(image_data)
            print(f"ä¿å­˜æ¡å½¢å›¾: {chart_path}")
        
        # ç”ŸæˆæŠ˜çº¿å›¾ï¼šAverage RR (2D-xy) across tasks by Group
        print("\n=== å¼€å§‹ç”Ÿæˆä»»åŠ¡é—´å˜åŒ–è¶‹åŠ¿å›¾ ===")
        trend_chart = create_task_trend_chart(df, "RR-2D-xy")
        
        # ç»„åˆæ‰€æœ‰å›¾è¡¨
        all_charts = bar_charts[:]  # å¤åˆ¶æ¡å½¢å›¾åˆ—è¡¨
        if trend_chart:  # åªæœ‰åœ¨æˆåŠŸç”Ÿæˆæ—¶æ‰æ·»åŠ 
            all_charts.append(trend_chart)
            print("âœ… è¶‹åŠ¿å›¾å·²æˆåŠŸæ·»åŠ åˆ°å›¾è¡¨åˆ—è¡¨")
            
            # ä¿å­˜è¶‹åŠ¿å›¾åˆ°æ–‡ä»¶
            trend_filename = f"trend_chart_{trend_chart['metric'].replace('-', '_')}.png"
            trend_path = os.path.join(step5_dir, trend_filename)
            
            # è§£ç base64å¹¶ä¿å­˜å›¾ç‰‡
            image_data = base64.b64decode(trend_chart['image'])
            with open(trend_path, 'wb') as f:
                f.write(image_data)
            print(f"ä¿å­˜è¶‹åŠ¿å›¾: {trend_path}")
        else:
            print("âŒ è­¦å‘Šï¼šè¶‹åŠ¿å›¾ç”Ÿæˆå¤±è´¥ï¼Œå°†è·³è¿‡")
            print("æ£€æŸ¥æ•°æ®æ˜¯å¦åŒ…å«å¿…è¦çš„åˆ—å’Œæœ‰æ•ˆå€¼...")
            print(f"RR-2D-xyåˆ—å­˜åœ¨: {'RR-2D-xy' in df.columns}")
            print(f"Groupåˆ—å­˜åœ¨: {'Group' in df.columns}")
            print(f"qåˆ—å­˜åœ¨: {'q' in df.columns}")
            if 'RR-2D-xy' in df.columns:
                print(f"RR-2D-xyéç©ºå€¼æ•°é‡: {df['RR-2D-xy'].notna().sum()}")
                print(f"RR-2D-xyç»Ÿè®¡: {df['RR-2D-xy'].describe()}")
        
        print(f"\n=== æ€»å…±ç”Ÿæˆäº† {len(all_charts)} ä¸ªå›¾è¡¨ ===")
        
        # ä¿å­˜å›¾è¡¨åˆ—è¡¨åˆ°JSONæ–‡ä»¶
        charts_file = os.path.join(step5_dir, 'visualization_charts.json')
        with open(charts_file, 'w', encoding='utf-8') as f:
            json.dump(all_charts, f, indent=2, ensure_ascii=False)
        print(f"ä¿å­˜å›¾è¡¨JSON: {charts_file}")
        
        # å‡†å¤‡ç»„åˆ«ç»Ÿè®¡æ•°æ®
        print("\n=== è®¡ç®—ç»„åˆ«ç»Ÿè®¡æ•°æ® ===")
        group_stats = []
        for group in df['Group'].unique():
            group_data = df[df['Group'] == group]
            print(f"ç»„ {group}: {len(group_data)} æ¡è®°å½•")
            
            # å®‰å…¨çš„ç»Ÿè®¡è®¡ç®—ï¼Œå¤„ç†NaNå€¼
            def safe_float(value):
                return float(value) if not np.isnan(value) else 0.0
            
            group_stats.append({
                'Group': group,
                'RR_mean': safe_float(group_data['RR-2D-xy'].mean()),
                'RR_std': safe_float(group_data['RR-2D-xy'].std()),
                'DET_mean': safe_float(group_data['DET-2D-xy'].mean()),
                'DET_std': safe_float(group_data['DET-2D-xy'].std()),
                'ENT_mean': safe_float(group_data['ENT-2D-xy'].mean()),
                'ENT_std': safe_float(group_data['ENT-2D-xy'].std()),
            })
        
        # ä¿å­˜ç»Ÿè®¡æ•°æ®
        stats_file = os.path.join(step5_dir, 'group_statistics.json')
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(group_stats, f, indent=2, ensure_ascii=False)
        print(f"ä¿å­˜ç»Ÿè®¡æ•°æ®: {stats_file}")
        
        # åˆ—å‡ºç”Ÿæˆçš„æ‰€æœ‰æ–‡ä»¶
        print(f"\n=== ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨ ===")
        for file in os.listdir(step5_dir):
            file_path = os.path.join(step5_dir, file)
            file_size = os.path.getsize(file_path)
            print(f"- {file} ({file_size} bytes)")
        
        # ä¿å­˜å‚æ•°å…ƒæ•°æ®
        save_param_metadata(parameters, 5)
        
        return jsonify({
            'status': 'success',
            'message': 'å¯è§†åŒ–ç”Ÿæˆå®Œæˆ',
            'data': {
                'param_signature': generate_param_signature(parameters),
                'charts': all_charts,
                'group_stats': group_stats,
                'total_charts': len(all_charts),
                'output_directory': step5_dir,
                'generated_files': os.listdir(step5_dir)
            }
        })
        
    except Exception as e:
        print(f"å¯è§†åŒ–é”™è¯¯: {e}")
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': f'å¯è§†åŒ–å¤±è´¥: {str(e)}'
        }), 500


@rqa_pipeline_bp.route('/api/rqa-pipeline/status', methods=['GET'])
def get_pipeline_status():
    """è·å–æµç¨‹çŠ¶æ€"""
    try:
        # è·å–å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
        params = request.args.to_dict()
        
        if params:
            # æ£€æŸ¥ç‰¹å®šå‚æ•°ç»„åˆçš„çŠ¶æ€
            param_dir = get_param_directory(params)
            step_dirs = ['step1_rqa_calculation', 'step2_data_merging', 
                        'step3_feature_enrichment', 'step4_statistical_analysis', 
                        'step5_visualization']
            
            status = {}
            for i, step_dir in enumerate(step_dirs, 1):
                step_path = os.path.join(param_dir, step_dir)
                status[f'step{i}'] = os.path.exists(step_path) and bool(os.listdir(step_path) if os.path.exists(step_path) else False)
        else:
            # æ£€æŸ¥é»˜è®¤ä½ç½®çš„çŠ¶æ€ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
            status = {
                'step1': os.path.exists('data/RQA_1D2D_summary_control.csv'),
                'step2': os.path.exists('data/All_Subjects_RQA_EyeMetrics.csv'),
                'step3': os.path.exists('data/All_Subjects_RQA_EyeMetrics_Filled.csv'),
                'step4': os.path.exists('data/group_stats_output.csv'),
                'step5': os.path.exists('data/multi_level_stats_output.csv'),
            }
        
        completed_steps = sum(status.values())
        progress = (completed_steps / 5) * 100
        
        return jsonify({
            'status': 'success',
            'data': {
                'step_status': status,
                'completed_steps': completed_steps,
                'total_steps': 5,
                'progress_percentage': progress,
                'param_signature': generate_param_signature(params) if params else None
            }
        })
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': f'è·å–çŠ¶æ€å¤±è´¥: {str(e)}'
        }), 500


@rqa_pipeline_bp.route('/api/rqa-pipeline/param-history', methods=['GET'])
def get_param_history_api():
    """è·å–å‚æ•°å†å²è®°å½•"""
    try:
        history = get_param_history()
        
        return jsonify({
            'success': True,
            'history': history,
            'total_records': len(history)
        })
        
    except Exception as e:
        print(f"è·å–å‚æ•°å†å²é”™è¯¯: {e}")
        traceback.print_exc()
        return jsonify({
            'success': False,
            'message': f'è·å–å‚æ•°å†å²å¤±è´¥: {str(e)}'
        }), 500


@rqa_pipeline_bp.route('/api/rqa-pipeline/results/<signature>', methods=['GET'])
def get_param_results(signature):
    """è·å–æŒ‡å®šå‚æ•°ç»„åˆçš„ç»“æœ"""
    try:
        # æ ¹æ®ç­¾åæ‰¾åˆ°å¯¹åº”çš„ç›®å½•
        param_path = os.path.join(PIPELINE_RESULTS_DIR, signature)
        
        if not os.path.exists(param_path):
            return jsonify({
                'status': 'error',
                'message': f'æœªæ‰¾åˆ°å‚æ•°ç»„åˆ {signature} çš„ç»“æœ'
            }), 404
        
        # æ£€æŸ¥å“ªäº›æ­¥éª¤å·²å®Œæˆ
        step_dirs = ['step1_rqa_calculation', 'step2_data_merging', 
                    'step3_feature_enrichment', 'step4_statistical_analysis', 
                    'step5_visualization']
        
        completed_steps = []
        results = {}
        
        for step_dir in step_dirs:
            step_path = os.path.join(param_path, step_dir)
            if os.path.exists(step_path) and os.listdir(step_path):
                completed_steps.append(step_dir)
        
        # å¦‚æœå¯è§†åŒ–æ­¥éª¤å®Œæˆï¼Œè¿”å›å›¾è¡¨æ•°æ®
        if 'step5_visualization' in completed_steps:
            charts_file = os.path.join(param_path, 'step5_visualization', 'visualization_charts.json')
            stats_file = os.path.join(param_path, 'step5_visualization', 'group_statistics.json')
            
            if os.path.exists(charts_file):
                with open(charts_file, 'r', encoding='utf-8') as f:
                    results['charts'] = json.load(f)
                    
            if os.path.exists(stats_file):
                with open(stats_file, 'r', encoding='utf-8') as f:
                    results['group_stats'] = json.load(f)
        
        # è¯»å–å…ƒæ•°æ®
        metadata_file = os.path.join(param_path, 'metadata.json')
        metadata = {}
        if os.path.exists(metadata_file):
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
        
        return jsonify({
            'status': 'success',
            'data': {
                'signature': signature,
                'metadata': metadata,
                'completed_steps': completed_steps,
                'completed_count': len(completed_steps),
                'total_steps': 5,
                'results': results
            }
        })
        
    except Exception as e:
        print(f"è·å–ç»“æœé”™è¯¯: {e}")
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': f'è·å–ç»“æœå¤±è´¥: {str(e)}'
        }), 500


@rqa_pipeline_bp.route('/api/rqa-pipeline/delete/<signature>', methods=['DELETE'])
def delete_param_results(signature):
    """åˆ é™¤æŒ‡å®šå‚æ•°ç»„åˆçš„ç»“æœ"""
    try:
        param_path = os.path.join(PIPELINE_RESULTS_DIR, signature)
        
        if not os.path.exists(param_path):
            return jsonify({
                'status': 'error',
                'message': f'æœªæ‰¾åˆ°å‚æ•°ç»„åˆ {signature} çš„ç»“æœ'
            }), 404
        
        # åˆ é™¤æ•´ä¸ªå‚æ•°ç›®å½•
        import shutil
        shutil.rmtree(param_path)
        
        return jsonify({
            'status': 'success',
            'message': f'å·²åˆ é™¤å‚æ•°ç»„åˆ {signature} çš„æ‰€æœ‰ç»“æœ'
        })
        
    except Exception as e:
        print(f"åˆ é™¤ç»“æœé”™è¯¯: {e}")
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': f'åˆ é™¤ç»“æœå¤±è´¥: {str(e)}'
        }), 500 