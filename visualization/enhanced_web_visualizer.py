# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆVRçœ¼åŠ¨æ•°æ®Webå¯è§†åŒ–å™¨
æä¾›æ›´å¼ºå¤§çš„å¯è§†åŒ–åŠŸèƒ½ï¼ŒåŒ…æ‹¬ROIç»˜åˆ¶ã€è½¨è¿¹å¯è§†åŒ–ã€äº‹ä»¶æ ‡è®°ç­‰
"""
import os
import sys
import json
import cv2
import math
import base64
import webbrowser
from io import BytesIO
import numpy as np
import pandas as pd
from PIL import Image, ImageDraw, ImageFont
from flask import Flask, render_template, jsonify, request, make_response
from typing import Dict, List, Optional, Tuple

# JSONåºåˆ—åŒ–è¾…åŠ©å‡½æ•°
def convert_numpy_types(obj):
    """é€’å½’è½¬æ¢numpyæ•°æ®ç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
    if isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_numpy_types(item) for item in obj)
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    else:
        return obj

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from analysis.enhanced_eyetracking_analyzer import EnhancedEyetrackingAnalyzer

class EnhancedWebVisualizer:
    """å¢å¼ºç‰ˆWebå¯è§†åŒ–å™¨"""
    
    def __init__(self, config_file: str = "config/eyetracking_analysis_config.json"):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆWebå¯è§†åŒ–å™¨
        
        Args:
            config_file: åˆ†æé…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.analyzer = EnhancedEyetrackingAnalyzer(config_file)
        self.config = self.analyzer.config
        
        # Flaskåº”ç”¨è®¾ç½®
        self.app = Flask(__name__, 
                        template_folder='templates', 
                        static_folder='static')
        self.setup_routes()
        
        # åˆå§‹åŒ–æ•°æ®ç¼“å­˜
        self.group_data = {}
        self.background_images = []
        self.mmse_scores = {}  # æ·»åŠ MMSEåˆ†æ•°ç¼“å­˜
        
        # åŠ è½½èƒŒæ™¯å›¾ç‰‡å’ŒMMSEåˆ†æ•°
        self._load_background_images()
        self._load_mmse_scores()  # åˆå§‹åŒ–æ—¶åŠ è½½MMSEåˆ†æ•°
        
        # å¯è§†åŒ–è®¾ç½® - ä¿®å¤é¢œè‰²é…ç½®åˆå¹¶é—®é¢˜
        self.visualization_config = self.config.get("visualization", {})
        
        # æ­£ç¡®åˆå¹¶é¢œè‰²é…ç½®ï¼Œç¡®ä¿ROIé¢œè‰²ä¸ç¼ºå¤±
        default_colors = self.get_default_colors()
        config_colors = self.visualization_config.get("colors", {})
        self.colors = {**default_colors, **config_colors}
        
        # æ­£ç¡®åˆå¹¶å°ºå¯¸é…ç½®
        default_sizes = self.get_default_sizes()
        config_sizes = self.visualization_config.get("sizes", {})
        self.sizes = {**default_sizes, **config_sizes}
        
        # é›†æˆRQAåˆ†æåŠŸèƒ½
        try:
            # å°è¯•ç›¸å¯¹å¯¼å…¥ï¼ˆç”¨äºåŒ…å¯¼å…¥ï¼‰
            try:
                from .rqa_api_extension import setup_rqa_integration
            except ImportError:
                # å°è¯•ç»å¯¹å¯¼å…¥ï¼ˆç”¨äºç›´æ¥è¿è¡Œï¼‰
                from rqa_api_extension import setup_rqa_integration
            
            setup_rqa_integration(self.app, self)
            print("âœ… RQAåˆ†æåŠŸèƒ½å·²å¯ç”¨")
        except ImportError as e:
            print(f"âš ï¸  RQAåˆ†æåŠŸèƒ½ä¸å¯ç”¨: {e}")
        
        # é›†æˆäº‹ä»¶åˆ†æåŠŸèƒ½
        try:
            # å°è¯•ç›¸å¯¹å¯¼å…¥ï¼ˆç”¨äºåŒ…å¯¼å…¥ï¼‰
            try:
                from .event_api_extension import setup_event_analysis_integration
            except ImportError:
                # å°è¯•ç»å¯¹å¯¼å…¥ï¼ˆç”¨äºç›´æ¥è¿è¡Œï¼‰
                from event_api_extension import setup_event_analysis_integration
            
            setup_event_analysis_integration(self.app, self)
            print("âœ… äº‹ä»¶åˆ†æåŠŸèƒ½å·²å¯ç”¨")
        except ImportError as e:
            print(f"âš ï¸  äº‹ä»¶åˆ†æåŠŸèƒ½ä¸å¯ç”¨: {e}")
        
        # é›†æˆMMSEå¯¹æ¯”åˆ†æåŠŸèƒ½
        try:
            # å°è¯•ç›¸å¯¹å¯¼å…¥ï¼ˆç”¨äºåŒ…å¯¼å…¥ï¼‰
            try:
                from .mmse_api_extension import register_mmse_routes
            except ImportError:
                # å°è¯•ç»å¯¹å¯¼å…¥ï¼ˆç”¨äºç›´æ¥è¿è¡Œï¼‰
                from mmse_api_extension import register_mmse_routes
            
            register_mmse_routes(self.app)
            print("âœ… MMSEå¯¹æ¯”åˆ†æåŠŸèƒ½å·²å¯ç”¨")
        except ImportError as e:
            print(f"âš ï¸  MMSEå¯¹æ¯”åˆ†æåŠŸèƒ½ä¸å¯ç”¨: {e}")
        
        # é›†æˆçœŸå®æ•°æ®æ•´åˆåŠŸèƒ½
        try:
            # å°è¯•ç›¸å¯¹å¯¼å…¥ï¼ˆç”¨äºåŒ…å¯¼å…¥ï¼‰
            try:
                from .real_data_integration_api import register_real_data_routes
            except ImportError:
                # å°è¯•ç»å¯¹å¯¼å…¥ï¼ˆç”¨äºç›´æ¥è¿è¡Œï¼‰
                from real_data_integration_api import register_real_data_routes
            
            register_real_data_routes(self.app)
            print("âœ… çœŸå®æ•°æ®æ•´åˆåŠŸèƒ½å·²å¯ç”¨")
        except ImportError as e:
            print(f"âš ï¸  çœŸå®æ•°æ®æ•´åˆåŠŸèƒ½ä¸å¯ç”¨: {e}")
        
        # ğŸ†• é›†æˆæœºå™¨å­¦ä¹ é¢„æµ‹åŠŸèƒ½ (æ¨¡å—9)
        try:
            # å°è¯•ç›¸å¯¹å¯¼å…¥ï¼ˆç”¨äºåŒ…å¯¼å…¥ï¼‰
            try:
                from .ml_prediction_api import register_ml_prediction_routes
            except ImportError:
                # å°è¯•ç»å¯¹å¯¼å…¥ï¼ˆç”¨äºç›´æ¥è¿è¡Œï¼‰
                from ml_prediction_api import register_ml_prediction_routes
            
            register_ml_prediction_routes(self.app)
            print("âœ… æœºå™¨å­¦ä¹ é¢„æµ‹åŠŸèƒ½å·²å¯ç”¨ (æ¨¡å—9)")
        except ImportError as e:
            print(f"âš ï¸  æœºå™¨å­¦ä¹ é¢„æµ‹åŠŸèƒ½ä¸å¯ç”¨: {e}")
        
                # é›†æˆRQAåˆ†ææµç¨‹åŠŸèƒ½
        try:
            # å°è¯•ç›¸å¯¹å¯¼å…¥ï¼ˆç”¨äºåŒ…å¯¼å…¥ï¼‰
            try:
                from .rqa_pipeline_api import rqa_pipeline_bp
            except ImportError:
                # å°è¯•ç»å¯¹å¯¼å…¥ï¼ˆç”¨äºç›´æ¥è¿è¡Œï¼‰
                from rqa_pipeline_api import rqa_pipeline_bp

            self.app.register_blueprint(rqa_pipeline_bp)
            print("âœ… RQAåˆ†ææµç¨‹åŠŸèƒ½å·²å¯ç”¨")
        except ImportError as e:
            print(f"âš ï¸  RQAåˆ†ææµç¨‹åŠŸèƒ½ä¸å¯ç”¨: {e}")

        # é›†æˆç»¼åˆç‰¹å¾æå–åŠŸèƒ½
        try:
            # å°è¯•ç›¸å¯¹å¯¼å…¥ï¼ˆç”¨äºåŒ…å¯¼å…¥ï¼‰
            try:
                from .feature_extraction_api import feature_extraction_bp
            except ImportError:
                # å°è¯•ç»å¯¹å¯¼å…¥ï¼ˆç”¨äºç›´æ¥è¿è¡Œï¼‰
                from feature_extraction_api import feature_extraction_bp

            self.app.register_blueprint(feature_extraction_bp)
            print("âœ… ç»¼åˆç‰¹å¾æå–åŠŸèƒ½å·²å¯ç”¨")
        except ImportError as e:
            print(f"âš ï¸  ç»¼åˆç‰¹å¾æå–åŠŸèƒ½ä¸å¯ç”¨: {e}")
        
        # é›†æˆæ¨¡å—10 Eye-Index ç»¼åˆè¯„ä¼°åŠŸèƒ½
        try:
            try:
                from .module10_eye_index.api import register_eye_index_routes
            except ImportError:
                # å°è¯•ç»å¯¹å¯¼å…¥ï¼ˆç”¨äºç›´æ¥è¿è¡Œï¼‰
                from module10_eye_index.api import register_eye_index_routes

            register_eye_index_routes(self.app)
            
            # æ·»åŠ æ¨¡å—10é™æ€æ–‡ä»¶è·¯ç”±
            @self.app.route('/static/js/eye_index.js')
            def eye_index_js():
                try:
                    from flask import send_from_directory
                    return send_from_directory(
                        os.path.join(os.path.dirname(__file__), 'module10_eye_index', 'static', 'js'),
                        'eye_index.js'
                    )
                except Exception as e:
                    print(f"âŒ åŠ è½½eye_index.jså¤±è´¥: {e}")
                    return "console.log('Eye-Index JSåŠ è½½å¤±è´¥');", 404
            
            print("âœ… æ¨¡å—10 Eye-Index ç»¼åˆè¯„ä¼°åŠŸèƒ½å·²å¯ç”¨")
        except ImportError as e:
            print(f"âš ï¸  æ¨¡å—10 Eye-Index ç»¼åˆè¯„ä¼°åŠŸèƒ½ä¸å¯ç”¨: {e}")
        
        # é›†æˆæ¨¡å—10-C æ¨¡å‹æœåŠ¡ä¸ç®¡ç†API
        try:
            import sys
            import os
            
            # æ·»åŠ backendè·¯å¾„
            backend_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'backend')
            if backend_path not in sys.path:
                sys.path.insert(0, backend_path)
            
            # å°è¯•å¯¼å…¥
            try:
                from backend.m10_service import service_bp, initialize_models
            except ImportError:
                try:
                    from m10_service import service_bp, initialize_models
                except ImportError:
                    # æœ€åå°è¯•ç›¸å¯¹å¯¼å…¥
                    from ..backend.m10_service import service_bp, initialize_models

            # æ³¨å†Œè“å›¾åˆ° /api/m10 è·¯å¾„å‰ç¼€
            self.app.register_blueprint(service_bp, url_prefix="/api/m10")
            
            # åˆå§‹åŒ–æ¨¡å‹ï¼ˆæ¿€æ´»bestç‰ˆæœ¬ï¼‰
            activated_count = initialize_models()
            
            print(f"âœ… æ¨¡å—10-C æ¨¡å‹æœåŠ¡APIå·²å¯ç”¨ ({activated_count} ä¸ªæ¨¡å‹æ¿€æ´»)")
        except ImportError as e:
            print(f"âš ï¸  æ¨¡å—10-C æ¨¡å‹æœåŠ¡APIä¸å¯ç”¨: {e}")
        except Exception as e:
            print(f"âš ï¸  æ¨¡å—10-C åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # é›†æˆæ¨¡å—10-B PyTorchè®­ç»ƒå¼•æ“API
        try:
            # å°è¯•å¯¼å…¥æ¨¡å—10-Bçš„Blueprint
            try:
                from backend.m10_training.api import m10b_bp
            except ImportError:
                try:
                    from m10_training.api import m10b_bp
                except ImportError:
                    # æœ€åå°è¯•ç›¸å¯¹å¯¼å…¥
                    from ..backend.m10_training.api import m10b_bp

            # æ³¨å†Œè“å›¾åˆ° /api/m10b è·¯å¾„å‰ç¼€
            self.app.register_blueprint(m10b_bp)
            
            print("âœ… æ¨¡å—10-B PyTorchè®­ç»ƒå¼•æ“APIå·²å¯ç”¨")
        except ImportError as e:
            print(f"âš ï¸  æ¨¡å—10-B è®­ç»ƒå¼•æ“APIä¸å¯ç”¨: {e}")
        except Exception as e:
            print(f"âš ï¸  æ¨¡å—10-B åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # é›†æˆæ¨¡å—10-D æ¨¡å‹æ€§èƒ½è¯„ä¼°API
        try:
            # å°è¯•å¯¼å…¥æ¨¡å—10-Dçš„Blueprint
            try:
                from backend.m10_evaluation.api import evaluation_bp
            except ImportError:
                try:
                    from m10_evaluation.api import evaluation_bp
                except ImportError:
                    # æœ€åå°è¯•ç›¸å¯¹å¯¼å…¥
                    from ..backend.m10_evaluation.api import evaluation_bp

            # æ³¨å†Œè“å›¾åˆ° /api/m10d è·¯å¾„å‰ç¼€
            self.app.register_blueprint(evaluation_bp, url_prefix="/api/m10d")
            
            print("âœ… æ¨¡å—10-D æ¨¡å‹æ€§èƒ½è¯„ä¼°APIå·²å¯ç”¨")
        except ImportError as e:
            print(f"âš ï¸  æ¨¡å—10-D æ€§èƒ½è¯„ä¼°APIä¸å¯ç”¨: {e}")
        except Exception as e:
            print(f"âš ï¸  æ¨¡å—10-D åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def get_default_colors(self) -> Dict:
        """è·å–é»˜è®¤é¢œè‰²é…ç½®"""
        return {
            'trajectory': (200, 80, 255),
            'fixation': (0, 0, 255),
            'saccade': (255, 100, 100),
            'start_point': (0, 255, 0),
            'end_point': (255, 0, 0),
            'roi_background': (0, 128, 255),
            'roi_instructions': (255, 165, 0),
            'roi_keywords': (255, 0, 0),
            'sequence_enter': (255, 0, 0),
            'sequence_exit': (0, 255, 0)
        }
    
    def get_default_sizes(self) -> Dict:
        """è·å–é»˜è®¤å°ºå¯¸é…ç½®"""
        return {
            'trajectory_width': 2,
            'fixation_radius': 1,
            'saccade_radius': 1,
            'start_end_radius': 3,
            'roi_alpha_bg': 60,     # èƒŒæ™¯ROIé€æ˜åº¦
            'roi_alpha_inst': 80,   # æŒ‡ä»¤ROIé€æ˜åº¦
            'roi_alpha_kw': 100,    # å…³é”®è¯ROIé€æ˜åº¦
            'font_size': 16
        }
        
    def setup_routes(self):
        """è®¾ç½®Flaskè·¯ç”±"""
        
        @self.app.route('/')
        def index():
            """ä¸»é¡µ"""
            return render_template('enhanced_index.html')
            
        @self.app.route('/test_frontend_params.html')
        def test_params():
            """å‚æ•°é…ç½®æµ‹è¯•é¡µé¢"""
            try:
                with open('test_frontend_params.html', 'r', encoding='utf-8') as f:
                    return f.read()
            except FileNotFoundError:
                return "<h1>æµ‹è¯•é¡µé¢æœªæ‰¾åˆ°</h1><p>è¯·ç¡®ä¿test_frontend_params.htmlæ–‡ä»¶å­˜åœ¨</p>"
        
        @self.app.route('/api/groups')
        def get_groups():
            """è·å–æ‰€æœ‰ç»„çš„ä¿¡æ¯"""
            return jsonify(self.get_groups_overview())
        
        @self.app.route('/api/group/<group_type>/data')
        def get_group_data(group_type):
            """è·å–æŒ‡å®šç»„çš„æ•°æ®åˆ—è¡¨"""
            return jsonify(self.get_group_data(group_type))
        
        @self.app.route('/api/visualize/<group_type>/<data_id>')
        def visualize_data(group_type, data_id):
            """ç”Ÿæˆå¢å¼ºç‰ˆæ•°æ®å¯è§†åŒ–"""
            # è·å–å¯è§†åŒ–å‚æ•°
            fixation_size = request.args.get('fixationSize', 3, type=int)
            trajectory_width = request.args.get('trajectoryWidth', 2, type=int)
            trajectory_style = request.args.get('trajectoryStyle', 'solid')
            point_size = request.args.get('pointSize', 1, type=int)
            
            # è·å–æ ¡å‡†åç§»é‡å‚æ•°ï¼ˆç”¨äºé¢„è§ˆï¼‰
            x_offset = request.args.get('xOffset', 0, type=float)
            y_offset = request.args.get('yOffset', 0, type=float)
            preview_mode = request.args.get('preview', False, type=bool)
            
            # è·å–æ—¶é—´èŒƒå›´å‚æ•°ï¼ˆç”¨äºæ—¶é—´æ ¡å‡†ï¼‰
            time_start = request.args.get('timeStart', 0, type=float)  # ç™¾åˆ†æ¯”
            time_end = request.args.get('timeEnd', 100, type=float)    # ç™¾åˆ†æ¯”
            
            vis_params = {
                'fixation_size': fixation_size,
                'trajectory_width': trajectory_width,
                'trajectory_style': trajectory_style,
                'point_size': point_size,
                'x_offset': x_offset,
                'y_offset': y_offset,
                'preview_mode': preview_mode,
                'time_start': time_start,
                'time_end': time_end
            }
            
            return jsonify(self.generate_enhanced_visualization(group_type, data_id, vis_params))
        
        @self.app.route('/api/statistics/<group_type>')
        def get_statistics(group_type):
            """è·å–ç»„ç»Ÿè®¡ä¿¡æ¯"""
            return jsonify(self.get_group_statistics(group_type))
        
        @self.app.route('/api/process/<group_type>/<data_id>')
        def process_single_data(group_type, data_id):
            """å¤„ç†å•ä¸ªæ•°æ®æ–‡ä»¶å¹¶ç”Ÿæˆè¯¦ç»†åˆ†æ"""
            return jsonify(self.process_single_adq(group_type, data_id))

        @self.app.route('/api/upload-group', methods=['POST'])
        def upload_file_group():
            """æ‰¹é‡ä¸Šä¼ æ•°æ®æ–‡ä»¶ç»„"""
            try:
                if 'files' not in request.files:
                    return jsonify({'success': False, 'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'})
                
                files = request.files.getlist('files')
                group = request.form.get('group')
                
                if len(files) == 0:
                    return jsonify({'success': False, 'error': 'æ–‡ä»¶åˆ—è¡¨ä¸ºç©º'})
                
                if not group or group not in ['control', 'mci', 'ad']:
                    return jsonify({'success': False, 'error': 'æ— æ•ˆçš„åˆ†ç»„'})
                
                # éªŒè¯æ–‡ä»¶æ•°é‡å’Œåç§°
                if len(files) != 5:
                    return jsonify({'success': False, 'error': f'å¿…é¡»ä¸Šä¼ 5ä¸ªæ–‡ä»¶ï¼Œå½“å‰ä¸Šä¼ äº†{len(files)}ä¸ª'})
                
                # æ”¯æŒä¸¤ç§æ–‡ä»¶åæ ¼å¼ï¼š1.txt-5.txt æˆ– level_1.txt-level_5.txt
                standard_names = {'1.txt', '2.txt', '3.txt', '4.txt', '5.txt'}
                level_names = {'level_1.txt', 'level_2.txt', 'level_3.txt', 'level_4.txt', 'level_5.txt'}
                uploaded_names = {f.filename for f in files}
                
                if uploaded_names != standard_names and uploaded_names != level_names:
                    return jsonify({'success': False, 'error': 'æ–‡ä»¶åå¿…é¡»æ˜¯1.txtåˆ°5.txtæˆ–level_1.txtåˆ°level_5.txt'})
                
                # è°ƒç”¨æ–‡ä»¶ç»„ä¸Šä¼ å¤„ç†æ–¹æ³•
                result = self.handle_file_group_upload(files, group)
                return jsonify(result)
                
            except Exception as e:
                return jsonify({'success': False, 'error': str(e)})
        
        @self.app.route('/api/process-group/<group_id>', methods=['POST'])
        def process_file_group(group_id):
            """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ç»„"""
            try:
                # è°ƒç”¨æ–‡ä»¶ç»„å¤„ç†æ–¹æ³•
                result = self.process_uploaded_file_group(group_id)
                return jsonify(result)
                
            except Exception as e:
                return jsonify({'success': False, 'error': str(e)})
 
        @self.app.route('/api/save-calibration', methods=['POST'])
        def save_calibration():
             """ä¿å­˜æ ¡å‡†åç§»é‡åˆ°æ•°æ®æ–‡ä»¶"""
             try:
                 data = request.get_json()
                 
                 group_type = data.get('groupType')
                 data_id = data.get('dataId')
                 x_offset = data.get('xOffset', 0)
                 y_offset = data.get('yOffset', 0)
                 time_start = data.get('timeStart', 0)
                 time_end = data.get('timeEnd', 100)
                 
                 if not group_type or not data_id:
                     return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'})
                 
                 # è°ƒç”¨æ ¡å‡†ä¿å­˜æ–¹æ³•
                 result = self.save_data_calibration(group_type, data_id, x_offset, y_offset, time_start, time_end)
                 return jsonify(result)
                 
             except Exception as e:
                 return jsonify({'success': False, 'error': str(e)})

        @self.app.route('/api/time-info/<group_type>/<data_id>')
        def get_time_info(group_type, data_id):
            """è·å–æ•°æ®çš„æ—¶é—´ä¿¡æ¯"""
            try:
                result = self.get_data_time_info(group_type, data_id)
                return jsonify(result)
            except Exception as e:
                return jsonify({'success': False, 'error': str(e)})

        @self.app.route('/api/data/<data_id>', methods=['DELETE'])
        def delete_data(data_id):
            """åˆ é™¤æ•°æ®æ–‡ä»¶ï¼ˆæ•´ç»„ï¼‰"""
            try:
                result = self.delete_data_group(data_id)
                return jsonify(result)
            except Exception as e:
                return jsonify({'success': False, 'error': str(e)})
        
        @self.app.route('/api/data/<data_id>/move', methods=['POST'])
        def move_data(data_id):
            """ç§»åŠ¨æ•°æ®åˆ°ä¸åŒç»„åˆ«"""
            try:
                data = request.get_json()
                from_group = data.get('fromGroup')
                to_group = data.get('toGroup')
                
                if not from_group or not to_group:
                    return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'})
                
                result = self.move_data_between_groups(data_id, from_group, to_group)
                return jsonify(result)
            except Exception as e:
                return jsonify({'success': False, 'error': str(e)})

        @self.app.route('/api/fix-data-files', methods=['POST'])
        def fix_data_files():
            """ä¿®å¤ç°æœ‰æ•°æ®æ–‡ä»¶ï¼Œæ·»åŠ ç¼ºå¤±çš„millisecondsåˆ—"""
            try:
                data = request.get_json() or {}
                group_type = data.get('groupType')  # å¯é€‰ï¼ŒæŒ‡å®šè¦ä¿®å¤çš„ç»„ç±»å‹
                
                result = self.fix_existing_data_files(group_type)
                return jsonify(result)
            except Exception as e:
                return jsonify({'success': False, 'error': str(e)})

        @self.app.route('/api/mmse-scores/<group_type>/<int:group_num>')
        def get_mmse_score_api(group_type, group_num):
            """è·å–æŒ‡å®šç»„çš„MMSEåˆ†æ•°"""
            try:
                mmse_data = self.get_mmse_score(group_type, group_num)
                if mmse_data:
                    # æ·»åŠ è¯„ä¼°ç­‰çº§ä¿¡æ¯
                    assessment = self.get_mmse_assessment_level(mmse_data['total_score'])
                    mmse_data['assessment'] = assessment
                    return jsonify({'success': True, 'data': mmse_data})
                else:
                    return jsonify({'success': False, 'error': 'æœªæ‰¾åˆ°MMSEåˆ†æ•°æ•°æ®'})
            except Exception as e:
                return jsonify({'success': False, 'error': str(e)})

        @self.app.route('/api/mmse-scores/<group_type>')
        def get_group_mmse_scores(group_type):
            """è·å–æŒ‡å®šç»„ç±»å‹çš„æ‰€æœ‰MMSEåˆ†æ•°"""
            try:
                group_scores = self.mmse_scores.get(group_type, {})
                result = {}
                for group_num, mmse_data in group_scores.items():
                    assessment = self.get_mmse_assessment_level(mmse_data['total_score'])
                    result[group_num] = {
                        **mmse_data,
                        'assessment': assessment
                    }
                return jsonify({'success': True, 'data': result})
            except Exception as e:
                return jsonify({'success': False, 'error': str(e)})

        @self.app.route('/api/mmse-statistics')
        def get_mmse_statistics():
            """è·å–æ‰€æœ‰ç»„çš„MMSEåˆ†æ•°ç»Ÿè®¡"""
            try:
                stats = {
                    'control': {'count': 0, 'avg_score': 0, 'min_score': 30, 'max_score': 0, 'scores': []},
                    'mci': {'count': 0, 'avg_score': 0, 'min_score': 30, 'max_score': 0, 'scores': []},
                    'ad': {'count': 0, 'avg_score': 0, 'min_score': 30, 'max_score': 0, 'scores': []}
                }
                
                for group_type, group_scores in self.mmse_scores.items():
                    if group_scores:
                        scores = [data['total_score'] for data in group_scores.values()]
                        stats[group_type].update({
                            'count': len(scores),
                            'avg_score': sum(scores) / len(scores),
                            'min_score': min(scores),
                            'max_score': max(scores),
                            'scores': scores
                        })
                
                return jsonify({'success': True, 'data': stats})
            except Exception as e:
                return jsonify({'success': False, 'error': str(e)})

        @self.app.route('/static/modules/<path:filename>')
        def serve_module_file(filename):
            """æä¾›æ¨¡å—HTMLæ–‡ä»¶"""
            try:
                from flask import send_from_directory
                module_dir = os.path.join(self.app.static_folder, 'modules')
                return send_from_directory(module_dir, filename)
            except Exception as e:
                return f"æ¨¡å—æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}", 404
        
        @self.app.route('/static/normalized_features/<path:filename>')
        def serve_normalized_features(filename):
            """æä¾›å½’ä¸€åŒ–ç‰¹å¾æ•°æ®æ–‡ä»¶ï¼ˆå¢å¼ºè°ƒè¯•ç‰ˆæœ¬ï¼‰"""
            import time
            start_time = time.time()
            
            print(f"ğŸŒ === æ”¶åˆ°å½’ä¸€åŒ–ç‰¹å¾æ–‡ä»¶è¯·æ±‚ ===")
            print(f"ğŸ“ è¯·æ±‚æ–‡ä»¶: {filename}")
            print(f"ğŸ• è¯·æ±‚æ—¶é—´: {time.strftime('%H:%M:%S')}")
            
            try:
                from flask import send_from_directory, Response, request
                
                # è·å–æ­£ç¡®çš„æ•°æ®ç›®å½•è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰
                project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
                features_dir = os.path.join(project_root, 'data', 'normalized_features')
                
                print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {project_root}")
                print(f"ğŸ“ ç‰¹å¾æ•°æ®ç›®å½•: {features_dir}")
                print(f"ğŸ“ ç›®å½•æ˜¯å¦å­˜åœ¨: {os.path.exists(features_dir)}")
                
                if os.path.exists(features_dir):
                    files_in_dir = os.listdir(features_dir)
                    print(f"ğŸ“‚ ç›®å½•ä¸­çš„æ–‡ä»¶: {files_in_dir}")
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                file_path = os.path.join(features_dir, filename)
                print(f"ğŸ“„ å®Œæ•´æ–‡ä»¶è·¯å¾„: {file_path}")
                print(f"ğŸ“„ æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {os.path.exists(file_path)}")
                
                if not os.path.exists(file_path):
                    print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    return f"å½’ä¸€åŒ–ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {filename}", 404
                
                # è·å–æ–‡ä»¶ä¿¡æ¯
                file_size = os.path.getsize(file_path)
                print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
                
                print(f"ğŸ“– å¼€å§‹è¯»å–æ–‡ä»¶...")
                read_start = time.time()
                
                # è¯»å–å¹¶è¿”å›CSVæ–‡ä»¶
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                read_time = time.time() - read_start
                print(f"ğŸ“– æ–‡ä»¶è¯»å–å®Œæˆï¼Œç”¨æ—¶: {read_time:.3f}ç§’")
                print(f"ğŸ“Š å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                print(f"ğŸ“Š å†…å®¹å‰100å­—ç¬¦: {content[:100]}")
                
                response = Response(content, mimetype='text/csv', headers={
                    'Content-Disposition': f'inline; filename="{filename}"',
                    'Access-Control-Allow-Origin': '*',
                    'Cache-Control': 'no-cache'
                })
                
                total_time = time.time() - start_time
                print(f"âœ… æ–‡ä»¶æä¾›æˆåŠŸï¼Œæ€»ç”¨æ—¶: {total_time:.3f}ç§’")
                print(f"ğŸŒ === è¯·æ±‚å¤„ç†å®Œæˆ ===")
                
                return response
                
            except Exception as e:
                error_time = time.time() - start_time
                print(f"âŒ æä¾›CSVæ–‡ä»¶å¤±è´¥: {str(e)}")
                print(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
                print(f"âŒ å¤±è´¥ç”¨æ—¶: {error_time:.3f}ç§’")
                print(f"âŒ é”™è¯¯å †æ ˆ: {str(e)}")
                print(f"ğŸŒ === è¯·æ±‚å¤±è´¥ ===")
                return f"å½’ä¸€åŒ–ç‰¹å¾æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}", 500
    
    def get_groups_overview(self) -> Dict:
        """è·å–æ‰€æœ‰ç»„çš„æ¦‚è§ˆä¿¡æ¯"""
        groups = {
            'control': {
                'name': 'å¥åº·å¯¹ç…§ç»„',
                'color': '#28a745',
                'description': 'å¥åº·äººç¾¤å¯¹ç…§æ•°æ®',
                'data_count': self.count_available_data('control')
            },
            'mci': {
                'name': 'MCIç»„',
                'color': '#ffc107', 
                'description': 'è½»åº¦è®¤çŸ¥éšœç¢æ•°æ®',
                'data_count': self.count_available_data('mci')
            },
            'ad': {
                'name': 'ADç»„',
                'color': '#dc3545',
                'description': 'é˜¿å°”èŒ¨æµ·é»˜ç—…æ•°æ®',
                'data_count': self.count_available_data('ad')
            }
        }
        
        print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: Control({groups['control']['data_count']}) MCI({groups['mci']['data_count']}) AD({groups['ad']['data_count']})")
        return groups
    
    def count_available_data(self, group_type: str) -> int:
        """ç»Ÿè®¡å¯ç”¨æ•°æ®æ•°é‡"""
        data_sources = self.config.get("data_sources", {})
        group_dir = data_sources.get(f"{group_type}_calibrated", "")
        
        if not os.path.exists(group_dir):
            return 0
        
        count = 0
        for root, dirs, files in os.walk(group_dir):
            count += len([f for f in files if f.endswith('_calibrated.csv')])
        
        return count
    
    def get_group_data(self, group_type: str) -> List[Dict]:
        """è·å–æŒ‡å®šç»„çš„æ•°æ®åˆ—è¡¨"""
        data_list = []
        data_sources = self.config.get("data_sources", {})
        group_dir = data_sources.get(f"{group_type}_calibrated", "")
        
        if not os.path.exists(group_dir):
            print(f"âš ï¸  æ•°æ®ç›®å½•ä¸å­˜åœ¨: {group_dir}")
            return data_list
        
        # ç”¨äºå»é‡çš„é›†åˆ
        seen_data_ids = set()
        
        # éå†ç›®å½•æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
        for root, dirs, files in os.walk(group_dir):
            for file in files:
                if file.endswith('_calibrated.csv'):
                    # è§£ææ–‡ä»¶åè·å–ä¿¡æ¯
                    data_info = self.parse_data_filename(file, group_type)
                    if data_info:
                        data_id = data_info['data_id']
                        # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨ç›¸åŒçš„data_id
                        if data_id not in seen_data_ids:
                            data_info['file_path'] = os.path.join(root, file)
                            data_list.append(data_info)
                            seen_data_ids.add(data_id)
                            print(f"âœ… åŠ è½½æ•°æ®: {group_type} - {data_id} ({file})")
        
        # æŒ‰ç»„å·å’Œé—®é¢˜å·æ’åº
        data_list.sort(key=lambda x: (x.get('group_num', 0), x.get('question_num', 0)))
        
        print(f"ğŸ“Š {group_type}ç»„å…±åŠ è½½ {len(data_list)} ä¸ªæ•°æ®æ–‡ä»¶")
        return data_list
    
    def parse_data_filename(self, filename: str, group_type: str) -> Optional[Dict]:
        """è§£ææ•°æ®æ–‡ä»¶å"""
        import re
        
        # åŒ¹é…ä¸åŒçš„æ–‡ä»¶åæ¨¡å¼
        patterns = {
            'control': r'n(\d+)q(\d+)_preprocessed_calibrated\.csv',
            'mci': r'm(\d+)q(\d+)_preprocessed_calibrated\.csv',
            'ad': r'ad(\d+)q(\d+)_preprocessed_calibrated\.csv'
        }
        
        pattern = patterns.get(group_type)
        if not pattern:
            return None
        
        match = re.match(pattern, filename)
        if match:
            group_num = int(match.group(1))
            question_num = int(match.group(2))
            
            data_id = f"{group_type[0]}{group_num}q{question_num}"
            if group_type == 'ad':
                data_id = f"ad{group_num}q{question_num}"
            
            return {
                'data_id': data_id,
                'group_num': group_num,
                'question_num': question_num,
                'filename': filename,
                'display_name': f"Group {group_num} - Question {question_num}"
            }
        
        return None
    
    def generate_enhanced_visualization(self, group_type: str, data_id: str, vis_params: Dict = None) -> Dict:
        """
        ç”Ÿæˆå¢å¼ºç‰ˆæ•°æ®å¯è§†åŒ–
        
        Args:
            group_type: ç»„ç±»å‹
            data_id: æ•°æ®ID
            
        Returns:
            å¯è§†åŒ–ç»“æœ
        """
        try:
            # è·å–æ•°æ®æ–‡ä»¶è·¯å¾„
            data_list = self.get_group_data(group_type)
            target_data = None
            
            for data_item in data_list:
                if data_item['data_id'] == data_id:
                    target_data = data_item
                    break
            
            if not target_data:
                return {'error': f'æ•°æ®ä¸å­˜åœ¨: {data_id}'}
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ ¡å‡†é¢„è§ˆæ¨¡å¼
            preview_mode = vis_params.get('preview_mode', False) if vis_params else False
            x_offset = vis_params.get('x_offset', 0) if vis_params else 0
            y_offset = vis_params.get('y_offset', 0) if vis_params else 0
            time_start = vis_params.get('time_start', 0) if vis_params else 0
            time_end = vis_params.get('time_end', 100) if vis_params else 100
            
            file_path = target_data['file_path']
            
            # å¦‚æœæ˜¯é¢„è§ˆæ¨¡å¼ï¼Œåˆ›å»ºä¸´æ—¶æ ¡å‡†æ•°æ®
            if preview_mode and (x_offset != 0 or y_offset != 0 or time_start > 0 or time_end < 100):
                file_path = self.create_preview_calibrated_data(target_data['file_path'], x_offset, y_offset, time_start, time_end)
                if not file_path:
                    return {'error': 'æ— æ³•åˆ›å»ºé¢„è§ˆæ•°æ®'}
            
            # å¢å¼ºç‰ˆåˆ†æ
            question = f"q{target_data['question_num']}"
            analysis_result = self.analyzer.analyze_eyetracking_data(
                file_path, 
                question,
                debug=True
            )
            
            if 'error' in analysis_result:
                return analysis_result
            
            # ç”Ÿæˆå¢å¼ºç‰ˆå¯è§†åŒ–å›¾åƒ
            visualization_image = self.create_enhanced_trajectory_visualization(
                analysis_result, question, vis_params
            )
            
            if visualization_image:
                # è½¬æ¢ä¸ºbase64
                img_buffer = BytesIO()
                visualization_image.save(img_buffer, format='PNG')
                img_str = base64.b64encode(img_buffer.getvalue()).decode()
                
                result = {
                    'success': True,
                    'data_id': data_id,
                    'question': question,
                    'image': img_str,  # ç›´æ¥è¿”å›base64å­—ç¬¦ä¸²ï¼Œä¸åŒ…å«å‰ç¼€
                    'roi_statistics': analysis_result['roi_statistics'],
                    'overall_statistics': analysis_result['overall_statistics'],
                    'fixations': analysis_result['fixations'],
                    'saccades': analysis_result['saccades'],
                    'roi_sequence_count': analysis_result['overall_statistics']['roi_sequence_count']
                }
                return convert_numpy_types(result)
            else:
                return {'error': 'æ— æ³•ç”Ÿæˆå¯è§†åŒ–å›¾åƒ'}
            
        except Exception as e:
            return {'error': f'å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {str(e)}'}
    
    def load_background_image(self, question: str, maintain_aspect: bool = True) -> Optional[Image.Image]:
        """
        åŠ è½½èƒŒæ™¯å›¾åƒ
        
        Args:
            question: é—®é¢˜ç¼–å·
            maintain_aspect: æ˜¯å¦ä¿æŒ1:1æ¯”ä¾‹
            
        Returns:
            PILå›¾åƒå¯¹è±¡
        """
        try:
            background_dir = self.analyzer.background_img_dir
            # åŒ¹é…Q1.jpg, Q2.jpgç­‰æ ¼å¼
            q_num = question[1:]  # å»æ‰'q'å‰ç¼€
            img_filename = f"Q{q_num}.jpg"
            img_path = os.path.join(background_dir, img_filename)
            
            if os.path.exists(img_path):
                # ä½¿ç”¨OpenCVåŠ è½½å›¾åƒ
                bgr_img = cv2.imread(img_path)
                if bgr_img is not None:
                    # è½¬æ¢é¢œè‰²æ ¼å¼
                    rgb_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)
                    # è½¬æ¢ä¸ºPILå›¾åƒï¼Œä¿æŒåŸå§‹å°ºå¯¸æ¯”ä¾‹
                    pil_img = Image.fromarray(rgb_img)
                    return pil_img
                    
        except Exception as e:
            print(f"âš ï¸  æ— æ³•åŠ è½½èƒŒæ™¯å›¾åƒ: {e}")
        
        return None
    
    def draw_rois_on_image(self, pil_img: Image.Image, roi_kw: List, roi_inst: List, roi_bg: List) -> Image.Image:
        """
        åœ¨å›¾åƒä¸Šç»˜åˆ¶ROIåŒºåŸŸ
        
        Args:
            pil_img: PILå›¾åƒå¯¹è±¡
            roi_kw: å…³é”®è¯ROIåˆ—è¡¨
            roi_inst: æŒ‡ä»¤ROIåˆ—è¡¨
            roi_bg: èƒŒæ™¯ROIåˆ—è¡¨
            
        Returns:
            ç»˜åˆ¶äº†ROIçš„å›¾åƒ
        """
        # æ ‡å‡†åŒ–ROIåæ ‡
        roi_kw = self.analyzer.normalize_roi(roi_kw)
        roi_inst = self.analyzer.normalize_roi(roi_inst)
        roi_bg = self.analyzer.normalize_roi(roi_bg)
        
        base = pil_img.convert("RGBA")
        w, h = base.size
        roi_layer = Image.new("RGBA", (w, h), (255, 255, 255, 0))
        text_layer = Image.new("RGBA", (w, h), (255, 255, 255, 0))
        d_roi = ImageDraw.Draw(roi_layer)
        d_txt = ImageDraw.Draw(text_layer)
        
        # å°è¯•åŠ è½½å­—ä½“
        try:
            font_ = ImageFont.truetype("msyh.ttc", 18)
        except:
            font_ = ImageFont.load_default()
        
        def measure_text(txt):
            try:
                bbox = font_.getbbox(txt)
                return bbox[2] - bbox[0], bbox[3] - bbox[1]
            except:
                return (len(txt) * 10, 18)  # ç®€å•ä¼°ç®—
        
        def draw_ones(roi_list, color, alpha_):
            """ç»˜åˆ¶ä¸€ç»„ROI"""
            for (rn, xmn, ymn, xmx, ymy) in roi_list:
                try:
                    X1 = int(xmn * w)
                    X2 = int(xmx * w)
                    # Yåæ ‡è½¬æ¢ï¼ˆPILçš„Yåæ ‡å‘ä¸‹é€’å¢ï¼‰
                    Ytop = int((1 - ymn) * h)
                    Ybot = int((1 - ymy) * h)
                    y1, y2 = min(Ytop, Ybot), max(Ytop, Ybot)
                    
                    # ç»˜åˆ¶å¡«å……çŸ©å½¢ - åŠé€æ˜æ•ˆæœ
                    fill_color = (*color, alpha_)
                    outline_color = (*color, 255)
                    d_roi.rectangle([(X1, y1), (X2, y2)],
                                   fill=fill_color, outline=outline_color, width=1)
                    
                    # ç»˜åˆ¶æ ‡ç­¾
                    tw, th = measure_text(rn)
                    tx, ty = X1 + 2, y2 + 2  # æ ‡ç­¾åœ¨çŸ©å½¢ä¸‹æ–¹
                    if tx + tw > w:
                        tx = w - tw
                    if ty + th > h:
                        ty = h - th
                    
                    # æ ‡ç­¾èƒŒæ™¯ - åŠé€æ˜ç™½è‰²
                    d_txt.rectangle([(tx, ty), (tx + tw, ty + th)],
                                   fill=(255, 255, 255, 180))
                    # æ ‡ç­¾æ–‡å­— - é»‘è‰²
                    d_txt.text((tx, ty), rn, fill=(0, 0, 0, 255), font=font_)
                except Exception as e:
                    print(f"âš ï¸  ç»˜åˆ¶ROI {rn} å¤±è´¥: {e}")
                    continue
        
        # æŒ‰å±‚æ¬¡ç»˜åˆ¶ï¼ˆèƒŒæ™¯ -> æŒ‡ä»¤ -> å…³é”®è¯ï¼‰
        try:
            bg_color = self.colors.get('roi_background', (0, 128, 255))
            inst_color = self.colors.get('roi_instructions', (255, 165, 0))
            kw_color = self.colors.get('roi_keywords', (255, 0, 0))
            
            draw_ones(roi_bg, bg_color, self.sizes['roi_alpha_bg'])
            draw_ones(roi_inst, inst_color, self.sizes['roi_alpha_inst'])
            draw_ones(roi_kw, kw_color, self.sizes['roi_alpha_kw'])
        except Exception as e:
            print(f"âš ï¸  ç»˜åˆ¶ROIå±‚æ—¶å‡ºé”™: {e}")
        
        # åˆå¹¶å›¾å±‚
        combined = Image.alpha_composite(base, roi_layer)
        combined = Image.alpha_composite(combined, text_layer)
        return combined.convert("RGB")
     
    def draw_trajectory_with_sequence(self, pil_img: Image.Image, df: pd.DataFrame, vis_params: Dict = None) -> Image.Image:
        """
        ç»˜åˆ¶å¸¦åºåˆ—æ ‡è®°çš„çœ¼åŠ¨è½¨è¿¹
        
        Args:
            pil_img: PILå›¾åƒå¯¹è±¡
            df: çœ¼åŠ¨æ•°æ®DataFrame
            vis_params: å¯è§†åŒ–å‚æ•°
            
        Returns:
            ç»˜åˆ¶äº†è½¨è¿¹çš„å›¾åƒ
        """
        w, h = pil_img.size
        traj_layer = Image.new("RGBA", (w, h), (255, 255, 255, 0))
        d = ImageDraw.Draw(traj_layer)
        
        # è·å–å¯è§†åŒ–å‚æ•°
        if vis_params is None:
            vis_params = {}
        
        fixation_size = vis_params.get('fixation_size', 3)
        trajectory_width = vis_params.get('trajectory_width', 2)
        trajectory_style = vis_params.get('trajectory_style', 'solid')
        point_size = vis_params.get('point_size', 1)
        
        # å°è¯•åŠ è½½å­—ä½“
        try:
            font_ = ImageFont.truetype("arial.ttf", self.sizes['font_size'])
        except:
            font_ = ImageFont.load_default()
        
        # è½¬æ¢åæ ‡ç‚¹
        pts = []
        for i in range(len(df)):
            px = int(df.at[i, "x"] * w)
            py = int((1 - df.at[i, "y"]) * h)  # Yåæ ‡ç¿»è½¬
            pts.append((px, py))
        
        # ç»˜åˆ¶è½¨è¿¹çº¿
        if len(pts) > 1:
            for i in range(len(pts) - 1):
                d.line([pts[i], pts[i + 1]], 
                       fill=(200, 80, 255, 160),  
                       width=trajectory_width)
        
        # ç»˜åˆ¶æ•°æ®ç‚¹
        for pt in pts:
            d.ellipse((pt[0] - point_size, pt[1] - point_size, 
                      pt[0] + point_size, pt[1] + point_size),
                     fill=(0, 0, 255, 160))
        
        # ç»˜åˆ¶èµ·å§‹ç‚¹å’Œç»“æŸç‚¹
        if pts:
            # èµ·å§‹ç‚¹ - ç»¿è‰²
            start_pt = pts[0]
            d.ellipse((start_pt[0] - fixation_size, start_pt[1] - fixation_size,
                      start_pt[0] + fixation_size, start_pt[1] + fixation_size),
                     fill=(0, 255, 0, 200))
            
            # ç»“æŸç‚¹ - çº¢è‰²
            if len(pts) > 1:
                end_pt = pts[-1]
                d.ellipse((end_pt[0] - fixation_size, end_pt[1] - fixation_size,
                          end_pt[0] + fixation_size, end_pt[1] + fixation_size),
                         fill=(255, 0, 0, 200))
        
        # ç»˜åˆ¶ROIåºåˆ—æ ‡è®°
        if "SequenceID" in df.columns and "EnterExitFlag" in df.columns:
            for i in range(len(df)):
                seq_id = df.at[i, "SequenceID"]
                flag = df.at[i, "EnterExitFlag"]
                if seq_id > 0 and flag in ("Enter", "Exit"):
                    label = ("E" if flag == "Enter" else "X") + str(seq_id)
                    
                    # ç»˜åˆ¶æ ‡è®°
                    px, py = pts[i]
                    d.text((px + 3, py - 15), label, 
                           fill=(255, 0, 0, 255), font=font_)
        
        # åˆå¹¶å›¾å±‚
        final = Image.alpha_composite(pil_img.convert("RGBA"), traj_layer)
        return final.convert("RGB")
    
    def create_enhanced_trajectory_visualization(self, analysis_result: Dict, question: str, vis_params: Dict = None) -> Optional[Image.Image]:
        """
        åˆ›å»ºå¢å¼ºç‰ˆè½¨è¿¹å¯è§†åŒ–å›¾åƒ
        
        Args:
            analysis_result: åˆ†æç»“æœ
            question: é—®é¢˜ç¼–å·
            
        Returns:
            PILå›¾åƒå¯¹è±¡
        """
        try:
            df = analysis_result['data']
            roi_defs = analysis_result['roi_definitions']
            
            # åŠ è½½èƒŒæ™¯å›¾åƒ
            background_img = self.load_background_image(question)
            if background_img:
                img = background_img.copy()
            else:
                # åˆ›å»ºé»˜è®¤ç™½è‰²èƒŒæ™¯
                img = Image.new('RGB', (800, 600), 'white')
            
            # ç»˜åˆ¶ROIåŒºåŸŸ
            img = self.draw_rois_on_image(img, 
                                         roi_defs['keywords'],
                                         roi_defs['instructions'], 
                                         roi_defs['background'])
            
            # ç»˜åˆ¶çœ¼åŠ¨è½¨è¿¹
            img = self.draw_trajectory_with_sequence(img, df, vis_params)
            
            return img
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºå¢å¼ºç‰ˆå¯è§†åŒ–å›¾åƒå¤±è´¥: {e}")
            return None
    
    def process_single_adq(self, group_type: str, data_id: str) -> Dict:
        """
        å¤„ç†å•ä¸ªæ•°æ®æ–‡ä»¶
        
        Args:
            group_type: ç»„ç±»å‹
            data_id: æ•°æ®ID
            
        Returns:
            å¤„ç†ç»“æœ
        """
        try:
            # è·å–æ•°æ®æ–‡ä»¶
            data_list = self.get_group_data(group_type)
            target_data = None
            
            for data_item in data_list:
                if data_item['data_id'] == data_id:
                    target_data = data_item
                    break
            
            if not target_data:
                return {'error': f'æ•°æ®ä¸å­˜åœ¨: {data_id}'}
            
            # æ‰§è¡Œå¢å¼ºç‰ˆåˆ†æ
            question = f"q{target_data['question_num']}"
            analysis_result = self.analyzer.analyze_eyetracking_data(
                target_data['file_path'], 
                question,
                debug=True
            )
            
            if 'error' in analysis_result:
                return analysis_result
            
            # ç”Ÿæˆå¯è§†åŒ–
            visualization_image = self.create_enhanced_trajectory_visualization(
                analysis_result, question
            )
            
            result = {
                'success': True,
                'data_id': data_id,
                'question': question,
                'analysis': analysis_result,
                'has_visualization': visualization_image is not None
            }
            
            # å¦‚æœæˆåŠŸç”Ÿæˆå¯è§†åŒ–ï¼Œæ·»åŠ base64ç¼–ç 
            if visualization_image:
                img_buffer = BytesIO()
                visualization_image.save(img_buffer, format='PNG')
                img_str = base64.b64encode(img_buffer.getvalue()).decode()
                result['image'] = img_str
            
            return result
            
        except Exception as e:
            return {'error': f'å¤„ç†å¤±è´¥: {str(e)}'}
    
    def get_group_statistics(self, group_type: str) -> Dict:
        """è·å–ç»„ç»Ÿè®¡ä¿¡æ¯"""
        try:
            data_list = self.get_group_data(group_type)
            
            stats = {
                'total_files': len(data_list),
                'questions': {},
                'groups': {}
            }
            
            # æŒ‰é—®é¢˜ç»Ÿè®¡
            for data_item in data_list:
                question = data_item['question_num']
                if question not in stats['questions']:
                    stats['questions'][question] = 0
                stats['questions'][question] += 1
            
            # æŒ‰ç»„ç»Ÿè®¡
            for data_item in data_list:
                group = data_item['group_num']
                if group not in stats['groups']:
                    stats['groups'][group] = 0
                stats['groups'][group] += 1
            
            return stats
            
        except Exception as e:
            return {'error': f'è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}'}
    
    def handle_file_group_upload(self, files, group: str) -> Dict:
        """
        å¤„ç†æ–‡ä»¶ç»„ä¸Šä¼ 
        
        Args:
            files: ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
            group: ç›®æ ‡åˆ†ç»„
            
        Returns:
            å¤„ç†ç»“æœ
        """
        import uuid
        import tempfile
        import shutil
        from datetime import datetime
        
        try:
            # ç”Ÿæˆå”¯ä¸€ç»„ID
            group_id = str(uuid.uuid4())
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # åˆ›å»ºä¸´æ—¶å­˜å‚¨ç›®å½•
            upload_dir = os.path.join("temp_uploads", group_id)
            os.makedirs(upload_dir, exist_ok=True)
            
            # ä¿å­˜æ‰€æœ‰æ–‡ä»¶
            file_info_list = []
            for file in files:
                filename = file.filename
                temp_file_path = os.path.join(upload_dir, filename)
                file.save(temp_file_path)
                
                file_info_list.append({
                    'filename': filename,
                    'temp_path': temp_file_path
                })
            
            # å­˜å‚¨ç»„ä¿¡æ¯
            group_info = {
                'group_id': group_id,
                'group': group,
                'upload_time': timestamp,
                'files': file_info_list,
                'status': 'uploaded'
            }
            
            # ä¿å­˜ç»„ä¿¡æ¯åˆ°ä¸´æ—¶æ–‡ä»¶
            info_file = os.path.join(upload_dir, 'group_info.json')
            with open(info_file, 'w', encoding='utf-8') as f:
                json.dump(group_info, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… æ–‡ä»¶ç»„ä¸Šä¼ æˆåŠŸ: {[f.filename for f in files]} -> {group_id}")
            
            return {
                'success': True,
                'groupId': group_id,
                'group': group,
                'fileCount': len(files),
                'message': 'æ–‡ä»¶ç»„ä¸Šä¼ æˆåŠŸ'
            }
            
        except Exception as e:
            print(f"âŒ æ–‡ä»¶ç»„ä¸Šä¼ å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': f'æ–‡ä»¶ç»„ä¸Šä¼ å¤±è´¥: {str(e)}'
            }
    
    def process_uploaded_file_group(self, group_id: str) -> Dict:
        """
        å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ç»„
        
        Args:
            group_id: æ–‡ä»¶ç»„ID
            
        Returns:
            å¤„ç†ç»“æœ
        """
        try:
            # è¯»å–ç»„ä¿¡æ¯
            upload_dir = os.path.join("temp_uploads", group_id)
            info_file = os.path.join(upload_dir, 'group_info.json')
            
            if not os.path.exists(info_file):
                return {'success': False, 'error': 'æ‰¾ä¸åˆ°æ–‡ä»¶ç»„ä¿¡æ¯'}
            
            with open(info_file, 'r', encoding='utf-8') as f:
                group_info = json.load(f)
            
            group = group_info['group']
            files = group_info['files']
            
            print(f"ğŸ”„ å¼€å§‹å¤„ç†æ–‡ä»¶ç»„: {[f['filename'] for f in files]}")
            
            # ç¬¬1æ­¥ï¼šè·å–å”¯ä¸€çš„ç»„ç¼–å·ï¼ˆå…³é”®ï¼šåªè°ƒç”¨ä¸€æ¬¡ï¼‰
            group_num = self._get_next_group_number(group)
            target_group_name = f"{group}_group_{group_num}"
            
            # ç¬¬2æ­¥ï¼šåˆ›å»ºç›®æ ‡ç›®å½•ç»“æ„
            raw_dir = f"data/{group}_raw/{target_group_name}"
            processed_dir = f"data/{group}_processed/{target_group_name}"
            calibrated_dir = f"data/{group}_calibrated/{target_group_name}"
            
            for dir_path in [raw_dir, processed_dir, calibrated_dir]:
                os.makedirs(dir_path, exist_ok=True)
            
            print(f"âœ… åˆ›å»ºç›®æ ‡ç»„ç›®å½•: {target_group_name}")
            
            # ç¬¬3æ­¥ï¼šå¤åˆ¶åŸå§‹æ–‡ä»¶åˆ°ç›®æ ‡ç›®å½•
            for file_info in files:
                source_path = file_info['temp_path']
                filename = file_info['filename']
                target_raw_file = os.path.join(raw_dir, filename)
                
                # æ£€æŸ¥æºæ–‡ä»¶
                if os.path.exists(source_path):
                    source_size = os.path.getsize(source_path)
                    print(f"ğŸ“ æºæ–‡ä»¶ {filename}: {source_size} bytes")
                else:
                    print(f"âŒ æºæ–‡ä»¶ä¸å­˜åœ¨: {source_path}")
                    continue
                
                import shutil
                shutil.copy2(source_path, target_raw_file)
                
                # éªŒè¯å¤åˆ¶ç»“æœ
                if os.path.exists(target_raw_file):
                    target_size = os.path.getsize(target_raw_file)
                    print(f"âœ… å¤åˆ¶åŸå§‹æ–‡ä»¶: {filename} ({target_size} bytes)")
                else:
                    print(f"âŒ å¤åˆ¶å¤±è´¥: {filename}")
            
            # ç¬¬4æ­¥ï¼šé¢„å¤„ç†æ‰€æœ‰æ–‡ä»¶
            processed_files = []
            for file_info in files:
                filename = file_info['filename']
                raw_file_path = os.path.join(raw_dir, filename)
                
                processed_result = self._process_raw_file_with_naming(
                    raw_file_path, processed_dir, group, group_num, filename
                )
                if not processed_result['success']:
                    return {
                        'success': False,
                        'error': f'é¢„å¤„ç†æ–‡ä»¶{filename}å¤±è´¥: {processed_result["error"]}'
                    }
                
                processed_files.append(processed_result['processed_file'])
                print(f"âœ… é¢„å¤„ç†å®Œæˆ: {filename}")
            
            # ç¬¬5æ­¥ï¼šæ ¡å‡†æ‰€æœ‰æ–‡ä»¶
            calibrated_files = []
            for processed_file in processed_files:
                calibrated_result = self._calibrate_processed_file(
                    processed_file, 
                    calibrated_dir, 
                    group
                )
                if not calibrated_result['success']:
                    filename = os.path.basename(processed_file)
                    return {
                        'success': False,
                        'error': f'æ ¡å‡†æ–‡ä»¶{filename}å¤±è´¥: {calibrated_result["error"]}'
                    }
                
                calibrated_files.append(calibrated_result['calibrated_file'])
                print(f"âœ… æ ¡å‡†å®Œæˆ: {os.path.basename(processed_file)}")
            
            # ç¬¬6æ­¥ï¼šæ¸…ç†ä¸´æ—¶æ–‡ä»¶
            import shutil
            shutil.rmtree(upload_dir)
            
            print(f"ğŸ‰ æ–‡ä»¶ç»„å¤„ç†å®Œæˆ: {target_group_name}")
            
            return {
                'success': True,
                'message': 'æ–‡ä»¶ç»„å¤„ç†å®Œæˆ',
                'group': group,
                'group_num': group_num,
                'target_group_name': target_group_name,
                'processed_files': len(processed_files),
                'calibrated_files': len(calibrated_files)
            }
            
        except Exception as e:
            print(f"âŒ æ–‡ä»¶ç»„å¤„ç†å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': f'æ–‡ä»¶ç»„å¤„ç†å¤±è´¥: {str(e)}'
            }
    
    def _get_next_group_number(self, group: str) -> int:
        """è·å–ä¸‹ä¸€ä¸ªç»„ç¼–å·"""
        try:
            base_dir = f"data/{group}_raw"
            if not os.path.exists(base_dir):
                return 1
            
            existing_groups = []
            for item in os.listdir(base_dir):
                if item.startswith(f"{group}_group_"):
                    try:
                        num = int(item.split('_')[-1])
                        existing_groups.append(num)
                    except ValueError:
                        continue
            
            return max(existing_groups) + 1 if existing_groups else 1
            
        except Exception:
            return 1
    
    def _process_raw_file_with_naming(self, raw_file: str, output_dir: str, group: str, group_num: int, original_filename: str) -> Dict:
        """å¤„ç†åŸå§‹æ–‡ä»¶ï¼ˆä½¿ç”¨æ­£ç¡®çš„å‘½åæ ¼å¼ï¼‰"""
        try:
            print(f"ğŸ” å¼€å§‹å¤„ç†åŸå§‹æ–‡ä»¶: {raw_file}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(raw_file):
                error_msg = f"åŸå§‹æ–‡ä»¶ä¸å­˜åœ¨: {raw_file}"
                print(f"âŒ {error_msg}")
                return {'success': False, 'error': error_msg}
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(raw_file)
            print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size} bytes")
            
            # æ£€æŸ¥æ–‡ä»¶å†…å®¹æ ·æœ¬
            try:
                with open(raw_file, 'r', encoding='utf-8') as f:
                    first_lines = []
                    for i, line in enumerate(f):
                        if i < 5:  # è¯»å–å‰5è¡Œ
                            first_lines.append(line.strip())
                        else:
                            break
                print(f"ğŸ“„ æ–‡ä»¶å‰5è¡Œå†…å®¹:")
                for i, line in enumerate(first_lines):
                    print(f"   {i+1}: {line}")
            except Exception as e:
                print(f"âš ï¸  æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹: {e}")
            
            # é¦–å…ˆå°è¯•è‡ªå®šä¹‰æ ¼å¼è§£æå™¨
            from data_processing.custom_vr_parser import process_custom_vr_file
            
            # æ ¹æ®åŸå§‹æ–‡ä»¶åç¡®å®šé—®é¢˜ç¼–å·
            # æ”¯æŒä¸¤ç§æ ¼å¼ï¼š1.txt -> q1, 2.txt -> q2, etc. æˆ– level_1.txt -> q1, level_2.txt -> q2, etc.
            base_name = os.path.splitext(original_filename)[0]
            
            try:
                # å°è¯•ç›´æ¥è§£æä¸ºæ•´æ•° (1.txt, 2.txt, etc.)
                question_num = int(base_name)
                print(f"ğŸ“ ä½¿ç”¨æ ‡å‡†æ ¼å¼è§£æé—®é¢˜ç¼–å·: {original_filename} -> q{question_num}")
            except ValueError:
                # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯• level_X æ ¼å¼
                import re
                level_match = re.match(r'level_(\d+)', base_name)
                if level_match:
                    question_num = int(level_match.group(1))
                    print(f"ğŸ“ ä½¿ç”¨levelæ ¼å¼è§£æé—®é¢˜ç¼–å·: {original_filename} -> q{question_num}")
                else:
                    raise ValueError(f"æ— æ³•ä»æ–‡ä»¶åè§£æé—®é¢˜ç¼–å·: {original_filename}")
            
            # éªŒè¯é—®é¢˜ç¼–å·èŒƒå›´
            if question_num < 1 or question_num > 5:
                raise ValueError(f"é—®é¢˜ç¼–å·è¶…å‡ºèŒƒå›´ (1-5): {question_num}")
            
            # ç”Ÿæˆç¬¦åˆç°æœ‰æ ¼å¼çš„è¾“å‡ºæ–‡ä»¶å
            if group == 'control':
                prefix = 'n'
            elif group == 'mci':
                prefix = 'm'
            elif group == 'ad':
                prefix = 'ad'
            else:
                prefix = group[0]
            
            output_filename = f"{prefix}{group_num}q{question_num}_preprocessed.csv"
            output_file = os.path.join(output_dir, output_filename)
            print(f"ğŸ“¤ è¾“å‡ºæ–‡ä»¶è·¯å¾„: {output_file}")
            
            # è°ƒç”¨è‡ªå®šä¹‰å¤„ç†å‡½æ•°
            print(f"ğŸ”„ è°ƒç”¨è‡ªå®šä¹‰VRæ ¼å¼å¤„ç†å™¨...")
            success = process_custom_vr_file(raw_file, output_file)
            print(f"âœ… è‡ªå®šä¹‰å¤„ç†å™¨è¿”å›ç»“æœ: {success}")
            
            # å¦‚æœè‡ªå®šä¹‰æ ¼å¼å¤„ç†å¤±è´¥ï¼Œå°è¯•æ ‡å‡†æ ¼å¼
            if not success:
                print(f"ğŸ”„ å°è¯•æ ‡å‡†æ ¼å¼å¤„ç†å™¨...")
                from data_processing.vr_eyetracking_processor import process_txt_file
                success = process_txt_file(raw_file, output_file)
                print(f"âœ… æ ‡å‡†å¤„ç†å™¨è¿”å›ç»“æœ: {success}")
            
            if success:
                # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦æˆåŠŸåˆ›å»º
                if os.path.exists(output_file):
                    output_size = os.path.getsize(output_file)
                    print(f"ğŸ“¤ è¾“å‡ºæ–‡ä»¶åˆ›å»ºæˆåŠŸï¼Œå¤§å°: {output_size} bytes")
                    
                    # è¯»å–è¾“å‡ºæ–‡ä»¶çš„å‰å‡ è¡ŒéªŒè¯
                    try:
                        import pandas as pd
                        df = pd.read_csv(output_file, nrows=3)  # è¯»å–å‰3è¡Œ
                        print(f"ğŸ“Š è¾“å‡ºæ–‡ä»¶éªŒè¯:")
                        print(f"   åˆ—å: {list(df.columns)}")
                        print(f"   æ•°æ®è¡Œæ•°: {len(df)}")
                        if len(df) > 0:
                            print(f"   ç¬¬ä¸€è¡Œæ•°æ®: {df.iloc[0].to_dict()}")
                    except Exception as e:
                        print(f"âš ï¸  è¾“å‡ºæ–‡ä»¶éªŒè¯å¤±è´¥: {e}")
                        
                    return {
                        'success': True,
                        'processed_file': output_file
                    }
                else:
                    error_msg = "å¤„ç†æˆåŠŸä½†è¾“å‡ºæ–‡ä»¶æœªåˆ›å»º"
                    print(f"âŒ {error_msg}")
                    return {'success': False, 'error': error_msg}
            else:
                error_msg = "æ•°æ®é¢„å¤„ç†å¤±è´¥"
                print(f"âŒ {error_msg}")
                return {'success': False, 'error': error_msg}
                
        except Exception as e:
            error_msg = f'æ•°æ®é¢„å¤„ç†é”™è¯¯: {str(e)}'
            print(f"âŒ {error_msg}")
            import traceback
            print(f"ğŸ“‹ è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
            return {'success': False, 'error': error_msg}
    
    def _process_raw_file(self, raw_file: str, output_dir: str) -> Dict:
        """å¤„ç†åŸå§‹æ–‡ä»¶"""
        try:
            print(f"ğŸ” å¼€å§‹å¤„ç†åŸå§‹æ–‡ä»¶: {raw_file}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(raw_file):
                error_msg = f"åŸå§‹æ–‡ä»¶ä¸å­˜åœ¨: {raw_file}"
                print(f"âŒ {error_msg}")
                return {'success': False, 'error': error_msg}
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(raw_file)
            print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size} bytes")
            
            # æ£€æŸ¥æ–‡ä»¶å†…å®¹æ ·æœ¬
            try:
                with open(raw_file, 'r', encoding='utf-8') as f:
                    first_lines = []
                    for i, line in enumerate(f):
                        if i < 5:  # è¯»å–å‰5è¡Œ
                            first_lines.append(line.strip())
                        else:
                            break
                print(f"ğŸ“„ æ–‡ä»¶å‰5è¡Œå†…å®¹:")
                for i, line in enumerate(first_lines):
                    print(f"   {i+1}: {line}")
            except Exception as e:
                print(f"âš ï¸  æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹: {e}")
            
            # é¦–å…ˆå°è¯•è‡ªå®šä¹‰æ ¼å¼è§£æå™¨
            from data_processing.custom_vr_parser import process_custom_vr_file
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            base_name = os.path.splitext(os.path.basename(raw_file))[0]
            output_file = os.path.join(output_dir, f"{base_name}_preprocessed.csv")
            print(f"ğŸ“¤ è¾“å‡ºæ–‡ä»¶è·¯å¾„: {output_file}")
            
            # è°ƒç”¨è‡ªå®šä¹‰å¤„ç†å‡½æ•°
            print(f"ğŸ”„ è°ƒç”¨è‡ªå®šä¹‰VRæ ¼å¼å¤„ç†å™¨...")
            success = process_custom_vr_file(raw_file, output_file)
            print(f"âœ… è‡ªå®šä¹‰å¤„ç†å™¨è¿”å›ç»“æœ: {success}")
            
            # å¦‚æœè‡ªå®šä¹‰æ ¼å¼å¤„ç†å¤±è´¥ï¼Œå°è¯•æ ‡å‡†æ ¼å¼
            if not success:
                print(f"ğŸ”„ å°è¯•æ ‡å‡†æ ¼å¼å¤„ç†å™¨...")
                from data_processing.vr_eyetracking_processor import process_txt_file
                success = process_txt_file(raw_file, output_file)
                print(f"âœ… æ ‡å‡†å¤„ç†å™¨è¿”å›ç»“æœ: {success}")
            
            if success:
                # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦æˆåŠŸåˆ›å»º
                if os.path.exists(output_file):
                    output_size = os.path.getsize(output_file)
                    print(f"ğŸ“¤ è¾“å‡ºæ–‡ä»¶åˆ›å»ºæˆåŠŸï¼Œå¤§å°: {output_size} bytes")
                    
                    # è¯»å–è¾“å‡ºæ–‡ä»¶çš„å‰å‡ è¡ŒéªŒè¯
                    try:
                        import pandas as pd
                        df = pd.read_csv(output_file, nrows=3)  # è¯»å–å‰3è¡Œ
                        print(f"ğŸ“Š è¾“å‡ºæ–‡ä»¶éªŒè¯:")
                        print(f"   åˆ—å: {list(df.columns)}")
                        print(f"   æ•°æ®è¡Œæ•°: {len(df)}")
                        if len(df) > 0:
                            print(f"   ç¬¬ä¸€è¡Œæ•°æ®: {df.iloc[0].to_dict()}")
                    except Exception as e:
                        print(f"âš ï¸  è¾“å‡ºæ–‡ä»¶éªŒè¯å¤±è´¥: {e}")
                        
                    return {
                        'success': True,
                        'processed_file': output_file
                    }
                else:
                    error_msg = "å¤„ç†æˆåŠŸä½†è¾“å‡ºæ–‡ä»¶æœªåˆ›å»º"
                    print(f"âŒ {error_msg}")
                    return {'success': False, 'error': error_msg}
            else:
                error_msg = "æ•°æ®é¢„å¤„ç†å¤±è´¥"
                print(f"âŒ {error_msg}")
                return {'success': False, 'error': error_msg}
                
        except Exception as e:
            error_msg = f'æ•°æ®é¢„å¤„ç†é”™è¯¯: {str(e)}'
            print(f"âŒ {error_msg}")
            import traceback
            print(f"ğŸ“‹ è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
            return {'success': False, 'error': error_msg}
    
    def _calibrate_processed_file(self, processed_file: str, output_dir: str, group: str) -> Dict:
        """æ ¡å‡†å¤„ç†åçš„æ–‡ä»¶"""
        try:
            print(f"ğŸ” å¼€å§‹æ ¡å‡†å¤„ç†åçš„æ–‡ä»¶: {processed_file}")
            
            # æ£€æŸ¥å¤„ç†åçš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(processed_file):
                error_msg = f"é¢„å¤„ç†æ–‡ä»¶ä¸å­˜åœ¨: {processed_file}"
                print(f"âŒ {error_msg}")
                return {'success': False, 'error': error_msg}
                
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(processed_file)
            print(f"ğŸ“ é¢„å¤„ç†æ–‡ä»¶å¤§å°: {file_size} bytes")
            
            from calibration.advanced_calibrator import AdvancedCalibrator
            
            # åˆ›å»ºæ ¡å‡†å™¨
            print(f"ğŸ”§ åˆ›å»ºæ ¡å‡†å™¨...")
            calibrator = AdvancedCalibrator()
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            base_name = os.path.splitext(os.path.basename(processed_file))[0]
            output_file = os.path.join(output_dir, f"{base_name}_calibrated.csv")
            print(f"ğŸ“¤ æ ¡å‡†è¾“å‡ºæ–‡ä»¶è·¯å¾„: {output_file}")
            
            # è·å–æ ¡å‡†å‚æ•°
            group_name = os.path.basename(os.path.dirname(processed_file))
            print(f"ğŸ·ï¸  ç»„å: {group_name}")
            
            x_offset, y_offset, method = calibrator.get_calibration_params(group_name)
            print(f"âš™ï¸  æ ¡å‡†å‚æ•°: x_offset={x_offset}, y_offset={y_offset}, method={method}")
            
            # æ‰§è¡Œæ ¡å‡†
            print(f"ğŸ”„ æ‰§è¡Œæ ¡å‡†...")
            success = calibrator.calibrate_csv_file(
                processed_file, 
                output_file, 
                x_offset, 
                y_offset
            )
            print(f"âœ… æ ¡å‡†è¿”å›ç»“æœ: {success}")
            
            if success:
                # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦æˆåŠŸåˆ›å»º
                if os.path.exists(output_file):
                    output_size = os.path.getsize(output_file)
                    print(f"ğŸ“¤ æ ¡å‡†æ–‡ä»¶åˆ›å»ºæˆåŠŸï¼Œå¤§å°: {output_size} bytes")
                    
                    # è¯»å–è¾“å‡ºæ–‡ä»¶çš„å‰å‡ è¡ŒéªŒè¯
                    try:
                        import pandas as pd
                        df = pd.read_csv(output_file, nrows=3)  # è¯»å–å‰3è¡Œ
                        print(f"ğŸ“Š æ ¡å‡†æ–‡ä»¶éªŒè¯:")
                        print(f"   åˆ—å: {list(df.columns)}")
                        print(f"   æ•°æ®è¡Œæ•°: {len(df)}")
                        if len(df) > 0:
                            print(f"   ç¬¬ä¸€è¡Œæ•°æ®: {df.iloc[0].to_dict()}")
                    except Exception as e:
                        print(f"âš ï¸  æ ¡å‡†æ–‡ä»¶éªŒè¯å¤±è´¥: {e}")
                        
                    return {
                        'success': True,
                        'calibrated_file': output_file,
                        'calibration_method': method
                    }
                else:
                    error_msg = "æ ¡å‡†æˆåŠŸä½†è¾“å‡ºæ–‡ä»¶æœªåˆ›å»º"
                    print(f"âŒ {error_msg}")
                    return {'success': False, 'error': error_msg}
            else:
                error_msg = "æ•°æ®æ ¡å‡†å¤±è´¥"
                print(f"âŒ {error_msg}")
                return {'success': False, 'error': error_msg}
                
        except Exception as e:
            error_msg = f'æ•°æ®æ ¡å‡†é”™è¯¯: {str(e)}'
            print(f"âŒ {error_msg}")
            import traceback
            print(f"ğŸ“‹ è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
            return {'success': False, 'error': error_msg}
    
    def save_data_calibration(self, group_type: str, data_id: str, x_offset: float, y_offset: float, time_start: float = 0, time_end: float = 100) -> Dict:
        """
        ä¿å­˜æ ¡å‡†åç§»é‡åˆ°æ•°æ®æ–‡ä»¶
        
        Args:
            group_type: ç»„ç±»å‹ (control, mci, ad)
            data_id: æ•°æ®ID
            x_offset: Xè½´åç§»é‡
            y_offset: Yè½´åç§»é‡
            
        Returns:
            æ“ä½œç»“æœå­—å…¸
        """
        try:
            print(f"ğŸ¯ å¼€å§‹ä¿å­˜æ ¡å‡†åç§»é‡: {group_type}/{data_id}")
            print(f"ğŸ“Š åç§»é‡: X={x_offset:.3f}, Y={y_offset:.3f}")
            
            # å‚æ•°éªŒè¯
            if not isinstance(x_offset, (int, float)) or not isinstance(y_offset, (int, float)):
                return {'success': False, 'error': 'åç§»é‡å‚æ•°ç±»å‹æ— æ•ˆ'}
            
            if abs(x_offset) > 1.0 or abs(y_offset) > 1.0:
                return {'success': False, 'error': 'åç§»é‡è¶…å‡ºå®‰å…¨èŒƒå›´ [-1.0, 1.0]'}
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ä¿®æ”¹ï¼ˆåæ ‡åç§»æˆ–æ—¶é—´èŒƒå›´ä¿®æ”¹ï¼‰
            has_coordinate_change = x_offset != 0 or y_offset != 0
            has_time_change = time_start > 0 or time_end < 100
            
            if not has_coordinate_change and not has_time_change:
                return {'success': False, 'error': 'æ²¡æœ‰æ£€æµ‹åˆ°æ ¡å‡†ä¿®æ”¹ï¼Œæ— éœ€ä¿å­˜'}
            
            # æŸ¥æ‰¾å¯¹åº”çš„æ ¡å‡†æ–‡ä»¶
            calibrated_dir = f"data/{group_type}_calibrated"
            if not os.path.exists(calibrated_dir):
                return {'success': False, 'error': f'æ ¡å‡†ç›®å½•ä¸å­˜åœ¨: {calibrated_dir}'}
            
            # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
            target_file = None
            print(f"ğŸ” æ­£åœ¨æŸ¥æ‰¾æ–‡ä»¶ï¼Œdata_id: '{data_id}'")
            print(f"ğŸ“‚ æœç´¢ç›®å½•: {calibrated_dir}")
            
            for root, dirs, files in os.walk(calibrated_dir):
                print(f"ğŸ“ æ£€æŸ¥ç›®å½•: {root}")
                print(f"ğŸ“„ ç›®å½•ä¸­çš„æ–‡ä»¶: {files}")
                for file in files:
                    if file.endswith('_calibrated.csv'):
                        # ä½¿ç”¨ parse_data_filename æ–¹æ³•æ­£ç¡®è§£ææ–‡ä»¶å
                        parsed = self.parse_data_filename(file, group_type)
                        if parsed and parsed['data_id'] == data_id:
                            target_file = os.path.join(root, file)
                            print(f"âœ… æ‰¾åˆ°åŒ¹é…æ–‡ä»¶: {target_file} (è§£æå¾—åˆ°çš„data_id: {parsed['data_id']})")
                            break
                        else:
                            parsed_id = parsed['data_id'] if parsed else 'N/A'
                            print(f"   æ£€æŸ¥æ–‡ä»¶: '{file}' -> è§£ædata_id: '{parsed_id}' != ç›®æ ‡data_id: '{data_id}'")
                if target_file:
                    break
            
            if not target_file:
                print(f"âŒ æœªæ‰¾åˆ°åŒ¹é…æ–‡ä»¶ï¼Œdata_id: '{data_id}'")
                # åˆ—å‡ºæ‰€æœ‰å¯èƒ½çš„æ–‡ä»¶ä¾›è°ƒè¯•
                all_files = []
                for root, dirs, files in os.walk(calibrated_dir):
                    for file in files:
                        if file.endswith('_calibrated.csv'):
                            all_files.append(file)
                print(f"ğŸ“‹ å¯ç”¨çš„æ ¡å‡†æ–‡ä»¶: {all_files[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ª
                return {'success': False, 'error': f'æœªæ‰¾åˆ°å¯¹åº”çš„æ ¡å‡†æ–‡ä»¶: {data_id}ã€‚å¯ç”¨æ–‡ä»¶æ•°: {len(all_files)}'}
            
            print(f"ğŸ“ ç›®æ ‡æ–‡ä»¶: {target_file}")
            
            # è¯»å–ç°æœ‰æ•°æ®
            try:
                df = pd.read_csv(target_file, encoding='utf-8')
                print(f"ğŸ“Š è¯»å–æ•°æ®: {len(df)} è¡Œ")
            except Exception as e:
                print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
                return {'success': False, 'error': f'è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}'}
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—
            if 'x' not in df.columns or 'y' not in df.columns:
                return {'success': False, 'error': 'æ–‡ä»¶ç¼ºå°‘x,yåæ ‡åˆ—'}
            
            original_count = len(df)
            
            # åº”ç”¨æ—¶é—´èŒƒå›´è¿‡æ»¤
            if has_time_change and 'milliseconds' in df.columns:
                min_time = df['milliseconds'].min()
                max_time = df['milliseconds'].max()
                total_duration = max_time - min_time
                
                # è®¡ç®—å®é™…çš„æ—¶é—´èŒƒå›´
                actual_start_time = min_time + (total_duration * time_start / 100)
                actual_end_time = min_time + (total_duration * time_end / 100)
                
                # è¿‡æ»¤æ•°æ®
                df = df[
                    (df['milliseconds'] >= actual_start_time) & 
                    (df['milliseconds'] <= actual_end_time)
                ]
                filtered_count = len(df)
                
                print(f"ğŸ¯ æ—¶é—´è¿‡æ»¤: {original_count} â†’ {filtered_count} è¡Œ "
                      f"(æ—¶é—´èŒƒå›´: {time_start:.1f}% - {time_end:.1f}%)")
            
            # åº”ç”¨åæ ‡åç§»é‡
            original_x_mean = df['x'].mean()
            original_y_mean = df['y'].mean()
            
            if has_coordinate_change:
                df['x'] = df['x'] + x_offset
                df['y'] = df['y'] + y_offset
            
            new_x_mean = df['x'].mean()
            new_y_mean = df['y'].mean()
            
            if has_coordinate_change:
                print(f"ğŸ“ˆ Xåæ ‡å˜åŒ–: {original_x_mean:.3f} â†’ {new_x_mean:.3f} (åç§»: {x_offset:.3f})")
                print(f"ğŸ“ˆ Yåæ ‡å˜åŒ–: {original_y_mean:.3f} â†’ {new_y_mean:.3f} (åç§»: {y_offset:.3f})")
            else:
                print(f"ğŸ“Š ä¿æŒåŸå§‹åæ ‡: X={new_x_mean:.3f}, Y={new_y_mean:.3f}")
            
            # ä¿å­˜æ ¡å‡†åçš„æ–‡ä»¶
            try:
                df.to_csv(target_file, index=False, encoding='utf-8')
                print(f"âœ… æ ¡å‡†æ–‡ä»¶å·²ä¿å­˜: {target_file}")
            except Exception as e:
                print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
                return {'success': False, 'error': f'ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}'}
            
            # éªŒè¯ä¿å­˜ç»“æœ
            try:
                df_verify = pd.read_csv(target_file, encoding='utf-8')
                if len(df_verify) == len(df):
                    print(f"âœ… æ–‡ä»¶éªŒè¯é€šè¿‡: {len(df_verify)} è¡Œæ•°æ®")
                else:
                    print(f"âš ï¸  æ•°æ®è¡Œæ•°ä¸åŒ¹é…: åŸå§‹{len(df)}, ä¿å­˜å{len(df_verify)}")
            except Exception as e:
                print(f"âš ï¸  æ–‡ä»¶éªŒè¯å¤±è´¥: {e}")
            
            return {
                'success': True,
                'message': 'æ ¡å‡†å·²æˆåŠŸä¿å­˜',
                'file': target_file,
                'changes': {
                    'x_offset': x_offset,
                    'y_offset': y_offset,
                    'original_x_mean': round(original_x_mean, 3),
                    'original_y_mean': round(original_y_mean, 3),
                    'new_x_mean': round(new_x_mean, 3),
                    'new_y_mean': round(new_y_mean, 3)
                }
            }
            
        except Exception as e:
            error_msg = f'ä¿å­˜æ ¡å‡†å¤±è´¥: {str(e)}'
            print(f"âŒ {error_msg}")
            import traceback
            print(f"ğŸ“‹ è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
            return {'success': False, 'error': error_msg}
    
    def get_data_time_info(self, group_type: str, data_id: str) -> Dict:
        """
        è·å–æ•°æ®çš„æ—¶é—´ä¿¡æ¯
        
        Args:
            group_type: ç»„ç±»å‹ (control, mci, ad)
            data_id: æ•°æ®ID
            
        Returns:
            æ—¶é—´ä¿¡æ¯å­—å…¸
        """
        try:
            print(f"ğŸ• è·å–æ—¶é—´ä¿¡æ¯: {group_type}/{data_id}")
            
            # è·å–æ•°æ®æ–‡ä»¶è·¯å¾„
            data_list = self.get_group_data(group_type)
            target_data = None
            
            for data_item in data_list:
                if data_item['data_id'] == data_id:
                    target_data = data_item
                    break
            
            if not target_data:
                return {'success': False, 'error': f'æ•°æ®ä¸å­˜åœ¨: {data_id}'}
            
            file_path = target_data['file_path']
            
            if not os.path.exists(file_path):
                return {'success': False, 'error': f'æ–‡ä»¶ä¸å­˜åœ¨: {file_path}'}
            
            # è¯»å–æ•°æ®æ–‡ä»¶
            try:
                df = pd.read_csv(file_path, encoding='utf-8')
                print(f"ğŸ“Š è¯»å–æ•°æ®: {len(df)} è¡Œ")
            except Exception as e:
                return {'success': False, 'error': f'è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}'}
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—
            if 'milliseconds' not in df.columns:
                return {'success': False, 'error': 'æ–‡ä»¶ç¼ºå°‘æ—¶é—´åˆ—'}
            
            # è®¡ç®—æ—¶é—´ä¿¡æ¯
            min_time = df['milliseconds'].min()
            max_time = df['milliseconds'].max()
            total_duration = max_time - min_time
            total_points = len(df)
            
            time_info = {
                'totalDuration': float(total_duration),  # æ€»æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
                'totalPoints': int(total_points),        # æ€»æ•°æ®ç‚¹æ•°
                'minTime': float(min_time),              # æœ€å°æ—¶é—´æˆ³
                'maxTime': float(max_time),              # æœ€å¤§æ—¶é—´æˆ³
                'avgInterval': float(total_duration / max(total_points - 1, 1))  # å¹³å‡é—´éš”
            }
            
            print(f"â±ï¸  æ—¶é—´ä¿¡æ¯: æ€»æ—¶é•¿={total_duration:.1f}ms, ç‚¹æ•°={total_points}, é—´éš”={time_info['avgInterval']:.1f}ms")
            
            return {
                'success': True,
                'data': time_info
            }
            
        except Exception as e:
            error_msg = f'è·å–æ—¶é—´ä¿¡æ¯å¤±è´¥: {str(e)}'
            print(f"âŒ {error_msg}")
            import traceback
            print(f"ğŸ“‹ è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
            return {'success': False, 'error': error_msg}
    
    def create_preview_calibrated_data(self, original_file: str, x_offset: float, y_offset: float,
                                     time_start: float = 0, time_end: float = 100) -> Optional[str]:
        """
        åˆ›å»ºé¢„è§ˆç”¨çš„ä¸´æ—¶æ ¡å‡†æ•°æ®æ–‡ä»¶
        
        Args:
            original_file: åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„
            x_offset: Xè½´åç§»é‡
            y_offset: Yè½´åç§»é‡
            
        Returns:
            ä¸´æ—¶æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            print(f"ğŸ” åˆ›å»ºé¢„è§ˆæ ¡å‡†æ•°æ®: {original_file}")
            print(f"ğŸ“Š åç§»é‡: X={x_offset:.3f}, Y={y_offset:.3f}")
            print(f"â° æ—¶é—´èŒƒå›´: {time_start:.1f}% - {time_end:.1f}%")
            print(f"â° æ—¶é—´èŒƒå›´: {time_start:.1f}% - {time_end:.1f}%")
            
            # è¯»å–åŸå§‹æ•°æ®
            if not os.path.exists(original_file):
                print(f"âŒ åŸå§‹æ–‡ä»¶ä¸å­˜åœ¨: {original_file}")
                return None
            
            try:
                df = pd.read_csv(original_file, encoding='utf-8')
                print(f"ğŸ“Š è¯»å–æ•°æ®: {len(df)} è¡Œ")
            except Exception as e:
                print(f"âŒ è¯»å–åŸå§‹æ–‡ä»¶å¤±è´¥: {e}")
                return None
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—
            if 'x' not in df.columns or 'y' not in df.columns:
                print(f"âŒ æ–‡ä»¶ç¼ºå°‘x,yåæ ‡åˆ—")
                return None
            
            # åˆ›å»ºæ•°æ®å‰¯æœ¬
            df_preview = df.copy()
            
            # åº”ç”¨æ—¶é—´èŒƒå›´è¿‡æ»¤
            if 'milliseconds' in df_preview.columns and (time_start > 0 or time_end < 100):
                min_time = df_preview['milliseconds'].min()
                max_time = df_preview['milliseconds'].max()
                total_duration = max_time - min_time
                
                # è®¡ç®—å®é™…çš„æ—¶é—´èŒƒå›´
                actual_start_time = min_time + (total_duration * time_start / 100)
                actual_end_time = min_time + (total_duration * time_end / 100)
                
                # è¿‡æ»¤æ•°æ®
                original_count = len(df_preview)
                df_preview = df_preview[
                    (df_preview['milliseconds'] >= actual_start_time) & 
                    (df_preview['milliseconds'] <= actual_end_time)
                ]
                filtered_count = len(df_preview)
                
                print(f"ğŸ¯ æ—¶é—´è¿‡æ»¤: {original_count} â†’ {filtered_count} è¡Œ "
                      f"(æ—¶é—´èŒƒå›´: {actual_start_time:.1f}ms - {actual_end_time:.1f}ms)")
            
            # åº”ç”¨åæ ‡åç§»
            df_preview['x'] = df_preview['x'] + x_offset
            df_preview['y'] = df_preview['y'] + y_offset
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶è·¯å¾„
            import tempfile
            temp_dir = tempfile.gettempdir()
            temp_filename = f"preview_calibrated_{os.path.basename(original_file)}"
            temp_file = os.path.join(temp_dir, temp_filename)
            
            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
            try:
                df_preview.to_csv(temp_file, index=False, encoding='utf-8')
                print(f"âœ… ä¸´æ—¶æ–‡ä»¶å·²åˆ›å»º: {temp_file}")
            except Exception as e:
                print(f"âŒ ä¿å­˜ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
                return None
            
            return temp_file
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºé¢„è§ˆæ•°æ®å¤±è´¥: {e}")
            import traceback
            print(f"ğŸ“‹ è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
            return None
    
    def delete_data_group(self, data_id: str) -> Dict:
        """åˆ é™¤æ•°æ®ç»„ï¼ˆåŒ…å«è¯¥data_idçš„æ•´ä¸ªç»„ï¼‰"""
        try:
            # è§£ædata_idä»¥è·å–ç»„ä¿¡æ¯
            if data_id.startswith('ad'):
                group_type = 'ad'
                group_num = data_id[2:data_id.find('q')]  # ä»ad3q1ä¸­æå–3
            elif data_id.startswith('m'):
                group_type = 'mci'
                group_num = data_id[1:data_id.find('q')]  # ä»m3q1ä¸­æå–3
            elif data_id.startswith('c'):
                group_type = 'control'
                group_num = data_id[1:data_id.find('q')]  # ä»c3q1ä¸­æå–3
            else:
                return {'success': False, 'error': 'æ— æ•ˆçš„data_idæ ¼å¼'}
            
            # ç¡®å®šè¦åˆ é™¤çš„ç›®å½•
            data_sources = self.config.get("data_sources", {})
            
            # è¦åˆ é™¤çš„ç›®å½•åˆ—è¡¨
            dirs_to_delete = []
            
            # æ·»åŠ calibratedç›®å½•
            calibrated_dir = data_sources.get(f"{group_type}_calibrated", "")
            if calibrated_dir:
                group_dir = os.path.join(calibrated_dir, f"{group_type}_group_{group_num}")
                if os.path.exists(group_dir):
                    dirs_to_delete.append(group_dir)
            
            # æ·»åŠ processedç›®å½•
            processed_dir = data_sources.get(f"{group_type}_processed", "")
            if processed_dir:
                group_dir = os.path.join(processed_dir, f"{group_type}_group_{group_num}")
                if os.path.exists(group_dir):
                    dirs_to_delete.append(group_dir)
            
            # æ·»åŠ rawç›®å½•
            raw_dir = data_sources.get(f"{group_type}_raw", "")
            if raw_dir:
                group_dir = os.path.join(raw_dir, f"{group_type}_group_{group_num}")
                if os.path.exists(group_dir):
                    dirs_to_delete.append(group_dir)
            
            if not dirs_to_delete:
                return {'success': False, 'error': 'æœªæ‰¾åˆ°è¦åˆ é™¤çš„æ•°æ®'}
            
            # åˆ é™¤ç›®å½•
            deleted_dirs = []
            for dir_path in dirs_to_delete:
                try:
                    import shutil
                    shutil.rmtree(dir_path)
                    deleted_dirs.append(dir_path)
                    print(f"ğŸ—‘ï¸ å·²åˆ é™¤ç›®å½•: {dir_path}")
                except Exception as e:
                    print(f"âŒ åˆ é™¤ç›®å½•å¤±è´¥ {dir_path}: {e}")
            
            if deleted_dirs:
                return {
                    'success': True, 
                    'message': f'æˆåŠŸåˆ é™¤{len(deleted_dirs)}ä¸ªç›®å½•',
                    'deleted_dirs': deleted_dirs
                }
            else:
                return {'success': False, 'error': 'åˆ é™¤æ“ä½œå¤±è´¥'}
                
        except Exception as e:
            print(f"âŒ åˆ é™¤æ•°æ®ç»„å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}

    def move_data_between_groups(self, data_id: str, from_group: str, to_group: str) -> Dict:
        """åœ¨ä¸åŒç»„åˆ«ä¹‹é—´ç§»åŠ¨æ•°æ®"""
        try:
            # è§£ædata_idä»¥è·å–ç»„å·å’Œé¢˜ç›®å·
            if data_id.startswith('ad'):
                old_group_num = data_id[2:data_id.find('q')]
                question_num = data_id[data_id.find('q')+1:]
            elif data_id.startswith('m'):
                old_group_num = data_id[1:data_id.find('q')]
                question_num = data_id[data_id.find('q')+1:]
            elif data_id.startswith('c'):
                old_group_num = data_id[1:data_id.find('q')]
                question_num = data_id[data_id.find('q')+1:]
            else:
                return {'success': False, 'error': 'æ— æ•ˆçš„data_idæ ¼å¼'}
            
            # è·å–æ–°çš„ç»„å·
            new_group_num = self._get_next_group_number(to_group)
            
            # æ„å»ºæ–°çš„data_id
            if to_group == 'ad':
                new_data_id = f"ad{new_group_num}q{question_num}"
            elif to_group == 'mci':
                new_data_id = f"m{new_group_num}q{question_num}"
            elif to_group == 'control':
                new_data_id = f"c{new_group_num}q{question_num}"
            else:
                return {'success': False, 'error': 'æ— æ•ˆçš„ç›®æ ‡ç»„åˆ«'}
            
            data_sources = self.config.get("data_sources", {})
            moved_files = []
            
            # ç§»åŠ¨æ¯ç§ç±»å‹çš„æ•°æ®
            for data_type in ['calibrated', 'processed', 'raw']:
                # æºç›®å½•
                src_dir_key = f"{from_group}_{data_type}"
                src_base_dir = data_sources.get(src_dir_key, "")
                if not src_base_dir:
                    continue
                
                src_group_dir = os.path.join(src_base_dir, f"{from_group}_group_{old_group_num}")
                
                # ç›®æ ‡ç›®å½•
                dst_dir_key = f"{to_group}_{data_type}"
                dst_base_dir = data_sources.get(dst_dir_key, "")
                if not dst_base_dir:
                    continue
                
                dst_group_dir = os.path.join(dst_base_dir, f"{to_group}_group_{new_group_num}")
                
                # å¦‚æœæºç›®å½•å­˜åœ¨ï¼Œåˆ™ç§»åŠ¨æ•´ä¸ªç›®å½•
                if os.path.exists(src_group_dir):
                    # ç¡®ä¿ç›®æ ‡ç›®å½•ä¸å­˜åœ¨ï¼ˆé¿å…è¦†ç›–ï¼‰
                    if os.path.exists(dst_group_dir):
                        import shutil
                        shutil.rmtree(dst_group_dir)
                    
                    # åˆ›å»ºç›®æ ‡ç›®å½•çš„çˆ¶ç›®å½•
                    os.makedirs(os.path.dirname(dst_group_dir), exist_ok=True)
                    
                    # ç§»åŠ¨ç›®å½•
                    import shutil
                    shutil.move(src_group_dir, dst_group_dir)
                    moved_files.append(f"{data_type}: {src_group_dir} -> {dst_group_dir}")
                    print(f"ğŸ“ ç§»åŠ¨ç›®å½•: {src_group_dir} -> {dst_group_dir}")
                    
                    # é‡å‘½åç›®å½•å†…çš„æ–‡ä»¶ï¼ˆæ›´æ–°æ–‡ä»¶åä¸­çš„ç»„åˆ«æ ‡è¯†ï¼‰
                    self._rename_files_in_moved_directory(dst_group_dir, old_data_id=data_id, new_data_id=new_data_id, from_group=from_group, to_group=to_group)
            
            if moved_files:
                return {
                    'success': True,
                    'message': f'æˆåŠŸå°†æ•°æ®ä»{from_group}ç»„ç§»åŠ¨åˆ°{to_group}ç»„',
                    'new_data_id': new_data_id,
                    'moved_files': moved_files
                }
            else:
                return {'success': False, 'error': 'æœªæ‰¾åˆ°è¦ç§»åŠ¨çš„æ•°æ®'}
                
        except Exception as e:
            print(f"âŒ ç§»åŠ¨æ•°æ®å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}

    def _rename_files_in_moved_directory(self, directory: str, old_data_id: str, new_data_id: str, from_group: str, to_group: str):
        """é‡å‘½åç§»åŠ¨åç›®å½•ä¸­çš„æ–‡ä»¶"""
        try:
            for filename in os.listdir(directory):
                file_path = os.path.join(directory, filename)
                if os.path.isfile(file_path):
                    # ç”Ÿæˆæ–°æ–‡ä»¶å
                    new_filename = self._generate_new_filename(filename, old_data_id, new_data_id, from_group, to_group)
                    if new_filename != filename:
                        new_file_path = os.path.join(directory, new_filename)
                        os.rename(file_path, new_file_path)
                        print(f"ğŸ“„ é‡å‘½åæ–‡ä»¶: {filename} -> {new_filename}")
        except Exception as e:
            print(f"âŒ é‡å‘½åæ–‡ä»¶å¤±è´¥: {e}")

    def _generate_new_filename(self, filename: str, old_data_id: str, new_data_id: str, from_group: str, to_group: str) -> str:
        """ç”Ÿæˆæ–°çš„æ–‡ä»¶å"""
        try:
            # ä»filenameä¸­æå–ç»„å·å’Œé—®é¢˜å·
            parsed_old = self.parse_data_filename(filename, from_group)
            if not parsed_old:
                print(f"âš ï¸ æ— æ³•è§£ææ–‡ä»¶å: {filename}")
                return filename
            
            old_group_num = parsed_old['group_num']
            question_num = parsed_old['question_num']
            
            # ä»new_data_idä¸­æå–æ–°çš„ç»„å·
            if new_data_id.startswith('ad'):
                new_group_num = new_data_id[2:].split('q')[0]
            else:
                new_group_num = new_data_id[1:].split('q')[0]
            
            # æ ¹æ®ç›®æ ‡ç»„ç¡®å®šæ–°çš„æ–‡ä»¶å‰ç¼€
            if to_group == 'control':
                new_prefix = 'n'
            elif to_group == 'mci':
                new_prefix = 'm'
            elif to_group == 'ad':
                new_prefix = 'ad'
            else:
                return filename
            
            # æ„å»ºæ–°çš„æ–‡ä»¶åå‰ç¼€
            new_file_prefix = f"{new_prefix}{new_group_num}q{question_num}"
            
            # æ‰¾åˆ°åŸæ–‡ä»¶åä¸­çš„æ—§å‰ç¼€éƒ¨åˆ†
            if from_group == 'control':
                old_file_prefix = f"n{old_group_num}q{question_num}"
            elif from_group == 'mci':
                old_file_prefix = f"m{old_group_num}q{question_num}"
            elif from_group == 'ad':
                old_file_prefix = f"ad{old_group_num}q{question_num}"
            else:
                return filename
            
            # æ›¿æ¢æ–‡ä»¶åä¸­çš„å‰ç¼€éƒ¨åˆ†
            if old_file_prefix in filename:
                new_filename = filename.replace(old_file_prefix, new_file_prefix)
                print(f"ğŸ“„ é‡å‘½åæ–‡ä»¶: {filename} -> {new_filename}")
                return new_filename
            else:
                # å¦‚æœç›´æ¥æ›¿æ¢ä¸è¡Œï¼Œè¿”å›åŸæ–‡ä»¶å
                print(f"âš ï¸ æ— æ³•åœ¨æ–‡ä»¶å {filename} ä¸­æ‰¾åˆ°å‰ç¼€ {old_file_prefix}ï¼Œä¿æŒåŸå")
                return filename
                
        except Exception as e:
            print(f"âŒ æ–‡ä»¶é‡å‘½åé”™è¯¯: {e}")
            return filename

    def run_server(self, host: str = '127.0.0.1', port: int = 8080, 
                   debug: bool = False, open_browser: bool = True):
        """
        è¿è¡ŒWebæœåŠ¡å™¨
        
        Args:
            host: ä¸»æœºåœ°å€
            port: ç«¯å£å·
            debug: è°ƒè¯•æ¨¡å¼
            open_browser: æ˜¯å¦è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
        """
        print(f"ğŸŒ å¯åŠ¨å¢å¼ºç‰ˆWebå¯è§†åŒ–æœåŠ¡å™¨")
        print(f"ğŸ“ åœ°å€: http://{host}:{port}")
        print(f"ğŸ” ROIå®šä¹‰æ•°é‡: {len(self.analyzer.roi_definitions)}")
        print(f"ğŸ¨ å¯è§†åŒ–åŠŸèƒ½: ROIç»˜åˆ¶ã€è½¨è¿¹åˆ†æã€åºåˆ—æ ‡è®°")
        print("=" * 60)
        
        if open_browser:
            # å»¶è¿Ÿæ‰“å¼€æµè§ˆå™¨
            import threading
            import time
            def open_browser_delayed():
                time.sleep(1.5)
                webbrowser.open(f'http://{host}:{port}')
            
            threading.Thread(target=open_browser_delayed, daemon=True).start()
        
        try:
            self.app.run(host=host, port=port, debug=debug)
        except Exception as e:
            print(f"âŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")

    def fix_existing_data_files(self, group_type: str = None) -> Dict:
        """
        ä¿®å¤ç°æœ‰æ•°æ®æ–‡ä»¶ï¼Œä¸ºç¼ºå°‘millisecondsåˆ—çš„æ•°æ®æ·»åŠ è¯¥åˆ—
        
        Args:
            group_type: æŒ‡å®šç»„ç±»å‹ï¼Œå¦‚æœä¸ºNoneåˆ™å¤„ç†æ‰€æœ‰ç»„
            
        Returns:
            ä¿®å¤ç»“æœç»Ÿè®¡
        """
        try:
            print(f"ğŸ”§ å¼€å§‹ä¿®å¤ç°æœ‰æ•°æ®æ–‡ä»¶...")
            
            stats = {
                'total_files': 0,
                'fixed_files': 0,
                'already_ok_files': 0,
                'error_files': 0,
                'details': []
            }
            
            # ç¡®å®šè¦å¤„ç†çš„ç»„ç±»å‹
            if group_type:
                group_types = [group_type]
            else:
                group_types = ['control', 'mci', 'ad']
            
            for gt in group_types:
                print(f"\nğŸ“ å¤„ç† {gt} ç»„æ•°æ®...")
                
                # è·å–è¯¥ç»„çš„æ‰€æœ‰æ•°æ®
                data_list = self.get_group_data(gt)
                
                for data_item in data_list:
                    stats['total_files'] += 1
                    file_path = data_item['file_path']
                    data_id = data_item['data_id']
                    
                    try:
                        # è¯»å–æ–‡ä»¶
                        if not os.path.exists(file_path):
                            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                            stats['error_files'] += 1
                            continue
                        
                        df = pd.read_csv(file_path, encoding='utf-8')
                        
                        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰millisecondsåˆ—
                        if 'milliseconds' in df.columns:
                            print(f"âœ… {data_id}: å·²æœ‰millisecondsåˆ—ï¼Œè·³è¿‡")
                            stats['already_ok_files'] += 1
                            continue
                        
                        print(f"ğŸ”§ {data_id}: æ·»åŠ millisecondsåˆ—...")
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰timestampåˆ—
                        if 'timestamp' not in df.columns:
                            print(f"âŒ {data_id}: ç¼ºå°‘timestampåˆ—ï¼Œæ— æ³•ä¿®å¤")
                            stats['error_files'] += 1
                            stats['details'].append(f"{data_id}: ç¼ºå°‘timestampåˆ—")
                            continue
                        
                        # æ·»åŠ millisecondsåˆ—
                        max_timestamp = df['timestamp'].max()
                        
                        if max_timestamp < 10000:  # ç›¸å¯¹æ—¶é—´å€¼ï¼ˆç§’ï¼‰
                            # è½¬æ¢ä¸ºæ¯«ç§’æ—¶é—´æˆ³
                            import time
                            current_time_ms = int(time.time() * 1000)
                            df['milliseconds'] = current_time_ms + (df['timestamp'] * 1000).astype(int)
                            print(f"   è½¬æ¢ç›¸å¯¹æ—¶é—´æˆ³: {df['timestamp'].min():.3f}s-{df['timestamp'].max():.3f}s -> {df['milliseconds'].min()}-{df['milliseconds'].max()}")
                        else:
                            # ç›´æ¥ä½¿ç”¨ä½œä¸ºæ¯«ç§’æ—¶é—´æˆ³
                            df['milliseconds'] = df['timestamp'].astype(int)
                            print(f"   ä½¿ç”¨ç°æœ‰æ—¶é—´æˆ³: {df['milliseconds'].min()}-{df['milliseconds'].max()}")
                        
                        # æ£€æŸ¥æ˜¯å¦ç¼ºå°‘å…¶ä»–å…¼å®¹æ€§åˆ—
                        added_columns = []
                        
                        # æ·»åŠ åº¦æ•°åˆ—ï¼ˆå¦‚æœç¼ºå°‘ï¼‰
                        if 'x_deg' not in df.columns and 'x' in df.columns:
                            fov_deg = 110.0  # é»˜è®¤è§†åœºè§’
                            df['x_deg'] = (df['x'] - 0.5) * fov_deg
                            added_columns.append('x_deg')
                        
                        if 'y_deg' not in df.columns and 'y' in df.columns:
                            fov_deg = 110.0  # é»˜è®¤è§†åœºè§’
                            df['y_deg'] = (df['y'] - 0.5) * fov_deg
                            added_columns.append('y_deg')
                        
                        # æ·»åŠ è§’é€Ÿåº¦åˆ—ï¼ˆå¦‚æœç¼ºå°‘ï¼‰
                        if 'velocity_deg_s' not in df.columns:
                            df['velocity_deg_s'] = 0.0
                            
                            # å¦‚æœæœ‰æ—¶é—´å·®åˆ—ï¼Œè®¡ç®—è§’é€Ÿåº¦
                            if 'time_diff' in df.columns and 'x_deg' in df.columns and 'y_deg' in df.columns:
                                for i in range(1, len(df)):
                                    dt = df.iloc[i]['time_diff'] / 1000.0  # è½¬æ¢ä¸ºç§’
                                    if dt > 0:
                                        dx_deg = df.iloc[i]['x_deg'] - df.iloc[i-1]['x_deg']
                                        dy_deg = df.iloc[i]['y_deg'] - df.iloc[i-1]['y_deg']
                                        
                                        import math
                                        angular_distance = math.sqrt(dx_deg**2 + dy_deg**2)
                                        velocity_deg_s = angular_distance / dt
                                        
                                        df.iloc[i, df.columns.get_loc('velocity_deg_s')] = velocity_deg_s
                            
                            added_columns.append('velocity_deg_s')
                        
                        if added_columns:
                            print(f"   æ·»åŠ çš„åˆ—: {', '.join(added_columns)}")
                        
                        # ä¿å­˜ä¿®å¤åçš„æ–‡ä»¶
                        df.to_csv(file_path, index=False, encoding='utf-8')
                        print(f"âœ… {data_id}: ä¿®å¤å®Œæˆ")
                        stats['fixed_files'] += 1
                        stats['details'].append(f"{data_id}: æˆåŠŸæ·»åŠ  {', '.join(['milliseconds'] + added_columns)}")
                        
                    except Exception as e:
                        print(f"âŒ {data_id}: ä¿®å¤å¤±è´¥ - {str(e)}")
                        stats['error_files'] += 1
                        stats['details'].append(f"{data_id}: é”™è¯¯ - {str(e)}")
                        continue
            
            print(f"\nğŸ“Š ä¿®å¤å®Œæˆç»Ÿè®¡:")
            print(f"   æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
            print(f"   ä¿®å¤æ–‡ä»¶æ•°: {stats['fixed_files']}")
            print(f"   å·²æ­£å¸¸æ–‡ä»¶æ•°: {stats['already_ok_files']}")
            print(f"   é”™è¯¯æ–‡ä»¶æ•°: {stats['error_files']}")
            
            if stats['details']:
                print(f"\nğŸ“‹ è¯¦ç»†ä¿¡æ¯:")
                for detail in stats['details']:
                    print(f"   {detail}")
            
            return {
                'success': True,
                'stats': stats
            }
            
        except Exception as e:
            error_msg = f'ä¿®å¤æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}'
            print(f"âŒ {error_msg}")
            import traceback
            print(f"ğŸ“‹ è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
            return {'success': False, 'error': error_msg}

    def _load_background_images(self):
        """åŠ è½½èƒŒæ™¯å›¾ç‰‡åˆ—è¡¨"""
        try:
            background_dir = "data/background_images"
            if os.path.exists(background_dir):
                self.background_images = [f for f in os.listdir(background_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]
                print(f"ğŸ“· åŠ è½½äº† {len(self.background_images)} ä¸ªèƒŒæ™¯å›¾ç‰‡")
            else:
                print(f"âš ï¸  èƒŒæ™¯å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {background_dir}")
                self.background_images = []
        except Exception as e:
            print(f"âŒ åŠ è½½èƒŒæ™¯å›¾ç‰‡å¤±è´¥: {e}")
            self.background_images = []

    def _load_mmse_scores(self):
        """åŠ è½½MMSEåˆ†æ•°æ•°æ®"""
        try:
            mmse_dir = "data/MMSE_Score"
            self.mmse_scores = {
                'control': {},
                'mci': {},
                'ad': {}
            }
            
            # æ–‡ä»¶æ˜ å°„
            mmse_files = {
                'control': 'æ§åˆ¶ç»„.csv',
                'mci': 'è½»åº¦è®¤çŸ¥éšœç¢ç»„.csv', 
                'ad': 'é˜¿å°”å…¹æµ·é»˜ç—‡ç»„.csv'
            }
            
            for group_type, filename in mmse_files.items():
                file_path = os.path.join(mmse_dir, filename)
                if os.path.exists(file_path):
                    df = pd.read_csv(file_path, encoding='utf-8')
                    for _, row in df.iterrows():
                        # å¤„ç†åˆ—åå·®å¼‚ï¼šæœ‰äº›æ–‡ä»¶ç”¨"å—è¯•è€…"ï¼Œæœ‰äº›ç”¨"è¯•è€…"
                        if 'å—è¯•è€…' in df.columns:
                            subject_id = row['å—è¯•è€…']
                        elif 'è¯•è€…' in df.columns:
                            subject_id = row['è¯•è€…']
                        else:
                            print(f"âš ï¸  {filename} æ–‡ä»¶ä¸­æ‰¾ä¸åˆ°å—è¯•è€…åˆ—")
                            continue
                        # è§£æç»„ç¼–å·
                        group_num = self.parse_subject_id(subject_id)
                        if group_num:
                            self.mmse_scores[group_type][group_num] = {
                                'subject_id': subject_id,
                                'total_score': row['æ€»åˆ†'],
                                'details': {
                                    # Q1: æ—¶é—´å®šå‘ (å¹´ä»½,å­£èŠ‚,æœˆä»½,æ˜ŸæœŸ)
                                    'q1_orientation_time': {
                                        'å¹´ä»½': row['å¹´ä»½'],
                                        'å­£èŠ‚': row['å­£èŠ‚'], 
                                        'æœˆä»½': row['æœˆä»½'],
                                        'æ˜ŸæœŸ': row['æ˜ŸæœŸ']
                                    },
                                    # Q2: åœ°ç‚¹å®šå‘ (çœå¸‚åŒº,è¡—é“,å»ºç­‘,æ¥¼å±‚)
                                    'q2_orientation_place': {
                                        'çœå¸‚åŒº': row['çœå¸‚åŒº'],
                                        'è¡—é“': row['è¡—é“'],
                                        'å»ºç­‘': row['å»ºç­‘'],
                                        'æ¥¼å±‚': row['æ¥¼å±‚']
                                    },
                                    # Q3: å³åˆ»è®°å¿†
                                    'q3_immediate_memory': row['å³åˆ»è®°å¿†'],
                                    # Q4: è®¡ç®—èƒ½åŠ› (100-7,93-7,86-7,79-7,72-7)
                                    'q4_calculation': {
                                        '100-7': row['100-7'],
                                        '93-7': row['93-7'],
                                        '86-7': row['86-7'],
                                        '79-7': row['79-7'],
                                        '72-7': row['72-7']
                                    },
                                    # Q5: å»¶è¿Ÿå›å¿† (è¯1,è¯2,è¯3)
                                    'q5_delayed_recall': {
                                        'è¯1': row['è¯1'],
                                        'è¯2': row['è¯2'],
                                        'è¯3': row['è¯3']
                                    }
                                }
                            }
                    print(f"ğŸ“Š åŠ è½½äº† {len(self.mmse_scores[group_type])} ä¸ª{group_type}ç»„MMSEåˆ†æ•°")
                else:
                    print(f"âš ï¸  MMSEæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            total_scores = sum(len(scores) for scores in self.mmse_scores.values())
            print(f"ğŸ§  æ€»å…±åŠ è½½äº† {total_scores} ä¸ªMMSEåˆ†æ•°è®°å½•")
            
        except Exception as e:
            print(f"âŒ åŠ è½½MMSEåˆ†æ•°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            self.mmse_scores = {'control': {}, 'mci': {}, 'ad': {}}

    def parse_subject_id(self, subject_id: str) -> Optional[int]:
        """
        è§£æå—è¯•è€…IDè·å–ç»„ç¼–å·
        
        Args:
            subject_id: å—è¯•è€…ID (å¦‚ n01, M01, ad01)
            
        Returns:
            ç»„ç¼–å· (å¦‚ 1, 2, 3...)
        """
        try:
            import re
            # åŒ¹é… n01, M01, ad01 ç­‰æ ¼å¼
            match = re.match(r'[a-zA-Z]+(\d+)', subject_id)
            if match:
                return int(match.group(1))
            return None
        except:
            return None

    def get_mmse_score(self, group_type: str, group_num: int) -> Optional[Dict]:
        """
        è·å–æŒ‡å®šç»„çš„MMSEåˆ†æ•°
        
        Args:
            group_type: ç»„ç±»å‹ (control, mci, ad)
            group_num: ç»„ç¼–å·
            
        Returns:
            MMSEåˆ†æ•°ä¿¡æ¯
        """
        try:
            return self.mmse_scores.get(group_type, {}).get(group_num)
        except:
            return None

    def get_mmse_assessment_level(self, score: int) -> Dict:
        """
        æ ¹æ®VR-MMSEåˆ†æ•°è·å–è®¤çŸ¥è¯„ä¼°ç­‰çº§
        
        VR-MMSEåˆ†ç±»æ ‡å‡†:
        - æ­£å¸¸ç»„ï¼š19.1Â±1.6 (çº¦17.5-20.7)
        - MCIç»„ï¼š18.0Â±1.9 (çº¦16.1-19.9)  
        - ADç»„ï¼š13.5Â±2.7 (çº¦10.8-16.2)
        
        Args:
            score: VR-MMSEæ€»åˆ†
            
        Returns:
            è¯„ä¼°ç­‰çº§ä¿¡æ¯
        """
        if score >= 20:
            return {'level': 'æ­£å¸¸', 'color': '#28a745', 'description': 'è®¤çŸ¥åŠŸèƒ½æ­£å¸¸'}
        elif score >= 19:
            return {'level': 'æ­£å¸¸èŒƒå›´', 'color': '#17a2b8', 'description': 'è®¤çŸ¥åŠŸèƒ½æ­£å¸¸èŒƒå›´'}
        elif score >= 16:
            return {'level': 'è½»åº¦è®¤çŸ¥éšœç¢', 'color': '#ffc107', 'description': 'è½»åº¦è®¤çŸ¥éšœç¢(MCI)'}
        elif score >= 11:
            return {'level': 'é˜¿å°”å…¹æµ·é»˜ç—‡', 'color': '#fd7e14', 'description': 'é˜¿å°”å…¹æµ·é»˜ç—‡(AD)'}
        else:
            return {'level': 'é‡åº¦è®¤çŸ¥éšœç¢', 'color': '#dc3545', 'description': 'é‡åº¦è®¤çŸ¥éšœç¢'}



def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•"""
    visualizer = EnhancedWebVisualizer()
    print("ğŸŒ å¢å¼ºç‰ˆWebå¯è§†åŒ–å™¨")
    print("=" * 50)
    print("åŠŸèƒ½ç‰¹æ€§:")
    print("  - å¤šå±‚ROIç»˜åˆ¶ (keywords, instructions, background)")
    print("  - äº‹ä»¶ç±»å‹å¯è§†åŒ– (å›ºè§†/æ‰«è§†)")
    print("  - ROIåºåˆ—æ ‡è®° (Enter/Exit)")
    print("  - å¢å¼ºç»Ÿè®¡é¢æ¿")
    print("  - èƒŒæ™¯å›¾åƒæ”¯æŒ")
    print("\nä½¿ç”¨ç¤ºä¾‹:")
    print("visualizer = EnhancedWebVisualizer()")
    print("visualizer.run_server(port=8080)")

if __name__ == "__main__":
    main() 